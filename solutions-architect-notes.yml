IAM access keys: 
  long term credentials, used with IAM users only:
  doesn't change automatically or regularly:
  use access keys when using CLI:
  similar to user name and password, but differences are:
    an IAM user can have two access keys, either 0, 1, or 2
    can be created, deleted, made inactive or active
  formed from two parts: 
    access key ID, and secret access key
    access key id is like username, while secret acces key is like password

  aws configure: 
    allows us to configure the default configuration for CLI
  aws configure --profile namedProfile: 
    named profiles, allow us to configure multiple aws accounts to our CLI instead of just using one account, whenever running commands append '--profile nameOfProfile' to run from NameOfProfile account

-----------------------------------------------------------------------------------------------:
NIST definition of cloud computing (read through): 
5 characteristics: 
  On demand self service: 
    allows you to provision capabilities as needed without requiring human interaction
  broad network access: 
    capabilities are available over the network and accessed through standard mechanisms
  Resource pooling: 
    about abstraction, there is a sense of location independence and resources are pooled to serve multiple consumers using a multi tenant model
  Rapid elasticity: 
    resources can be elastically provisioned and released to scale rapidly in real time with no human interaction
  Measured service: 
    Resource usage can be monitored, controlled, reported, and billed 

-----------------------------------------------------------------------------------------------:
Public vs Private vs Hybrid vs Multi Cloud Models (KNOW ALL):
  multi-cloud: 
    using multiple public clouds for higher levels of availability and durability
  private: 
    is on premise and still meets 5 requirements of cloud computing, ex aws outposts, most on premise traditional datacenters do not meet 5 requirements
  hybrid: 
    using both public cloud and private cloud cooperating together as a single environment

-----------------------------------------------------------------------------------------------:
YAML: 
  language for defining data or configuration:
  characteristics:
    unordered collection of key-value pairs, (dictionary)
    indentation matters, spaces are like brackets

-----------------------------------------------------------------------------------------------:
Encryption:
  Encryption at rest: 
    helps to secure data from physical access, example would be encrypting data stored on a hard drive, and decrypting when its read.  Secret is used to decrypt the data.:
  encryption at transit: 
    helps to secure data that is being transferred between two places:
    encrypted with an encryption tunnel:

  Plaintext: 
    un-encrypted data
  algorithm: 
    code that takes plain text and an encryption key and generates encrypted data
  ciphertext: 
    encrypted data

  key: decrypts ciphertext, "password"
    different types: 
      symmetric key: 
        same key is used for both encryption and decryption processes:
        used by a single party:
      asymmetric key: 
        comprised of public and private keys, receiver's public key is used to encrypt, receiver's private key is used to decrypt:
        used by two or more parties:

  signing: 
    allows verification of who sent the encrypted message:
    requires the sender to sign with private key:
    requires the receiver to decrypt signature with sender's public key:
  steganography: 
    allows you to embed data inside other data, like hiding ciphertext in an image

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
NETWORKING:

  OSI 5 LAYER MODEL: 
    networking stack, that includes seven software layers for transporting data:
    layer x understands its layer and below, built on top of other layers:
    physical, data link, network, transport, session, presentation, application layers
    media layers: 
      how data is moved between point A and B
      physical, data link, network, 
    host layers: 
      how data is chopped up and reassembled for transport and formatted to be understood by both sides of a network connection
      transport, session, presentation, application layers

-----------------------------------------------------------------------------------------------:
  Physical Layer: Layer 1: 
    defines the transmission and reception of raw bit streams between a device and a shared physical medium.:

    can be different physical mediums, copper(electrical), fibre(light), or WIFI(RF)
    no device addressing at layer 1, all data is processed by all devices
    if multiple devices transmit at once a collission occurs
    no media access control and no collision detection
    tends to not scale very well
    hub: 
      anything received on any port, is transmitted on every other port, including errors and collisions:

-----------------------------------------------------------------------------------------------:
  Data Link Layer: Layer, 2: 
    packages frames to be sent on layer 1, the physical layer:
    ethernet: 
      most popular l2 protocol used generally for local networks 
    MAC address: 
      unique hardware address

    frames: 
      format for sending information over layer 2 network, can be addressed to MAC address or broadcast to all
      payload: 
        the data the frame carries from source to destination.  It's generally provided by layer 3 and the ET attribute defines which l3 protocol is used

    CSMA/CD: 
      provides controlled access to physical medium via:
        carrier sense multiple access: 
          checks carrier(to see if anything is transmitting on layer 1), and if no carrier then passes frame to layer 1 to transmit, if there is carrier it waits to send until a carrier is not detected
        collision detection: 
          detects if there was collision, if there was collision reattempts collision after a certain amount of time

    switch: 
      intelligent layer 2 device that works similarly to a hub but much more efficient because it understands layer 2:
      HOW IT WORKS:
        learns which devices are connected at layer 2 and populates a MAC address table with that MAC address's port:
        if it receives frame for MAC address in table, it orders that frame to be sent to that port only on layer 1, unlike a hub which would send to every single port
        if it receives frame for MAC address not in table, it sends to all ports
        every x port has X collision domains, meaning if there is a collision on a port, it only occurs on that single port

-----------------------------------------------------------------------------------------------:
  Network Layer, Layer 3: 
    gets data from one location to another:
    uses packets which are similar to frames:
    generally an ip packet doesnt change, while a frame does between each lvl2 destination because a frame has address of next intermediary address and ip packet has source ip address and destination ip address
    contains l4 protocol in packet in protocol field and data received from layer 4 
    ipv4 and ipv6 packets, ipv6 packets have larger addresses
    ipv4 addressing:
      DHCP: 
        machine that assigns IP addresses:
        Ip addresses have a network part and a host part, if they have same network part they are on same network
      subnet masks: 
        divides the host and network part via shorthand prefix(starts from left), letting an IP device know if another device is on same network or not, which influences if it communicates directly with device or default gateway:
    ipv6 addressing:
      340 sextillion address spaces vs 4 billion ipv4 address spaces

    Every router has one or more route tables, letting router know destination where to send packet to.  Packets are routed, hop by hop across the internet:  

    default gateway: 
      IP address on local network that packets are forwarded to if intended destination is not a local ip address, typically a router:

    ARP, Address Resolution Protocol: 
      finds Mac address for a given IP address:
      a protocol that allows us to encapsulate layer 3 packet in a l2 frame, broadcasts to all devices on the ip address network and asks for mac adress, the corresponding device then sends mac address and frame is created

    How to convert decimal to binary for IPv4 addressing: 
      133.33.33.7 <=> 10000101.00100001.00100001.00000111
      Each number in dotted decimal notation maps to 8 bits:

-----------------------------------------------------------------------------------------------:
  The Transport Layer, Layer 4 and the Session Layer, Layer 5: 
    provides most of the functionality that supports most of the networking of the internet:
    packages data in segments, they are encapsulated within IP packets:
    implements two protocols:
      Transmission Control Protocol, TCP:
      User datagram Protocol, UDP:

    TCP/IP: 
      TCP is layer 4 protocol running on top of IP, used in most of the important Application Layer protocols like HTTP, HTTPS, SSH, and so on, TCP is connection oriented protocol:
      has source and destination ports, sequence number
      connection between random port on client and known port on the server, flags set up connection, makes error checking, ordering, and retransmission possible because everything is connection based
    UDP/IP: 
      used in same way as tcp:
      faster (no overhead) but it is also less reliable:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
STATEFUL FIREWALL: 
  understands TCP/IP connection so only one rule is needed:
  extension of security group 
  initiating request source port and destination port and response traffic is automatically allowed

-----------------------------------------------------------------------------------------------:
STATELESS FIREWALL, (Network ACL, NACL): 
  two rules will be required, one to allow the outbound traffic source and destination port and the other to allow the inbound traffic source and destination port:

-----------------------------------------------------------------------------------------------:
NAT, NETWORK ADDRESS TRANSLATION: 
    helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse:
    takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table:
    static NAT: 
      1 private to 1 fixed public address, use when a private IP needs access to public internet by using a public IP and where these IPs need to be consistent 
    dynamic NAT: 
      1 private to 1st available public ip in Nat table pool, use when you have less public IPs than private IP addresses and you want to be efficient with how they're used, if there are no available in the pool then public access can fail
    port address Translation: 
      many private to 1 public address, use for home router, use when you have many devices that use a single public address, NAT gateway with AWS use this way, uses ports to identify devices by using a source and destination port for each packet, 
    NAT table: 
      maps one to one private Ip to allocated public ip address, not configured though
    ipv6 doesn't need any network translation because there are so many more addresses

-----------------------------------------------------------------------------------------------:
CIDR : 
  defines a way of expressing a size of a network:
  subnet: 
    takes a larger network and breaks down into smaller networks, each of which has a higher prefix:
  entire internet: /0
  class a network: /8
  class b network: /16
  class c network: /24
  one ip address: /32
  takeway: 
    larger the subnet mask prefix, the smaller the network:
    every number (x) you increment in prefix, the number of networks it splits that prior network into goes up by 2^x

-----------------------------------------------------------------------------------------------:
DDOS attacks: 
  attacks designed to overload websites:
  compete against legitimate connections
  distributed and therefore hard to block individual IPs/ranges
  involve large armies of compromised machines (botnets)
  3 types:
    application Layer - HTTP flood: 
      take advantage of imbalance of processing between client and server, send countless requests
    protocol attack - SYN flood: 
      takes advantage of the connection based nature of requests, spoof a source ip address and server can't complete stage 2 of handshake and waits an amount of time before it stops trying consuming resources
    volumetric- DNS amplification: 
      takes advantage of how certain protocols like DNS take only a small amount of data to make request and require large response

-----------------------------------------------------------------------------------------------:
SSL AND TLS, SECURE SOCKETS LAYER and TRANSPORT LAYER SECURITY: 
  provide privacy and data integrity between client and server:
  both do the same thing but TLS is newer and more secure version:

  allows identity verification two ways possible, but generally is client verifying server:
  provides private communications that are encrypted first asymmetric and then symmetric:
  provides reliable connection, detects data alteration

  TLS phases that occur after TCP connection is made between client and server:
    Cipher suites: 
      set of protocols used by TLS that are agreed on for communications, server sends server certificate that contains public key
    authentication: 
      client needs to validate server certificate and public key is valid,  verifies via the CA, verifies the server has the private key
    key exchange: 
      moves from assymetric encryption to symmetric encryption

-----------------------------------------------------------------------------------------------:
HASH FUNCTIONS AND HASHING:
  hashing: 
    process where algorithm is used to turn any kind of data into a fixed length representation of that data:
  hash function: 
    input is data and output is fixed length representation called hash, unique data results in unique hash value, hash function is one way because you can't use hash as input for hash function to get original data as output:
  collision: 
    when different inputs result in same hash value, if there are collisions encryption is unreliable and less secure
  MD-5: 
    not reliable because data can be manipulated to cause collisions
  SHA-256: 
    modern and reliable hashing algorithm:

-----------------------------------------------------------------------------------------------:
Public vs Private Services:
  refers to networking only
  public aws service: 
    something which is accessed using public endpoints, located in the AWS public zone and anyone can connect, but permissions are required to access the service:
  private aws service: 
    something which is located within a VPC and is accessed within a VPC or public endpoints in a VPC:

-----------------------------------------------------------------------------------------------:
Three Different Network Zones:
  public internet zone: 
    internet services, anyone can access
  aws public zone: 
    aws public services like s3, operates in between public internet zone and aws private zone, users access public aws zone using internet as transit
  aws private zone: 
    where VPCs reside,  VPCs are isolated unless configured otherwise:
    nothing from the internet can access the VPC unless you allow it via public ip 
    on premise can access via AWS VPN or Direct Connect
    internet gateway: 
      allows VPC service access to public internet with an allocated public ip address: 
      allows VPC service to be accessed via public internet by allocating public ip address: 
      allows VPC service access to public aws service as long as data does not touch public internet at any point
      EXAM:
        only allows public services to access outside VPC, does not work with private service so need to implement NAT gateway if you want private service inside vpc to access outside its VPC:

-----------------------------------------------------------------------------------------------:
AWS GLOBAL INFRASTRUCTURE:
  region: 
    a physical geographic location that is a collection of availability zones:
    geographic separation allows isolated fault domain
    geopolitical seperation allows different governance that is dicted by that region and data stays within that region
    reference by: 
      region code or region name, ap-southeast-2 or Asia Pacific (Sydney):

  edge locations: 
    allow us to cache content and lower latency: 
    generally only have content distribution services, and edge computing: 

  availability zone: 
    single data center or a collection of data centers within a region:
    provides redundant networking, low latency, bc if one availability zone is affected the others most likely wont be affected

  service resilience:
    globally resilient: 
      service operates globally with a single database and its data is replicated across several regions, is highly available because it is always available unless every single region fails, ex route53 and IAM:
    regionally resilient:
      service that operates in a single region with one set of data per region.  replicates data across multiple availability zones within that region.:
    AZ resilient:
      service that operates in a single availability zone, if availability zone fails then that service fails:

-----------------------------------------------------------------------------------------------:
VIRTUAL PRIVATE CLOUD:
  allows you to create a secure virtual private network in the AWS cloud where you secure your services and resources:
  is within 1 account and 1 region, regionally resilient
  private and isolated unless you decide otherwise

  Default VPC: 
    max of 1 per region, come preconfigured by aws:
    a lot less flexible than custom VPCs, you can remove and redeploy if you want
    DEFAULT VPC CIDR: 
      172.31.0.0/16:
    to replicate across availabity zones: 
      the CIDR range is broken into subnets so that there is a subnet for each availability zone
    Subnets assign public IPv4 addresses: 
      anything deployed in their subnets get deployed with public addresses:
    preconfigured with: 
      IGW, SG, & NACL:

  Custom VPCs: 
    you can have many in 1 region:
    require you to configure everything end to end:
    private by default

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
EC2:
  allows you to rent and manage virtual servers in the cloud elastically:
  ec2 instances are the virtual servers running on the physical servers:
  default compute service within AWS:
  operating system for each instance configured in a certain way with a certain set of allocated resources
  private service by default: 
    uses VPC networking
  resiliency: 
    AZ resilient:
  features:
    different instance sizes and capabilities, you can set when you provision or some settings can be changed even after deployment:
    on demand billing, per second
  different types of storage: 
    many, including on host storage or Elastic Block Store, EBS
  states:
    running and stopped: 
      can toggle between states, like off/on switch:
    terminated: 
      fully deleted once you move to termination state:
  charges:
    cpu, memory, disk, and networking, charged per second,
    charged for all four while running: 
    charged for disk (attached ebs volumes) only while stopped:

-----------------------------------------------------------------------------------------------:
AMI:
    template of an instance configuration that allows us to provision instances:
    AMI => EC2 => AMI:
    ec2 instance can be generated from an amazon machine image, or generate an ami from ec2:

    permissions: 
      public: 
        everyone allows
      owner: 
        private, implicitly allows the owner to create ami from ec2,
      explicit: 
        owner explicitly allows access to specific aws accounts

    root volume: 
      the c drive in windows, the drive that boots the OS
    block device mapping: 
      determines which volume is root volume and which volume is a data volume

-----------------------------------------------------------------------------------------------:
CONNECTING TO AN EC2:
    EXAM: 
      since we have different OS available for instances, we connect to different ports based on the OS:
        3389: Remote Desktop Protocol, windows instances:
        22: SSH Protocol, Linux instances:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
S3:
  global object storage service that is highly available and durable:
  regional based because data is stored inside a specific region and never moves unless you explicitly move it
  default storage service for AWS
  Regionally resilient:
  public service, unlimited data and multi-user
  economical
  accessed via: 
    UI, CLI, API, HTTP

  objects: 
    data that s3 stores, like a file:
    composed of key: 
      like a fileName
    composed of value: 
      data or value of the object, can range from 0 bytes to 5TB
    other components: 
      version ID, metadata, access control, and subresources

  buckets: 
    containers that store objects:
    deployed in a specific aws region:
    regionally resilient
    name needs to be globally unique
    unlimited number of objects
    structure: 
      flat, all objects are stored at the root level.  However, displayed like there is a file structure in UI:
    configurations: 
      most configurations for s3 are set at bucket level

  exam powerup: 
    bucket names are globally unique:
    3-63 characters, all lower case, no underscores:
    start with a lowercase letter or a number:
    can't be IP formatted:
    number of buckets has 100 soft limit, 1000 hard limit per account:
    unlimited objects in bucket, 0 bytes to 5 TB:
    key=name, value = data:

  Patterns and Antipatterns:
    s3 is an object store, not file or block:
    you can't mount an s3 bucket as K:\ or /images, block storage has a single user limitation and s3 does not have that limitation
    great for large scale data storage, distribution, or upload
    great for offload, which deals with storing data on s3 bucket rather than ec2 instance:
    should be default input for any aws services or output for most aws products:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
Cloudformation:
  allows you to create templates to provisions AWS resources using IAC:
  written either in YAML or JSON:
  template: 
    tells stack what logical resources to contain and their properties: 
    contains:
      resources: 
        contains at least one logical resource, they have type and properties
      description: 
        gives some details about the template, decription has to immediately follow AWSTemplateFormatVersion if there is one
      metadata: 
        controls how resources look in the UI, and other things
      parameters: 
        fields that prompt user for more information
      mappings: 
        optional, it allows you to create lookup tables
      conditions: 
        allow decision making in template that only occur if conditions are met
      outputs: 
        once the template is finished, it can produce outputs

  stack: 
    created by template and contains all of the logical resources the template tells it to contain, then builds a physical resource in your account for each corresponding logical resource:
    if you update the stack, it updates the corresponding physical resources:
    if you delete the stack, it deletes the physical resources:

-----------------------------------------------------------------------------------------------:
Cloudwatch:
  collection of services that help you monitor and observe your cloud resources and it allows you to collect logs, metrics, trigger events, and set alarms:
  characteristics:
    Metrics: 
      AWS Products, apps, on premises
    Logs: 
      AWS Products, apps, on premises
    Events: 
      AWS services and schedules 
  Namespace:
    container for monitoring data
  metric:
    time ordered set of data points, ex CPUUtilization
  Datapoint:
    measurement that contains time stamp and value
  dimension:
    separate datapoints for different things or perspectives with the same metric, ex one ec2 instance cpu utilization
  alarms:
    takes an action based on a specific metric

-----------------------------------------------------------------------------------------------:
High Availability vs Fault Tolerance vs Disaster Recovery (KNOW ALL):
  High Availability:
    highly available systems are designed so that when it fails, it is designed to be fixed as quickly as possible. 
    maximizes the system's online time, generally measured in percent of uptime of year, like 99.9%
  fault tolerance: 
    a fault tolerant system operates properly even while faults, which are failures of a system, are present.  Much more expensive than high availability because redundancies are put into place.
  disaster recovery:
    multiple stage of processes are designed to keep the crucial and non replaceable parts of your system safe so when disaster occurs you don't lose anything irreplaceable and can rebuild after disaster

-----------------------------------------------------------------------------------------------:
DNS Fundamentals:
  allows us to translate machine ip addresses into readable language and vice versa:
  discovery service
  www.amazon.com => 104.98.34.131
  it's a huge database and has to be distributed
  zone: 
    a part of the DNS database that corresponds to a domain, think of it as logical domain, ex amazon.com, a zone contains dns namerservers, which contains zone files:
  DNS NAME SERVER:
    nameserver (NS), is a server that hosts the zonefiles:
  zone file:
    physical database for a zone that contains the DNS info for a domain, contains the DNS records which has the mapping of website name to IP address:
  DNS resolver Server:
    located on your client, or router, or server within your IP, finds the NS you are looking for, then queries the server for the zone file:
  ROOT OF DNS, or DNS ROOT ZONE:
    starting point of the DNS lookup
    database hosted on 13 Root servers, 12 large companies manage the servers but not database
    delegates authority to top level domains authoritative servers
  Root Hints file:
    installed on OS'S, pointer to root servers which allows your device to trust root servers

  authoritative: 
    trusted
  delegated: 
    authoritative entity passes trust to other entity so now other entity is trusted 
  top level domains:
    part of domain that is immediately left of last period examples are .com .org .uk
  registries: 
    organizations that are delegated from IANA to help manage top level domains
  registrars: 
    organizations that have relationship with registry that allow you to create a domain registration with them

-----------------------------------------------------------------------------------------------:
DNS Record Types (KNOW ALL):
  A, AAAA: 
    map hostnames to IP, a maps to ipv4, aaaa maps to ipv6
  CNAME: 
    host to host record, lets you create the equivalent of DNS shortcuts, reduces admin overhead by pointing various services to same A record, so you will only have to update single A record, can only point to names not ips
  MX: 
    finds a mail server (SMTP) for a domain
  TXT: 
    allow you to add arbitrary text to a domain allowing further functionality, in some cases used to prove domain ownership
  TTL: 
    numerical value that can be set on DNS records, once a DNS record from zonefile is recovered by NS, records are cached on resolver server for TTL seconds, caching it so resolver server does not have to perform lookup again for TTL seconds

-----------------------------------------------------------------------------------------------:
Route 53:
  allows us to register domains:
  allows us to host zones files on managed nameservers which it provides:
  global resilient service:
  when registering a domain:
    checks with TLD to see if domain is available:
    creates a zone file for registered domain, known as hosted zone:
    allocates 4 nameservers for the hosted zone:
    adds nameserver records to TLD that indicate that the nameservers are authoritative for domain:
  hosted zone:
    zone file in AWS:
    hosted on four managed NS:
    can be public or private linked to VPCs
    stores records, known as recordsets

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
IAM, ACCOUNTS, AND AWS ORGANIZATIONS:
*******************************************
IAM POLICIES:
  JSON document that allows or denies permissions for IAM users, groups, and roles (identities):
  contains 1 or more statements:
  Sid: 
    statement id, optional field, allows you to provide summaryName of statement to quickly identify what it does, 
  Action: 
    can provide a wildcard * which means "all", list of multiple independent actions, or a specific individual action 
    ["service: operationName"]
  Resource: 
    same options above as action 
    ["arnResourceNames"]
  Effect: 
    either allow or deny, controls what aws does if action and resource match

  Every interaction you have with AWS, is a combination of resource you're interacting with and the actions that your attempting to perform on that resource:
  Policy precedence when two policies overlap permissions(ordered in highest precedence vs least:
    1. Explicit deny:
    2. explicit allow:
    3. default deny (implicit), by default aws identities have no access to any resources (besides root):

  Inline Policies:
    applying JSON to each account individually:
    use only when you need a special or exceptional allow or deny for a single user:
  Managed Policies: 
    create policy, then attach to any identity that wishes to use it
    reusable, low management overhead
    aws managed policies: 
      managed and created by AWS
    customer managed policies: 
      created and managed by you specific to your needs

-----------------------------------------------------------------------------------------------:
IAM USERS:
  entities you create in IAM to represent the person or application needing to access your aws resources:
  principal: 
    represents an entity trying to access aws account:
    authenticates via username and password or access keys
  authenticated identity:
    principal that's proved it is who it says it is, aws performs authorization once the authenticated identity tries to access a resource

-----------------------------------------------------------------------------------------------:
IAM ARNS:
  uniquely identify resources within any AWS accounts:
  formats differ depending on what your trying to do:
    arn:partition:service:region:account-id:resource-id
    arn:partition:service:region:account-id:resource-type/resource-id
    arn:partition:service:region:account-id:resource-type:resource-id

  fields are split by a colon, if you see ::: 
    the field does not have to be specified, like for s3 buckets region and account id does not have to be specified because buckets are globally unique:

  ex of similar looking arn accessing different things:
    arn:aws:s3:::catgifs : accesses bucket
    arn:aws:s3:::catgifs/* : objects in the bucket

  EXAM:
    limit of 5000 IAM users per account:
    iam user can be a member of 10 groups
    therefore: 
      if you have a system which requires more than 5000 identities then you cant use one IAM user for each identity, IAM roles and Identity Federation fix this: 

-----------------------------------------------------------------------------------------------:
IAM Roles:
  roles allow user or group to temporarily assume permissions via policies:  
  two types of policies:
    trust policy:
      controls which identities can assume that role:
      can ref identities in same or diff acount: 
      if identity has access to role then temporary security credentials are made available that expire after a given amount of time:
    permissions policy:
      allows or denies permissions for aws resources and services:
      sts:AssumeRole is policy given to access temporary security credentials and perform whatever permissions it was granted
  
  USE ROLES WHEN...:
    generally used when an unknown number of users is uncertain:
    use when a set of credentials are needed, bc we dont want to hard code into code or on service because it is security risk as well as problematic if we ever need to change or rotate keys (roles use temporary security credentials):
    emergency or out of the usual situations
    when you're adding aws into an existing corporate network, external accounts or external identities can't be used to interact with AWS directly, so instead assign the external identities a role for them to interact with aws:
    users > 5000, use Web identity federation that uses roles:
    give users from one account access to resources in another aws account via roles:
    service linked roles:
      a IAM role linked to a specific aws service:
      predefined by a service that provide permissions that a service needs to interact with other AWS services on your behalf, service might create or delete the role or allow you to during setup or within IAM
      you can't delete the role until it's no longer required!
    ListRoles and passRole permissions: 
      when you want a group the ability to use roles but not create them

-----------------------------------------------------------------------------------------------:
Organizations: 
  allows you to centrally manage multiple AWS accounts under one umbrella:
  benefits:
    consolidated billing:
    cost effective with little to no overhead (has volume discounts):
    account governance:
  comprised of:
    member accounts and management account
  oranizational root:
  top level of hiearichal structure, container which contains member accounts, management account, and Organizatinal Units, OUs
  OUs: 
    organization units, contains member accounts or other OUs:
  SCPs:
    service control policies, help you restrict permissions you want everyone in org to follow:
  best practice:
    have single account to handle logins with identities or identity federation, then identities account uses roles to access the other member accounts:

-----------------------------------------------------------------------------------------------:
SCPS:
    policy document that limits what the account can do, including root users.  They do not grant permissions:
    they grant access to what can and cant be allowed, but identities still need identity policies in order to grant that permission.  So an identity needs SCPs to allow and also identity policies to allow access to resources:
    They can be attached to individual accounts, OUs, and the organizational root.  they inherit down the organization tree.
    management account is never affected by SCPs
    Allow list:
      block everything by default, and allow certain services:
      Removes FullAWSAccess policy and add any services that can be allowed access for users for account
      more overhead but more secure
    Deny List:
      allow everything by default via FullAWSAccess policy and deny certain services:
      this is default, when you enable SCPS applied to entire organization:

-----------------------------------------------------------------------------------------------:
CLOUDWATCH LOGS:
  public service
  store, monitor, and access logging data
  many aws integrations
  can generate metrics based on logs via metric filter
  contains log events, log streams, log groups put defn in from building demo

-----------------------------------------------------------------------------------------------:
CLOUDTRAIL:
  logs api actions which effect aws accounts:
  not realtime, there is a delay:
  by default, logs events from one region:
  cloudtrail event:
    logged api call/activity
  event history:
    events stored for 90 days by default, no cost
  Different types of events:
    management, data, and insight events:
    by default only logs management events:
    management: 
      logs management operations performed on resource 
    data: 
      logs the resource operations performed on or within a resource
    insight: 
      logs any unusual activity, errors, or user behavior in your account:
  log file integrity validation feature:
    allows you to determine whether a log file was modified, deleted, or unchanged after cloudtrail delivers it:


-----------------------------------------------------------------------------------------------:
TRAILS:
    customize the cloudtrail service:
    is a regional service, it logs events for the region it's created in:
    two types: 
      one region trail: 
        only ever in region it was created in and logs these events only:
      global trail (all regions trail): 
        collection of trail in every region but managed as single logical trail and logs are sent to a single s3 bucket:

    global service events:
      some global services like IAM, STS, Cloudfront log their events only to one region, need to have this enabled to log these events
    event storage:
      stores events in a definable s3 bucket, can be stored indefinately
      can store events in cloudwatch logs as well to use metric filter or search through data
    organizational trail:
      single management point for every api call made across the organization

-----------------------------------------------------------------------------------------------:
AWS Control Tower:
  allows the quick and easy setup of multi-account environments:
  orchestrates other AWS services to provide its functionality including: 
    Organizations, IAM Identity Center, CloudFormation, Config, and more...
  Landing zone:
    multi-account environment, what most people interact with
    provides SSO/ID federation, centralized logging and auditing
  home region:
    region you initially provision into, always available
  contains two OUs: 
    the foundational OU named Security and the custom OU named sandbox
    foundational: 
      contains two accounts, the Audit Account and Log Archive account
    custom:
      generally used for testing and for account factory
  Account factory:
    automates creating, updating, and deleting AWS accounts as your business needs them
    changes based on templates and implements Cloudformation to provision 
  automates: 
    account provisioning, identities with appropriate permissions, guardrails, account and network standard configurations, and can be fully integrated with a businesses SDLC
  other features:
    guard rails: 
      detect and mandate rules/standards across all acounts
    rule categories:
      mandatory, strongly recommended, or elective
    preventative functional type of rule: 
      stops you from doing things via Organization SCPs
    detective functional type of rule: 
    performs compliance checks via config rules
    dashboard:
    single page oversight of the entire environment
    you can create other OU's and accounts in a real world environment

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
S3:
*******************************************
  private, by default, account root user is only identity which can access the bucket:
  bucket policies:
    form of resource policy:
    resource policy:
      like identity policies, but are attached to resources instead of identities.  You are controlling who has access to that resource: 
      resource policies have principal field, whereas an identity policy does not have one because it is implied
    benefit:
      allow/deny access to identities in same or DIFFERENT accounts (diff than identity policies):
      allow/deny anonymous principals:

  ACLS, Access Control Lists:
    allow a way to provide security on objects and bucket, basically limited resource policies:
    aws recommends you do not use them and instead use bucket policies:
    inflexible and simple permissions because you cannot apply conditions like bucket policies and you cannot apply a certain ACL on a group of objects, you would need to apply several ACLs

  Block Public Access:
    adds a further level of security so that if you do accidentally allow public and anonymous access to bucket via resource policy, then access is still blocked:

  EXAM (KNOW ALL):
    when to use resource policies, identity policies, or ACLs:
      identity: 
        controlling access to multiple resources because not every service supports resource policies
        you have a preference for a single place to control permissions
        working with permissions in same account
      resource policies: 
        managing permissions on a single service
        allow anonymous or cross account access
      ACLs: 
        never, unless you must!

  static website hosting:
    this feature allows access via HTTP:
    website endpoint is created:
    normal access is via AWS APIs
    index and error documents are set
    you can have custom domains via R53 and bucket name matters
    CORS configuration:
      allows you to define a way for client web applications that are loaded in one domain to interact with resources in a different domain:
    use cases for s3 static website hosting:
      offloading:
        store static data from website on s3, have compute service handle dynamic content and then retrieve static data from s3 to save money on storage:
      out-of-band-pages:
        show error or other notification pages on static s3 site in case server is offline or ec2 service as a whole:

  pricing:
    cost to store data on s3 expressed as GB/month fee
    data transfer fee from out of s3 expressed in GB
    cost for requesting data, diff operations have different costs per 1000 operations

-----------------------------------------------------------------------------------------------:
Object Versioning:
  versioning:
    lets you store multiple versions of objects within a bucket.  operations which would modify objects generate a new version.:
  states: 
    disabled, enabled, and suspended
    versioning is by default disabled, once you enable you can never disable again but can suspend and reenable:
  implementation:
    each object has a key(name) and an id attribute:
    when object versioning is disabled: 
      all ids are set to null.  Whenever changes are made to object, the original object is overwritten:
    when object versioning is enabled: 
      ids are given to each object.  Whenever changes are made to object, the original object stays in bucket with same id, while a new object with same key but diff id is generated.  When you access objects from bucket with key the last version is returned.  You can specify which version you want returned with key and id:   
      When deleting, if you specify key and id the corresponding object is deleted(hidden under delete marker) and the current version marker is reset to corresponding object.  If you do not specify id, then all objects are hidden under delete marker.  To undo delete you can delete the delete marker:

-----------------------------------------------------------------------------------------------:
MFA delete:
  enabled in versioning configuration of a bucket:
  MFA is required to change bucket versioning state or to delete versions:
  implementation:
    serial number of MFA + code are passed with API calls

-----------------------------------------------------------------------------------------------:
S3 Performance Optimization:
  problem for Single PUT upload:
    single data stream to s3 requires full restart if any part of stream fails, then upload fails:
    speed and reliability is limited in a limit of 1 stream
    limited to transferring up to 5GB
  solution for Single PUT upload:
    Multipart Upload:
      data is broken up, and each individual part is treated as a single stream, if parts fails, then those individual parts are restarted:
    minimum data size is 100MB

  global transfer of data to s3 buckets:
    without s3 accelarated transfer:
      we have to use public internet to route data from source to destination and doesn't usually take an optimal route:
    with s3 transfer acceleration:
      transfers data to closest edge location, then transfers data from there directly to closest edge location to s3 destination bucket: 
      much faster because the aws network is built for performance between regions:
      by default, disabled, so you have to enable if you wish to use:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
KMS, Key management service:
*******************************************
  allows you to create, store, and manage keys:
  can perform cryptographic operations, including encrypt and decrypt and others:
  keys never leave KMS or by default, region, provides FIPS 140-2 L2:
  supports rotation:
  both symmetric and asymmetric keys
  regional and public service
  can create aliases
  aws managed or customer managed keys:
    aws managed: 
      service default key:
    customer managed keys:
      created explicitly by customer to use in application or service:
      more configurable: 

-----------------------------------------------------------------------------------------------:
KMS KEYS:
  logical key that has ID, data, policy, desc, and state:
  backed by physical key material (backing key) that is managed by KMS service, we use logical key to have KMS generate, import, or encrypt or decrypt data up to 4 kb in size:
implementation:
  identity performs call, provides data, KMS creates key and performs operations
  always returns data only, not key:
provides role separation:
  individual permissions are needed for different operations, like creating key, managing KMS, encrypt, decrypt:
custom key store feature:
  combines controls provided by aws cloudhsm with the integration and ease of use of aws kms:

-----------------------------------------------------------------------------------------------:
DEKs, DATA ENCRYPTION KEYS:
  encrypt and decrypt data on data greater than 4KB in size:
  generated by KMS keys via GenerateDataKey call
  KMS doesn't store the DEK in any way, provides to you or service using KMS to perform crypto operations
  implementation:
    when DEK is created, KMS provides you with two versions of that key:
    plaintext version:
      used to encrypt data then discarded:
    ciphertext version:
      encrypted via KMS key:
      then the encrypted key is stored with encrypted data:
      when decrypting, you send encrypted key to KMS, it decrypts it and then you decrypt data with the key and then discard key

-----------------------------------------------------------------------------------------------:
Key POLICIES AND SECURITY:
  key policy:
    type of resource policy that has to explicitly grant permission to account owner access to KMS:
    every key has one:
  typical way of accessing key:
    broad key policy for account + IAM policies
    granular key policies
    or key policies + grants

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
S3 ENCRYPTION:
  buckets aren't encrypted, objects are:
  each object inside bucket could be using different encryption settings
  provides encryption at rest via:
    client side encryption:
      data is encrypted by client, sent as ciphertext to s3 endpoint, then sent to s3 storage:
      provides you a lot of control, but you do everything when it comes to management:
    server side encryption:
      data is sent as plaintext to s3 endpoint, then encrypted by s3 endpoint, and sent to s3 storage:
      three types available:
        SSE-C: 
          ss encryption with customer provided keys:
        SSE-S3: 
          ss encryption with s3 managed keys, uses AES256 encryption:
        SSE-KMS: 
          ss encryption with KMS keys stored in KMS, best: 

  Bucket Default Encryption:
    allows you to set default encrytion method used if not specified on object level:
    you can set bucket default at object level when uploading object via header x-amz-server-side-encryption or you can set bucket default in the console at bucket level

-----------------------------------------------------------------------------------------------:
S3 OBJECT STORAGE CLASSES (KNOW ALL):
  standard: 
    default
    objects are stored across at least 3 AZs
    no minimums, you are charged per GB for data transferred out and per 1000 requests
    recommended for frequently accessed data which is important and non replaceable data:
    takes ms to retrieve
  standard infrequent access: 
    architecture is mostly the same as s3 standard except that it is cheaper to store data but more expensive to retrieve the data
    has minimum duration of 30 days for storage and min billing size per object, 128KB:
    recommended for long lived data, which is important but where access is infrequent:
    takes ms to retrieve
  one zone-infrequent access: 
    cheaper than s3 standard or s3-IA
    architecture mostly the same as s3-IA, but data is only stored in one AZ:
    recommended for long lived data, which is non critical and replaceable and infrequently accessed
  glacier- instant retrieval:
    architecture mostly the same as s3-IA, but cheaper storage, more expensive retrieval costs, and longer minimums
    recommended for long lived data, accessed once per quarter with fast, millisecond access:
  glacier flexible retrieval: 
    architecture mostly the same as s3-glacier instant retrievel, but cheaper storage, more expensive retrieval costs, and longer access times
    think of objects as chilled state, unable to access immediately:
    cannot be made publicly accessible, and requires a retrieval process
    expedited: 1-5 minutes
    standard: 3-5 hours
    bulk: 5-12 hours
    recommended for archival data, accessed once per year, and minutes-hours retrieval:
  glacier deep archive: 
    cheapest storage option:
    think of objects as frozen state, unable to access immediately:
    architecture mostly the same as s3-glacier flexible retrievel, but cheaper storage, much longer access times
    standard: 12 hours
    bulk: up to 48 hours
    recommended for archival data, accessed very rarely if ever, and hours-days retrieval:
  intelligent tiering: 
    helps you automatically move data to most cost effective storage tier based on access patterns
    tiers are like classes we have frequent access, infrequent access, archive instant access, archive access, and deep archive
    recommended for long lived data with changing or unknown patterns:

-----------------------------------------------------------------------------------------------:
S3 LIFECYCLE CONFIGURATION:
  set of rules that consist of actions that can automatically transition or delete objects in the bucket:
  transition actions: 
    change storage classes of objects after x amount of time:
    can only transition down classes:
  expiration actions: 
    can delete objects, or versions after x amount of time

-----------------------------------------------------------------------------------------------:
S3 REPLICATION: 
  allows you to configure replication of objects between a source and destination s3 bucket:
  cross-region replication, CRR:
  same-region replication, SRR:
  architecture differs depending whether same account or different account
    same account:
      replication rule is applied to source bucket, dest bucket, and specifies IAM role
      the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
    different account:
      replication rule is applied to source bucket, dest bucket, and specifies IAM role
      the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
      additionally, a bucket policy is needed on destination bucket to allow role from different account access:
  s3 replication options:
    all objects or subset:
    storage class, default is to maintain same class but you can change
    ownership, default is source account but you can change
    RTC, replication time control, keeps buckets in sync within 15 mins of each other:
  s3 replication considerations:
    EXAM: 
      not retroactive and versioning for both buckets needs to be on:
      one way replication only from source to destination:
      can handle unencrypted objects, SSE-S3, AND SSE-KMS with extra config:
      source bucket owner needs permissions to objects that will be replicated:
      will not replicate system events, glacier, or glacier deep archive objects:
      delete markers are not replicated by default:
  why use replication?:
    SRR- log aggregation
    SRR- PROD and TEST sync
    SRR- Resilience with strict sovereignty
    CRR- global resilience improvements
    CRR- latency reduction

-----------------------------------------------------------------------------------------------:
S3 PRESIGNED URLS:
  allow you to give an unauthenticated user access to an object inside an s3 bucket using generated credentials from an IAM user in a safe and secure way:
  credentials are included in URL:
  see slides for architectures
  used when offloading media and access to a private s3 bucket needs to be controlled and you don't want to run application servers to broker that access
  EXAM POWERUPS:
    you can create a URL for an object you have no access to, (which will also have no access lol)
    when using the URL, the permissions match the identity's which generated it CURRENTLY, so if they change so do the URL's permissions:
    do not generate URL's with a role!  URL stops working when temporary credentials expire:

-----------------------------------------------------------------------------------------------:
S3 SELECT AND GLACIER SELECT:
  SQL-like statements that allow you to retrieve parts of object rather than whole object:
  formats of data: 
    CSV, JSON, Parquet, BZIP2 compression for CSV and JSON
  why use?:
    you might want to retrieve only parts of object to save on computing costs and for faster performance:

-----------------------------------------------------------------------------------------------:
S3 EVENTS:
  allows you to create event notifications when events occur in a bucket:
  can be delivered to SNS, SQS, and Lambda Functions, all need resource policy allowing s3 principal access:
  Object created events
  Object Delete events
  Object restore events
  Replication events
  EventBridge:
    alternative and supports more types of events and more services:
    would use this instead of s3 events unless you had a specific reason not to do so:

-----------------------------------------------------------------------------------------------:
S3 ACCESS LOGS:
  Enable logging on source bucket and sends logs to target bucket:
  s3 log delivery group reads permissions you set, the bucket acl on target group allows access to s3 log delivery group
  use cases:
    security functions
    access audits for employees
    customer access patterns
    understand charges

-----------------------------------------------------------------------------------------------:
S3 OBJECT LOCK:
  group of related features that enables Write-Once-Read-Many (WORM) architecture, which means no deletes, no overwrites:
  Requires versioning, individual versions are locked:
  Bucket can have default object lock settings
  Retention Period and Legal Hold: 
      Retention Period:
        specifies days and years which object is locked:
        compliance mode:
          can't be adjusted, deleted, overwritten for duration of retention period:
        governance mode:
          special permissions can be granted allowing lock settings to be adjusted:
      Legal Hold:
        you set legal hold to be on or off, there is not retention period:
        no deletes or changes until removed

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
VPC SIZING AND STRUCTURE:
  overview:
  what size should the VPC be?
  are there any networks we can't use, do not overlap networks of VPC's, cloud, on-premises, partners, and vendors
  try to predict the future situation
  vpc structure: tiers and resiliency availability zones
  tiers: 
    seperate application components and allow different security to be applied:

  VPC minimum /28 (16 IP), maximum /16 (65536)
  Personal preference for the 10.x.y.z range
  avoid common ranges, like 10.1-10.15
  reserve 2+ networks per region being used per account
  in our scenario, we have 3 US, 1 Europe, 5 australia, x 2 and assuming max 4 accounts = 40 IP
  VPC sizing in depth:
    how many subnets will you need?
    how many IPs total?  How many per subnet?
  formula?:
    a subnet is located in one AZ, so determine how many AZs you will be using, he mentions by default he uses 4, 3 for AZs and one spare
    each tier has its own subnet in each AZ, by default 4 tiers, web, app, db, and spare so now we have 16 subnets by default

-----------------------------------------------------------------------------------------------:
CUSTOM VPCS:
  regional service, operates from all of the AZs in that region:
  allows you to create isolated networks
  allow nothing IN or OUT without explicit configuration:
  flexible configuration, simple or multi-tier
  hybrid networking to other cloud and on premises
  default or dedicated tenancy, allows resources created inside VPC to be provisioned on shared or dedicated hardware
  can use public ipv4 and private ipv4 addresses, by default they use private addresses:
  public addresses are used when you want resource to communicate with public internet or aws public zone:
  allocated one primary private IPv4 CIDR block, configured when you create VPC, optional secondary ipv4 blocks, think of it as a VPC has a pool of private addresses and optionally it can use public addresses
  optional single assigned ipv6 /56 CIDR block
  can have many in one region

  fully featured DNS:
    provided by r53
    vpc address is base IP + 2
    enableDnsHostnames: 
      gives instances DNS names, if set to true then instances with public ip addresses get public DNS hostnames
    enableDnsSupport: 
      enables DNS resolution in VPC, indicates whether enabled or disabled

-----------------------------------------------------------------------------------------------:
VPC subnets:
  a subnetwork of a VPC, within a particular AZ:
  1 subnet can have only one AZ, one AZ can have zero to many subnets:
  subnets can communicate with other subnets in the VPC with proper configurations:
  AZ resilient: 
  allow you to add structure, functionality, and resilience to VPCs
  ipv4 CIDR is a subset of the VPC CIDR, and cannot overlap with any other subnets within VPC
  optional ipv6 CIDR
  5 reserved ip addresses:
    network address: first starting address in available range
    network + 1: address used for VPC router
    network + 2: address used for Reserved DNS
    network + 3: address used for future requirements
    broadcast address: last IP in subnet

-----------------------------------------------------------------------------------------------:
DHCP OPTIONS SET: 
  dynamic host configuration protocol:
    configuration object applied to VPC that allows computing devices to receive IP addresses automatically:
  one applied to a VPC at one time
  if you want to change settings you need to create a new one via:
    auto assign public ipv4:
      automatically assigns public ipv4 address to resource that has private ipv4 address
    auto assign ipv6:
      automatically assigns ipv6 address to resource

-----------------------------------------------------------------------------------------------:
VPC ROUTING AND INTERNET GATEWAY:
  internet gateway: 
    region resilient gateway attached to a vpc:
    1 VPC = 0 or 1 IGW, if 0 then it is entirely private:
    1 IGW = 0 or 1 VPC:
    Runs from within AWS Public zone, or "border between public zone and VPC"
    Gateways traffic between the VPC and internet or between VPC and public aws zone
    Managed by aws for performance
    only allows public services to access outside VPC, does not work with private service so need to implement with NAT gateway if you want private service inside vpc to access outside its VPC:
      maintains mappings of ipv4 public addresses for private instances within VPC:
        instances that have ipv4 addressing enabled, have only private ipv4 address
        the igw maintains a public ipv4 address mapping to private ip address
        when private instance sends packet, the igw replaces the private ip address with public ip address
      ipv6 addresses for private instances are natively routable:
        instances that have ipv6 addressing enabled, have both private ipv6 address and public ipv6 address
        igw passes traffic for ipv6, it doesn't need to do mapping:

-----------------------------------------------------------------------------------------------:
VPC ROUTER:
  routes traffic between subnets:
  every vpc has a vpc router, highly available:
  in every subnet, the router uses network+1 address
  controlled by route tables, each subnet has one only
  a VPC has a main route table, it is default table for subnet if there hasn't been an explicit association with custom table, when subnet does get associated with custom table the main route table is dissasociated:
  route table:
    basically just a list of routes, matches destination ip address on packet with destination field in route table:
    if multiple routes match, the most specific, higher prefix route is selected:
  target field:
    point to an aws internet gateway or local, which means destination is within vpc itself

-----------------------------------------------------------------------------------------------:
BASTION HOST/ JUMPBOX:
  is ec2 instance in a public subnet inside a VPC that is generally used as an entry point for private only VPCs:
  historically was only way to implement entry point for private only VPCs but now there are better alternatives:
  allow incoming management connections and then access internal VPC resources

-----------------------------------------------------------------------------------------------:
STATEFUL VS STATELESS FIREWALLS:
  things to think about when dealing with firewall rules:
    inbound and outbound:
      request and response
      inbound and outbound rules can be both requests and responses, just depends on client/server relationship
    stateless firewalls:
      need rule for each request/response, 2 total:
      request is always to well known port, and response is always to ephemeral port and since it does not have state, you have to allow traffic to all ephemeral ports
    stateful firewall:
      recognize connection and treats each request/response as 1 rule, 1 total rule allows/denies request and response is automatically allowed/denied:
      lower admin overhead

-----------------------------------------------------------------------------------------------:
NACL, NETWORK ACCESS CONTROL LISTS:
  traditional stateless firewall available within aws vpc:
  acts as a firewall for associated subnets, each subnet has one only:
  NACLS filter traffic crossing the subnet boundary inbound or outbound:
  connections between things inside subnet are not affected by NACLs:
  contain rules grouped into INBOUND and OUTBOUND:
  matches first rule found in order then stops processing, so first rule matched takes effect
  default NACL is implemented in a VPC with all traffic allowed
  you cannot reference any logical resources, only IPS/CIDR, ports, and protocols, explicit allows and denies:
  good practice is to implement together with security groups to allows traffic with security groups and deny traffic with NACLS:

-----------------------------------------------------------------------------------------------:
VPC SECURITY GROUPS:
  traditional stateful firewall:
  associated with individual resources and control traffic in and out of those resources:
  if request is allowed, then response is automatically allowed 
  no explicit deny, only allow or implicit deny by not allowing:
  can't block specific bad actors
  support IP/CIDR rules and logical resource references, which includes other security groups and itself, both help scale, you could use self reference to allow intra app communication:
  are attached to individual EC2 instances, network interfaces, or load balancers:

-----------------------------------------------------------------------------------------------:
NETWORK ADDRESS TRANSLATION, NAT, AND NAT GATEWAYS:
  NAT, Network Address Translation: 
    set of processes that remap source or destination IPs
  static nat: 
    what igw does
  ip masquerading: 
    hiding CIDR blocks behind one IP, helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse, takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table
  how to implement IP masquerading in AWS, will refer to ip masquerading as nat:
    nat gateway:
      allows private services in private subnet to access internet gateway:
      provision nat gateway into public subnet with route table, diff route table for private subnet, so that packets sent from private service go to nat gateway, then get sent to internet gateway:
      uses elastic IP (static ipv4 public address)
      az resilient service, deploy a nat gateway in each az for region resilience:
      2 charges, one per hour and another per gb of data consumed
    nat instance:
      implementing ec2 instance as NAT gateway, not really all that important to know because nat gateways are mostly used, however for exam know that if you do implement nat instance, you need to disable source/destination checks:
    differences between two:
      nat instance is managed by you, so you can do other things than just performing nat service, with nat gateway you can only do nat service:
      nat gateway is highly available within az, nat instance isn't:
      nat gateways only support NACLS, while nat instances support security groups and NACLS:
    ipv6:
      NAT isn't required for ipv6 and nat gateways don't work with ipv6:
      all ipv6 addresses in aws are publicly routable, the igw works with all ipv6 ips directly
      you can enable default ipv6 address route + igw for bi directional connectivity or with egress only igw for outbound only:

-----------------------------------------------------------------------------------------------:
VIRTUALIZATION 101:
  virtualization: 
    process of running more than one operating system on a piece of physical hardware or server:
    user mode and privileged mode
    application needs to make calls to OS(kernel)(privileged state) that makes calls to hardware, apps cannot make system calls directly to hardware:

  virtualization allows multiple os to make calls to hardware:
  emulated virtualization:
   host os, which runs in privileged mode, has hypervisor, means it has knowledge of the different guest os and can allow privileges 
  virtual machine:
    container that contains applications and guest operating system, logical hardware is allocated by hypervisor which has access to physical hardware
  binary translation:
    guest os not modified, whenever the virtual machine makes call to physical hardware, the hypervisor steps in and makes the call to physical hardware
  Para-virtualization:
    similar architecture to emulated virtualization, but guest os is modified, instead of making calls to physical hardware, it makes calls to hypervisor 
  hardware assisted virtualization:
    hardware itself is virtualization aware,
    guest os make calls to physical hardware and hardware does not halt execution, but redirects these calls back to hypervisor
  SR-IOV:
    hardware is split up into several physical hardware pieces available for guest os so no translation has to happen by hypervisor
  Enhanced Networking:
    this process occurs in EC2 with network cards and improves latency, less cpu usage

-----------------------------------------------------------------------------------------------:
EC2 ARCHITECTURE AND RESILIENCE:
  ec2 instances are virtual machines running on the ec2 hosts, which is physical hardware 
  ec2 hosts are either shared hosts or dedicated hosts
  az resilient, host runs on a single AZ
  instance store: 
    local storage on ec2 host
  storage networking:
    remote storage can attach to elastic block store service
  EBS, elastic block store: 
    storage device, a volume, that can be attached to or removed from your instance,
    data persists when instance is not running, 
    can only be attached to one instance in same AZ,
    recommended for quickly accessible data, running a database on an instance, long term data storage
  ec2 instances remain on same host unless host fails or host is stopped and started:
  use cases:
    when you've got a traditional os+application compute
    long-running compute needs:
    server style applications, that are waiting for incoming connections
    either bursts or steady state load requirements
    monolithic application stacks
    migrated application workloads or disaster recovery
    tends to be default compute service within aws:

-----------------------------------------------------------------------------------------------:
EC2 INSTANCE TYPES:
  characteristics:
    raw resources you get, cpu memory, local storage capacity, and type
    resource ratios
    storage and data network bandwidth 
    system architecture/vendor
    additional features and capabilities, like gpus, fpgas

  five main categories:
    general purpose:
      default, diverse workloads, equal resource ratio
    compute optimized: 
      have higher cpu than memory ratio
      media processing, HPC, scientific modeling, gaming, machine learning,
    memory optimized:
      higher higher memory than cpu ratio
      processing large in memory datasets, some database workloads
    accelerated computing:
      have additional features and capabilities
      hardware gpu, FGPAs
    storage optimized:
      large amounts of super fast local storage
      sequential and random IO, scale out transacitonal databses, data warehousing, elasticsearch, analytic workloads

  Decoding EC2 types:
    check out slide:
    [instance family] [instance generation] [additional capabilities]. [instance size]

-----------------------------------------------------------------------------------------------:
CONNECTING TO EC2 (KNOW ALL):
  ec2 instance connect:
    use to connect to linux instances using SSH via CLI or ec2 instance connect API:
    you do not need to share and manage ssh keys, instead uses identity policy permissions:
    your browser connects to aws, then it redirects to connect to ec2 instance
    using the GUI can only connect to public ec2 instances, can connect to private using CLI
  ec2 ssh client:
    standard way to connect to linux instances via SSH:
    need to manage ssh access key:
  session manager:
    lets you manage instances via interactive one click browser based shell or through aws cli:
    not a valid way to log into linux ec2 instance:
    no need for ssh or bastion hosts, aws fully manages the process via secure tunnel:

-----------------------------------------------------------------------------------------------:
STORAGE REFRESHER:
  3 main categories of storage available in AWS:
    block storage: 
      Volume presented to the OS as a collection of blocks.  no structure provided. mountable. bootable. :
      implemented via SSDs, (physical storage) or by logical volumes backed by SSDs:
      ideal for applications that require low-level access to storage, such as databases, operating systems, and booting storage:
    file storage:
      presented as a file share. has structure. mountable. not bootable.:
      can be accessed by servers and virtual machines using file protocols such as NFS(linux) or SMB(windows):
      ideal for applications that require shared access to files, such as file servers or content management systems:
    object storage:
      collection of objects.  no structure (flat). Not mountable.  Not bootable.:  
      typically used for applications that require highly scalable, durable, and cost-effective storage for large amounts of unstructured data, such as multimedia content, backups, and archives:

  storage performance:
    IO, or block size:
      size of blocks of data that your writing to disk
      think size of wheels of car
    IOPS:
      number of io operations the storage system can support in a second
      think speed of engine of car, power
    throughput: 
      rate of data storage system can store on a particular piece of storage, mb/sec
      think average speed of car
      throughput = io x iops:

-----------------------------------------------------------------------------------------------:
EBS, ELASTIC BLOCK STORE:
  provides block storage that can be encrypted using KMS:
  instances see block device and create file system on this device
  provisioned in one az, resilient in that AZ:
  attached to one ec2 instance over a storage network, or multiple but have to be in same AZ:
  persistent, they can be detached and reattached to same or different instances:
  you can perform backups, or snapshots, of data into s3.  You can migrate the data and create volumes from that snapshot:  
  can be provisioned with different physical storage types, different sizes, and different performance profiles
  billed based on gb-month, and in some cases performance

-----------------------------------------------------------------------------------------------:
EBS VOLUME TYPES GENERAL PURPOSE SSD:
  up to 16000 IOPS per volume:
  GP2:
    default general purpose ssd storage:
    high performance storage for fairly low price
    can be as small as 1GB or as large as 16TB
    io credits and baseline performance found on slide
    used for boot volumes, low latency apps, and dev and test environments:
  GP3:
    cheaper than GP2:
    io credits and baseline performance found on slide
    benefits of GP2 and IO1:
    used for boot volumes, low latency apps, and dev and test environments, virtual desktops, medium sized single instance databases
    up to 64,000 IOPS per volume:

EBS VOLUME TYPES PROVISIONED IOPS SSD:
  designed for super high performance situations, IOPS can be adjusted independently of size:
  consistent low latency:
  faster than gp3
  there is a max per instance performance, found on slide
  io1:
  io2:
    up to 256,000 IOPS per volume
  io2 block express:
    used for high performance, latency sensitive workloads, IO intensive NoSQL and relational databases.  also used when you have small volumes and you need high performance:

EBS VOLUME TYPES HARD DISK DRIVE (HDD) -BASED:
  have moving bits, slower but good for specific situations
  used for larger, cheaper storage that is frequently accessed for throughput intensive sequential workloads.  think big data, data warehouses, and log processing:
  cannot boot
  st1:
    fast hard drive, not agile
    sequentially accessed data
  sc1:
    cold HDD
    designed for infrequently accessed workloads where cheaper storage is priority:

-----------------------------------------------------------------------------------------------:
INSTANCE STORE VOLUMES:
  provide block storage:
  physically connected to one EC2 host:
  instances on that host can access them:
  highest storage performance in aws by much higher levels:
  included in instance price
  attached at launch of ec2 instance:
  number of and size of instance store volumes depend on type of instance:
    EXAM:
      local on ec2 host
      add at launch only
      data lost on instance move, resize, hardware failure, or when ec2 stops and starts because aws allocates new host: 
      data not lost on restart (instance stays on same host):
      price included in instance:
      temporary data

CHOOSING BETWEEN INSTANCE STORE AND EBS:
  EBS: persistant data :
  EBS: higher resilience :
  EBS: storage isolated from instance lifecycle
  depends: when you need resilience and app supports in built replication of data, instance stores are an option because you can have many ec2 instances with instance store to provide the resilience and the data is persisted within app
  depends: high performance needs
    RAID0 + EBS: max number of IOPS per instance is 260,000 IOPS, used by combination of high number of volumes and efficient architecture
  instance store: super high performance needs, more than 260,000 IOPS to millions:
  instance store: when cost is a primary concern because no cost for instance store:

-----------------------------------------------------------------------------------------------:
EBS SNAPSHOTS:
  incremental volume copies to s3, improves az resilience to region resilient:
  the first is a full copy of data on the volume, future snaps are incremental:
  volumes can be created or restored from snapshots, and can be copied to another region :
  billing: GB/month of used data, not allocated data :

  when you create new EBS volume w/out snapshot: 
    full performance immediately
  when you create new EBS volume w/ snapshot: 
    restores lazily, the blocks are fetched gradually:
    to force blocks to be fetched immediately you can implement a forced read of all data immediately with CLI tools or OS tools or you can implement Fast snapshot Restore(FSR):
  Fast snapshot restore, FSR: 
    immediately restores data, up to 50 snaps per region
    extra cost

-----------------------------------------------------------------------------------------------:
  EBS ENCRYPTION:
    by default, no encryption is applied:
    provides at rest encryption for snapshots and ebs volumes:
  implementation:
    KMS generated encrypted DEK key to be stored on instance
    Whenever wanting to read/write data, the volume makes request to KMS, KMS sends response with decrypted DEK key which is stored on the ec2 host and is used to encrypt and decrypt data stored on instance volume.  Whenever instance moves hosts, the key is discared
    Instance retains encrypted DEK key, and whenever wanting to read/write data repeats step 2.  same implementation for snapshots. 
  EXAM POWER-UP:
    accounts can be set to encrypt ebs volumes by default, you can set a default KMS key or choose a custom KMS key to use:
    each volume uses 1 unique DEK: 
    snapshots and future volumes use the same DEK:
    can't change a volume to not be encrypted once it's encrypted:
    OS isn't aware of the encryption, so there is no performance loss:

-----------------------------------------------------------------------------------------------:
NETWORK INTERFACES, INSTANCE IPS, AND DNS:
  every ec2 instance has one network interface called the primary ENI (elastic network interface):
  An ENI can have its own network settings, including IP addresses, a MAC address, and a set of security group rules, which control the traffic that is allowed to and from the ENI:
  optionally, you can attach one or more secondary ENIs (everything needs to be in 1 AZ), only difference between primary and secondary ENI is that you can detach secondary ENIs and attach to diff instances:
  on the eni:
    mac address
    primary ipv4 private ip
    0 or more, secondary IPs
    0 or 1 public ipv4 address: 
      changes whenever instance is restarted because allocated new host
    1 elastic ip per private ipv4 address: 
      (elastic ips are like public ipv4 addresses but different bc normally you can only have 1 public address per interface while you can have many elastic ips 1 per private ipv4 address)
    0 or more ipv6 addresses:
    security groups
    source/destination check

  elastic ip addresses: 
    replaces the public ipv4 address:
    elastic ips are like public ipv4 addresses but different bc normally you can only have 1 public address per interface while you can have many elastic ips bc 1 elastic ip is associated per private ipv4 address on primary interface or second interface:
    allocated to your aws account
  EXAM POWER-UPS:
    EIPs have a cost if not associated with anything
    EIPs are per account, per region:
    secondary eni + MAC = licensing, and you can then move licenses between ec2 instances
    you can apply different security groups on multiple interfaces to enforce different access permissions for different IP ranges
    the OS doesn't see public ipv4, the private ip is mapped to public via NAT
    0 or 1 public ipv4 address changes whenever instance is restarted, or started and stopped because allocated new host, if you don't want dynamic address then use elastic ip instead:

-----------------------------------------------------------------------------------------------:
AMI, Amazon Machine Image:
  template of an instance configuration that allows us to provision instances:
  images that help us launch ec2 instance
  you can create an AMI from an ec2 instance you want to template
  aws, community provided (marketplace), or you can create your custom AMI
  regional so unique id provided per region
  has permissions:
  AMI Lifecycle:
    launch: where you use an ami to launch ec2 instance
    configure: add some customization
    create image: create an ami with ec2 default configs + your customization, contains permissions and block device mapping, which references the snapshots created of ebs volumes 
    launch: new instance is launched with resources and configs, snapshots are used to create corresponding ebs volumes in region
  EXAM POWER-UPS:
    AMI = one region, only works in that one region:
    AMI Baking = creating an AMI from a configured instance + application:
    An AMI can't be edited, you have to create a new one
    can be copied between regions:
    permissions by default allow only your account to access:

-----------------------------------------------------------------------------------------------:
EC2 PURCHASE OPTIONS (LAUNCH TYPES):
  ON-DEMAND:
    instances of different sizes run on the same ec2 hosts, they are isolated but run on shared hardware:
    per second billing while an instance is running.  also some resources like disk are always billed, regardless of instance state :
    characteristics:
      predictable pricing, no upfront cost, no discounts:
      default:
      good for short term, unknown workloads, and apps which can't be interrupted:

  SPOT:
    selling unused ec2 host capacity for up to 90% discount, the spot price is based on the spare capacity at a given time:
    implementation:
      customers set max price they are willing to pay.  if capacity is available and price aws is charging is lower than max price, they are allocated capacity.  However, once the price goes above the customers max price, the customers capacity is terminated:  
    characteristics:
      never use spot for workloads which can't tolerate interruptions, good for anything which is stateless or anything which can be rerun:
      if you need burst capacity needs
      non time critical:

  RESERVED INSTANCES:
    Standard Reserved:
      committment for long term consumption of ec2 resources:
      unused reservations are still billed:
      reduced or no per second price for corresponding on demand instance
      can be regional or zonal reservation or neither
      commitments:
        1 year or 3 year terms:
        no upfront, partial upfront, and all upfront.  first two reduce /sec fee, while last eliminates
      characteristics:
        known usage, need consistent access to compute, long-term: 

    Scheduled Reserved:
      ideal for long term usage which doesn't run constantly:
      you specify frequency, duration, and time:
      minimum of 1200 hours per year for 1 year:

    Convertible Reserved:
      allow you to exchange for another convertible reserved instance of a different instance family, OS types, and tenancies:
      
  DEDICATED:
    Dedicated Hosts:
      hosts that are allocated to you entirely:
      you pay for the host, no instance charges:
      characteristics:
        host affinity, links instances to hosts
        socket and core licensing requirements
        when you have strict regulatory or data requirements:

    Dedicated Instances:
      instances that are allocated to you entirely:
      host is not shared between you and other customers:
      you do not pay for host, pay for flat hourly charge and for dedicated instances themselves: 
      characteristics:
        when you have strict regulatory or data requirements:

  CAPACITY RESERVATIONS:
    from highest to lowest priority:
      reserved purchases
      on demand 
      spot
    types:
      regional reservation: 
        allows you to launch instance in any AZ in region but does not reserve capacity within an AZ:
        cheaper than on demand
      zonal reservation: 
        only apply to one AZ and reserves capacity within AZ:
        cheaper than on demand
      on-demand capacity reservation: 
        can be booked to ensure you always have access to capacity in an AZ but not discounted price and you pay even if you dont use it:
        no commitment requirements

  EC2 SAVINGS PLANS:
    hourly commitment for a 1 or 3 year term:
    two types:
      general compute $ amounts: 
        ex, 20$ per hour for 3 years, for ec2, fargate, and lambda, saves you money because savings plan rate is lower than on demand rate for these products
      specific ec2 savings plan: 
        you pick size and OS

-----------------------------------------------------------------------------------------------:
INSTANCE STATUS CHECKS AND AUTORECOVERY:
  2 status checks per instance:
    system status: 
      things like loss of system power, ec2 host or service failures
    instance status: 
      things like corrupted file system, incorrect instance networking, os kernel
  autorecovery:
    allows you to automatically recover from failed status checks:
    moves instance to a new host, keeps same configs and reperforms status checks:
  termination protection:
    in instance settings, you can enable termination protection which allows you to protect your instance from being accidentally terminated, or terminated by someone without the permission

-----------------------------------------------------------------------------------------------:
HORIZONTAL VS VERTICAL SCALING:
  Vertical scaling:
    allows you to resize an ec2 instance:
    benefits:
      no application modification required:
      works for all applications, even monoliths
  disadvantages:
    each resize requires a reboot which leads to slower rection times and disruptions
    larger instances often carry a money premium that is more than linear the bigger you get:
    there is an upper cap on performance due to set limit on instance size:

  horizontal scaling:
    allows you to change number of instances:
    how it works:
      a copy of application is stored on each instance and instances work in parallel via load balancer:
      sessions are everything: 
        requires application support or off-host sessions:
        off host sessions:
          session is stored externally somewhere else and servers are stateless
    benefits:
      no disruption when scaling:
      no real limits to scaling:
      often less expensive than vertical scaling:
      allows you to be more granular when you scale with addition or subtraction of smaller instances:
    
-----------------------------------------------------------------------------------------------:
INSTANCE METADATA:
  ec2 service that provides data to instances:
  accessible inside all instances, anything running inside instance can query the metadata:
  no authentication nor encrypted, can and does get exposed:
  http://169.254.169.254/latest/meta-data/ is IP address to access instance metadata:
  ec2-metadata tool allows us to easily query data via cli
  categories:
    environment
    networking
    authentication
    user-data

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
CONTAINERS:
*******************************************
solve this problem with virtualization: 
  guest os consumes a lot of the available resources that's allocated to each virtual machine because same os is copied for each instance:
containers:
  allows an application to run within an isolated environment:
  runs as an application running within host os to solve problem with virtualization:
  allows us to run many more applications on top of same hardware than when using virtualization, saves so much space and scales really well:
  portable, isolated, very consistent as they always run as expected:
  ports are exposed to the host and beyond
  container engines are like shared guest os and the containers are memory for application and runtime environment: 
  application stacks can be multi container
  architecture:
    dockerfile: 
      file used to build docker images. each step creates file system (fs) layers
      stacks of docker images
    docker image: 
      are created from a base image or scratch
      images contain readonly layers, changes are layered using a differential architecture that is comprised of fs layers
    docker container: 
      running copy of a docker image (fs layers), with an addition read/write layer
    read/write layer: 
      allows container to run, any applications store changes in this layer and containers differ only in this area, have same base fs layers
    container registries: 
      hub or registry of container images

-----------------------------------------------------------------------------------------------:
ECS CONCEPTS, ELASTIC CONTAINER SERVICE:
  allows you to use containers running on infrastructure that aws partially or fully manages to reduce admin overhead of using docker container:
  clusters run in ec2 mode or fargate mode:
    ec2 mode: 
      uses ec2 instances to run containers
    fargate mode: 
      serverless way of running docker containers
  ecs container engine maintains clusters
  ecs allows you to create cluster
  cluster: 
    where your containers run from, tasks or services get deployed into cluster:
  container definition: 
    tells ecs information about single container, including pointer reference and which port it uses 
  task definitions: 
    represents a self contained application as a whole, contains one or more container definitions
    store resources used by application like cpu, memory, compatibility, task roles
  task role:
    iam role the tasks can assume to interact with aws resources :
    task roles are BEST PRACTICE to give ecs containers permissions to access aws services and resources:
    store sensitive data in aws secrets manager or aws systems manager parameter store, and access via execution task role, taskRoleArn: 
  service definition: 
    defines a service, service is how for ecs we can define how we want task [definitions] to scale, or availability, resiliency, copies, capacity, etc

-----------------------------------------------------------------------------------------------:
ECS EC2 MODE:
  uses ec2 instances to run containers (clusters):
  ecs cluster is created and runs within VPC, benefits from many AZs within region
  auto scaling group helps provision instances
  you generally manage ec2 instances (container host), availability, capacity:
  ecs will handle tasks and service definitions:

-----------------------------------------------------------------------------------------------:
ECS FARGATE MODE: 
  serverless way of running docker containers:
  aws provides more management of overhead, manages Fargate shared infrastructure:
  each cluster runs on Fargate, and ecs tasks and services are injected into VPC and given ENI to access from VPC or public if vpc config allows:
  ecs will handle tasks and service definitions
  you pay for resources you consume

-----------------------------------------------------------------------------------------------:
CHOOSING BETWEEN EC2 VS ECS EC2 MODE VS ECS FARGATE MODE:
  ec2: if you use containers already
  ec2 mode: large workload, price conscious over effort(higher admin overhead)
  Fargate: large workload, low admin overhead
  Fargate: small/burst/batch/periodic workloads
  
-----------------------------------------------------------------------------------------------:
ECR, ELASTIC CONTAINER REGISTRY:
  managed container image registry service:
  like docker hub but for aws:
  each aws account has a public and private registry
  each registry can have many respositories and each repository can contain many images, think github
  container images can have several unique tags within your repository
  public repository: 
    anyone has read access but write requires permissions
  private registry: 
    permissions are required for any access
  benefits:
    integrated with IAM
    image scanning via basic and enhanced scan via aws inspector
    provides real time metrics based on authentication, pushing and pulling
    logs all api actions via cloudtrail
    delivers events to eventbridge
    you can replicate cross region and cross account

-----------------------------------------------------------------------------------------------:
KUBERNETES, 101:
  open source container orchestration system that allows you to automate the scaling, deployment, and management of containerized applications:
  like docker but automated:
  kubernetes cluster:
    structure on slide
    key components:
      cluster: a deployment of kubernetes, management, orchestration, and service access
      node: resources, pods are placed on nodes to run
      pod: 1+ containers, smallest unit in kubernetes, often 1 container, 1 pod, nonpermanent
      service: abstraction, service running on 1 or more pods, its what you interact with like application
      job: ad hoc, creates one or more pods until completion
      ingress: how external entity accesses a service, ex would be ingress->routing->service->1+ pods
      ingress controller: used to provide ingress, software that allows hardware to ingress
      important to uses stateless architecture for data stored on pods since they are temporary, 
      persistant storage, pv: volumes whose lifecylces lives beyond any 1 pod using it

-----------------------------------------------------------------------------------------------:
ELASTIC KUBERNETES SERVICE, EKS:
  aws managed kubernetes, open source and cloud agnostic:
  it can run on aws, outposts, eks anywhere, or eks distro:
  control plane scales and runs on multiple AZs:
  integrates with AWS services like ECR, ELB, IAM, VPC
  eks cluster: eks control plane and eks nodes
  etcd: distributd across multiple AZs
  Nodes: can be self managed, managed node groups, or fargate pods
  for storage: can use EBS, EFS, FSx Lustre, FSx for NetApp ONTAP
  architecture: on slide

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
ADVANCED EC2:
*******************************************
  bootstrapping:
    process of running scripts or other configurations when an instance is first launched:
    allows ec2 build automation once launched
    enabled via user data, accessed via 169.254.169.254/latest/user-data :
    anything in user data is executed by the instance OS:
    ec2 doesn't interpret, it passes the data blindly, the os needs to understand the user data and execute, 
    can be done in UI when launching ec2 instance or you can bootstrap via including user data inside cloudformation template that launches ec2 instance:
  architecture:
    ec2 bootstrapping slide
    key points:
      user data is not secure, don't use it for passwords or long term credentials:
      executed only once at launch time:
      user data can be modified when instance stopped

  Boot-time-to-service-time: 
    the time period to provision ec2 instance and when instance is available to use
    generally takes minutes
    ami baking: 
      removes post launch time by putting configurations into AMI
    post launch time:
      configuring the instance after launch before it is available for service
    optimal way:
      ami bake in the installation part, and bootstrap the final configurations:

-----------------------------------------------------------------------------------------------:
ENHANCED BOOTSTRAPPING WITH CFN-INIT:
  allows you to pass complex bootstrapping instructions into ec2 instance, much more complex than user data instructions:
  more powerful because it tells ec2 desired state rather than procedural script like user data and also can run more than once, ex watching for updates to metadata then performing update:
  cfn-init: 
    helper script installed on ec2 os: 
    simple configuration management system that tells ec2 desired state, then ec2 performs functionality required to do so
  aws::CloudFormation::Init: 
    where cfn-init configuration is stored in cfn template
  cfn CreationPolicy:
    added to a logical resource inside a cfn template, it does not allow resource to move into create complete status until cfn receives signal:
  cfn Signals:
    allows ec2 to send confirmation signal to cfn stack if configuration was created successfully:

-----------------------------------------------------------------------------------------------:
EC2 INSTANCE ROLES:
  roles that an instance can assume and anything running within instance assumes, like CLI :
  should always be used rather than adding access keys into instance:
  InstanceProfile: 
    wrapper around an IAM role that allows permissions to get inside of instance:
    when you use UI it abstracts the process of wrapping iam instanceProfile but when you use CLI or CFN you need to implement instanceProfile manually

  temp credentials are inside instance meta-data:
    http://16.254.169.254/latest/meta-data/iam/security-credentials/role-name
    automatically rotated and always valid
-----------------------------------------------------------------------------------------------:
SSM PARAMETER STORE:
  storage for configuration and secrets:
  allows 3 different types of parameters: 
    String, StringList, SecureString:
    ex: license codes, database strings, full configs and passwords:
  changes can create events:
  hierarchies and versioning
  plaintext and ciphertext, integrates with KMS to encrypt sensitive info and other services need access via IAM permissions 
  public parameters, like latest AMIs per region

-----------------------------------------------------------------------------------------------:
SYSTEM AND APPLICATION LOGGING ON EC2:
  cloudwatch by default monitors external metrics:
    network utilization, cpu utilization, disk i/o (reads and writes) and disk i/o performance:
  Cloudwatch Agent (software) is required to capture:
     system and application logs (capturing metrics from within os), configurations, permissions, memory utilization, disk swap utilization, disk space utilization, page file utilization, and cpu usage:    
  agent configuration: 
    configures which logs and metrics we want to capture
  permissions: 
    roles grant permissions for instance, (and everything running inside of it), the permission to access kms and write to cloudwatch logs

-----------------------------------------------------------------------------------------------:
EC2 PLACEMENT GROUPS:
  placement groups allow you to influence placement of ec2 instances within AZ:

  cluster: 
    pack instances close together:
    used when you want to achieve the highest possible performance, tradeoff is that low resiliency is achieved bc if hardware fails then it could take down several instances:
    EXAM: 
      best practice to use same type of instance and launch all at the same time in a single AZ:
      all members have direct connections to each other and sometimes will share same host
      can span vpc peers, but impacts performance
      requires a supported instance type
    use cases: high performance, fast speeds, low latency, low resiliency:

  spread: 
    keep instances separated:
    ensure maximum availability and resiliency:
    can span AZs:
    esch instance has its own isolated networking and power supply from other instances
    limit of 7 instances per AZ, (1 in each partition), placement is done by aws:
    not supported for dedicated instances or hosts
    use cases: 
      small number of critical instances that need to be kept separated from each other:

  partion (spread + cluster): 
    like a spread placement group but a little different because you still have 7 max partitions per AZ, HOWEVER some of the instances can be in same partition whereas in spread they are all separated, so this means more than 7 instances are able to be put into a single AZ:
    placement is done by you or aws:
    use cases: 
      huge scale parallel processing systems where you need to create groupings of instances and need system to be seperated:

-----------------------------------------------------------------------------------------------:
EC2 DEDICATED HOSTS:
  hosts that are allocated to you entirely:
  you pay for the host, no instance charges:
  host hardware has physical sockets and cores which dictate how many instances can be run and organizations can utilize for socket and core licensing requirements
  use cases: 
    when you have strict regulatory or data requirements:
  characteristics:
    AMI limits, some are not supported
    amazon RDS instances are not supported
    placement groups are not supported
    hosts can be shared with other organization accounts
    basically remember there are some features that are not supported on dedicated hosts:

-----------------------------------------------------------------------------------------------:
ENHANCED NETWORKING AND EBS OPTIMIZED:
  two optimization techniques
    enhanced networking: 
      offers logical card to each ec2 instance instead of a single physical card on the host that the host manages which ec2 instance has access to:
      uses SR-IOV, single root io virtualization, to improve latency of networking:
      NIC is virtualization aware
      characteristics:
        higher packets per second, PPS:
        consistent lower latency:
        more bandwidth, high throughput
        offers higher IO and loswer host cpu usage
        no charge, available on most EC2 types
    ebs optimized: 
      means dedicated capacity for EBS:
      most instances support and have enabled by default:
      historically network was shared between data networking(ENIs) and EBS
      on older instances some support, but enabling costs extra

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
ROUTE 53:
*******************************************
ROUTE 53 FUNDAMENTALS:
  allows us to register domains:
  allows us to host zones files(called hosted zones) on managed nameservers which AWS provides:
  global service with a single database:
  globally resilient

-----------------------------------------------------------------------------------------------:
ROUTE 53 PUBLIC HOSTED ZONES:
  DNS DB for a domain, (zone files in AWS):
  hosted on four managed NS specific for the zone:
  accessible from the public internet and VPCs:
  stores RRs, resource records which are actual items of data dns uses:
  you can point to externally registered domains:
  hosted zones are authoritative for a domain
  architecture differs slightly between public internet access and vpc access, see slide

-----------------------------------------------------------------------------------------------:
R53 PRIVATE HOSTED ZONES:
  hosted zone that is only accessible within VPC it was created in and accessible within associated VPCs:
  you can grant access to different users within your account via UI, CLI/API, and grant access to users of different accounts via CLI/API
  split-view is possible where you have overlapping public and private hosted zones for public and internal use with the same zone name

-----------------------------------------------------------------------------------------------:
R53 CNAME RECORDS VS ALIAS RECORDS:
  cname records:
    maps a name to another name:
    invalid for naked/apex of a domain (domain with no subdomain, like no www., ex catagram.io) and problem is that many aws services use a DNS name which is naked/apex of a domain:
    so just cname catagram.io would be invalid
  alias records:
    map a name to an aws resource:
    can be used for both apex and normal records: 
    there is no charge for alias requests pointing at aws resources
  EXAM: 
    for aws services, default to picking alias record:
    sub types, so you need to match alias type to corresponding record it is pointing at, for ex has A and AAAA alias types:
    use alias records for services such as gateway, cloudfron, elb, elastic beanstalk, global accelerator, and s3

-----------------------------------------------------------------------------------------------:
R53 SIMPLE ROUTING:
  simple routing supports 1 record per name(ex of name is www):
  each record can have multiple values (ips?), all are returned in a random order
  client chooses and uses 1 value
  simple routing does not support health checks-all values are returned for a record when queried:
  there are no checks by record to ensure resource being pointed at is actually operational
  EXAM: 
    use when you want to route requests towards one service such as a web server:

-----------------------------------------------------------------------------------------------:
R53 HEALTH CHECKS:
  health checkers located globally can check aws targets and anything accessible over public internet:
  seperate from, but are used by records
  health checkers check every 30s, every 10s costs extra
  checks can be TCP, HTTP/HTTPS, HTTP/HTTPS with string matching, these checks build upon each other
  results in record being in healthy or unhealthy state, healthy if 18%+ of health checkers report as healthy:
  three types:
    endpoint: 
      actual endpoint that you specify:
    cloudwatch alarm: 
      check cloudwatch alarms: 
    check of checks: 
      check application wide health with lots of individual components:

-----------------------------------------------------------------------------------------------:
FAILOVER ROUTING:
  we can add multiple records of the same name, a primary and a secondary (ex 2 www records):
  they both point at unique resource:
  use case:
    when you want to configure active passive failover, meaning when you want to use the primary record if it passes its health check and the secondary record if primary record is unhealthy:

-----------------------------------------------------------------------------------------------:
MULTI VALUE ROUTING:
  like a mixture between simple and failover routing policies:
  you can create many records with the same name:
  client chooses and uses 1 value, any records which fail health checks won't be returned when queried:
  up to 8 healthy records are returned, if more exist, 8 are reandomly selected, 
  EXAM POWER UP: 
    multi value improves availability
    It is not a replacement for load balancing
    used when you have many resources that can service request and you want them all health checked and returned at random:

-----------------------------------------------------------------------------------------------:
WEIGHTED ROUTING:
  EXAM POWER UP: 
    use when you want simple load balancing or testing new software versions:
    implements record weights, precendence is based on its record weight vs total weight:
    if a chosen record is unhealthy, the process of selection is repeated until a healthy record is chosen

-----------------------------------------------------------------------------------------------:
LATENCY-BASED ROUTING:
  EXAM: 
    use when optimizing for performance and user experience:
    only 1 same name per region:
    the record returned is the one which offers the lowest estimated latency and is healthy:
    aws maintains a db of latency between the users general location and the regions tagged in records

-----------------------------------------------------------------------------------------------:
GEOLOCATION ROUTING:
  location of customers and location of resources are used to influence resolution decisions.  does not return the closest record.  allows you to choose resources that serve traffic based on geographic location of users, or default if no match, or 'no answer' : 
  records are tagged with location.  either us state, country, continent, or default :
  ip check verifies the location of the user
  use case:
    when you want to restrict content to a specific region:
    when you want to provide language specific content
    when you want to load balance across regional endpoints

-----------------------------------------------------------------------------------------------:
GEOPROXIMITY ROUTING:
  returns lowest distance, including bias:
  you define a record a region is created in via tag name or lat and long coordinates:
  bias: 
    + increases a regions size, and - descreases a regions size:

-----------------------------------------------------------------------------------------------:
INTEROPABILITY:
  R53 normally has 2 jobs, domain registrar and domain hosting:
  r53 can do both, or just domain registrar or just domain hosting:
    to do this think of r53 having a registrar role and hosting role that are seperate and work together to perform functionality
    see slides for diff architectures of working together, working seperate
  when registering a domain:
    checks with TLD to see if domain is available:
    creates a zone file for registered domain, known as hosted zone:
    allocates 4 nameservers for the zone:
    adds nameserver records to TLD that indicate that the nameservers are authoritative for domain:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
RDS, RELATIONAL DATABASE SERVICE:
*******************************************
RELATIONAL(SQL) VS NON-RELATIONAL(NOSQL) (READ ALL):
  relational:
    use SQL language
    structure in and between tables of data - rigid Schema
    schema is defined in advance before you put any data into the table
    fixed relationship between tables before data is put into the table
  nosql:
    is everything which isn't relational
    generally a much more relaxed schema
    relationships are handled differently

-----------------------------------------------------------------------------------------------:
DIFFERENT TYPES OF NO-SQL DATABASES (KNOW ALL):
  key-value:
    stores list of key value pairs
    no schema
    scalable and really fast
  wide column store:
    variation of key-value
    each row has one or more keys, one of them is partition key (minimum), secondary key is called sort or range key
    partition key needs to be unique if there are no sort keys, otherwise the composite key needs to be unique
  no attribute schema: 
    attributes can be all same/different, some empty, or no attributes
    scalable and really fast
  document:
    designed to store and query data as documents
    documents: 
      items in db, formatted as json, structure can be different, but id needs to be unique
      provides flexible indexing, which allows really powerful queries
      use cases: 
        order databases, contacts, or collections databases, ie interacting with whole documents or deep attribute interactions(nested data items within a document structure)

-----------------------------------------------------------------------------------------------:
DIFFERENT STYLES OF DATABASES (know all lines):
  row store:
    data stored as rows
    ideal if you are operating with rows adding, updating, deleting whole row or record at a time
    ideal for Online Transaction Processing, or OLTP
  column store:
    data stored as columns, ie every attribute for each document is stored together
    ideal for analytics and reporting, or OLAP
  graph:
    relationships between things are formally defined and stored in the database as well as data
    relationships, they aren't rerendered every time you make a query, different from relational database because those relationships are computed every time you make a query
    nodes: 
      objects, can have key-value properties
    edges: 
      relationships between objects, have name and direction, can have name/value pairs

-----------------------------------------------------------------------------------------------:
ACID VS BASE:
  ACID and BASE are DB transaction models
  tradeoffs between CAP Theorem: 
    consistency, availability, and partition tolerant(resilience):
    consistency: 
      every read to a database will receive the most recent write or it will get an error
    availability: 
      every read might get the most recent write but will not get an error
    partition tolerance: 
      the system can be made of multiple network partitions and system continues to operate even if there are a number of dropped messages or errors between these network nodes
  CAP theorem: 
    any database product will only be able to deliver a maximum of two of these different factors

  ACID:
    focuses on consistency:
    atomic: 
      all or no components of a transaction succeeds or fails, 
    consistent: 
      enforces most recent write is received or error:
      transactions move the database from one valid state to another, nothing in between is allowed, enforces most recent write is received or error 
    isolated: 
      transactions execute as if its the only one transacting, even if multiple transactions are occurring at once, 
    durable: 
      once committed, transactions are durable.  resilient to power outages or crashes.
    EXAM: 
      rds databases, limits databases to scale: 

  BASE: 
    focuses on availability:
      basically available: 
        read and write operations are available, without any consistency guarantees:
      soft state: 
        database doesn't enforce consistency, offloaded onto the application/user
      eventually consistent: 
        if we wait long enough, reads from the system will be consistent
      EXAM: 
        dynamoDB or NoSQL databases:

-----------------------------------------------------------------------------------------------:
DATABASES ON EC2:
  generally a bad practice:
  why you might want to do it (but still it is bad practice lol):
    you need access to the db instance OS:
    you need advanced db option tuning via dbroot permissions, aws managed db services does offer some of these options without dbroot permissions
    vendor or decision makers demand it
    you need db, db version, architecture, or specific os/db combination aws doesn't provide, generally the only reason why you would want to run a db on ec2
  if you are going to do it:
    best practice is to split monolithic stack of application, into two instances where one instance is running application and web server and another instance running the db on it.  This way you can export db to aws managed service if needed:
  why you shouldn't do it:
    adds significant cost of admin overhead of managing ec2 and dbhost:
    adds significant cost of backup and disaster recovery management  
    ec2 is single AZ, significant reduction of availability vs having regional availability when using services:
    less features, you would be missing out on some amazing features offered by aws db services
    does not offer serverless features or easy scaling
    adds significant cost of replication overhead
    adds significant cost to performance:

-----------------------------------------------------------------------------------------------:
RDS ARCHITECTURE:
  Database Server as a Service:
  range of db engines to use (MySQL, mariaDB, PostgreSQL, Oracle, Microsoft SQL server, Aurora):
  is not a public service, required to operate within a VPC:
  provides multiple databases on one DB server(instance)
  Amazon Aurora is a different product
  Managed service by aws, no access to OS or SSH access*, rds custom provides low level access
  EXAM: 
    RDS subnet group: 
      a list of subnets RDS can use for a given database instance or instances: 
  EXAM: 
    each RDS instance can have 1+ databases:
  EXAM: 
    each RDS instance has one dedicated storage EBS volume:
  synchronous replication: 
    where each time data is written to primary instance, the data is then copied from primary to standby immediately:
  asynchronous replication: 
    read replicas back up data asynchronously to standby instances in same region or different region:
  EXAM POWER UP: 
    backups and snapshots of data are transferred to s3:
  costs:
    you're billed for resource allocation
    instance size and type
    multi az or not
    storage type and amount
    data transferred 
    backups and snapshots
    licensing if applicable

-----------------------------------------------------------------------------------------------:
RDS ENHANCED MONITORING (KNOW ALL LINES):
  gathers metrics from an agent on the rds instance
  enhanced monitoring is useful when you want to see how different processes or threads on a DB instance use the CPU
  not enabled by default
  provides more detail on cpu utilization than cloudwatch does by default because cloudwatch gathers metrics from the hypervisor, 

-----------------------------------------------------------------------------------------------:
RDS MultiAZ:
  offers high availability via multiAZ instance deployment or multiAZ cluster deployment:
  
  multiAZ Instance Deployment:
    primary rds instance contains all of the database instances and is replicated synchronously to a standby instance in another AZ:
    you always access the primary database instance, all reads and writes:
    backups and snapshots to s3 are implemented via the standby instance: 
    primary and standby have to be in same region:
    one standby replica only:
    not free tier, extra cost for replica
    60-120 seconds for failover
    synchronous replication: 
      data is written to primary database and replicated to standby before it is viewed as committed
    use cases for failover switch:
      AZ outage, primary failure, manual failover, instance type change, software patching
      historically this used to be the only way
      when failing over, rds simply flips the CNAME for your DB instance to point to standby:

  multiAZ Cluster Deployment:
    EXAM: 
      one writer can implement synchronous replication to 2 readers only:
      readers can be read by clients to scale read workloads:
      hardware is much faster, so benefits from fast writes to local storage, which are then backed up to ebs volume:
      writer is like primary rds instance
      replication is via transaction logs which are more efficient, failover is around 35s
    EXAM: 
      data is committed when data has been replicated to at least one reader:
      each instance has its own local storage:
      readers have to be in same region as writer:
    cluster endpoint: 
      like a database CNAME in previous architecture, and points at the writer.  userd for reads, writes, and administration
    reader endpoint: 
      points to any reads at an available reader instance, and in some cases does include writer instance
    instance endpoint: 
      point at a specific instance. generally these are used for testing/ fault finding. each instance has one:  

-----------------------------------------------------------------------------------------------:
RDS BACKUP AND RESTORE:
  RPO: 
    designates the variable amount of data that will be lost or will have to be re-entered during network downtime. 
    good rpo minimizes amount of data that will be lost:
  RTO: 
    designates the amount of “real time” that can pass before the disruption begins to seriously and unacceptably impede the flow of normal business operations:

  automated backups and snapshots:
    backups occur from standby in multi az deployments or from only available instance if not multi az:
    regionally resilient:
    both are backed up to aws managed s3 buckets, so you can't see the backups in s3 service however you can see backups in the RDS console
    snapshots:
      done manually:
      like the s3 snapshots because first snap is full size of consumed data then incremental onward: 
      live on even after rds instance is deleted, you will have to delete manually:
    automated backups:
      scheduled backups:
      are automatically deleted by aws for a retention period of 0 to 35 days:

  transaction logs:
    store the actual operations which change the data:
    every 5 minutes transaction logs are backed up to s3
  cross-region:
    rds backups can be replicated to another region:
    charges apply for cross region data copy and the storage in destination region has to be enabled:
    both snapshots and transaction logs?
  rds restores are implemented via snapshots or automated backups:
    creates a new rds instance with new address so you will need to update applications to point to this address:
    restores aren't fast, we have to think about RTO

-----------------------------------------------------------------------------------------------:
RDS READ REPLICAS:
  benefits:
    performance benefits for read operations:
      able to deploy 5 direct read replicas per db instance:
      read replicas can have read replicas but can introduce lag issues
      global performance improvements for read workloads
    help us provide really low RTO, meaning they speed up the process of rds restores:
      snapshots and backups improve RPO but not RTO
      read replicas offer near 0 rpo and near 0 rto but only good for failure and not for corruption:
    help us create cross region failover capability providing global resilience:
  characteristics:
    only read operations, unless promoted to rds instance:
    aren't part of the primary instance in any way, have their own db endpoint address
    can be created in same or different region as primary instance
  kept in sync with primary instance via asynchronous replication:
    data is written to primary instance first, viewed as committed, then replicated to read replicas:

-----------------------------------------------------------------------------------------------:
RDS SECURITY:
  encryption in transit is available:
    EXAM: 
      SSL/TLS (in transit) can be mandatory:
    EXAM: 
      by default, encryption is done via ebs volume encryption via KMS:
      handled by host/ebs
      aws or Customer managed CMK generated DEKs (data encryption keys) which are used for encryption operations
      storage, logs, snapshots, and replicas are encrypted
    EXAM: 
      encryption can't be removed once it's added:

  RDS MSSQL and RDS Oracle support TDE:
    transparent data encryption handled within the db engine
    EXAM: 
      oracle supports integration with CloudHSM which has much stronger key controls because it removes aws from the chain of trust:

  IAM authentication:
    you can configure rds to allow IAM user authentication against a database:
    EXAM: 
      policy attached to users or roles map that IAM identity onto the local RDS user via generating token that can be used in place of db user password that generates an auth token:
    EXAM: 
      authentication only! authorization is always controlled by the db engine which grants permissions to local db user:

-----------------------------------------------------------------------------------------------:
RDS CUSTOM:
  allows you to access OS so negates the need for running db on an ec2 instance:
  works for MSSQL and Oracle and can connect using SSH, RDP, and session manager:
  fills gap between rds and you running a ec2 db engine and being able to access os

-----------------------------------------------------------------------------------------------:
AMAZON AURORA (provisioned) (provisioned db clusters) ARCHITECTURE:
  is a database engine officially part of RDS but its architecture is radically different than any of the other db engines
  key differences:
    consists of a single primary instance + 0 or more replicas:
    can have a max of 15 replicas:
    uses a cluster:
    provides benefits of both rds multi az (restore on failure) and rds read replicas (readers and writers):
  storage:
    doesn't use local storage for the instances, instead uses shared cluster volume:
    storage has 6 replicas that spread across AZ, replicated automatically across az's:
    all SSD based, high IOPS, low latency:
    faster provisioning, improved availability, and performance
  features:
    backups in aurora work in same way as rds:
    restores create a new cluster
    backtrack: 
      can be used which allows in place rewinds to a previous point in time:
    fast clones:
       allow you to make a new database much faster than copying all data, it provides reference to unchanged data and copies changed data, known as copy on write:
  billing:
    is different bc storage is billed based on what's used, high water mark so billed for the most used, and replicas can be added and removed without requiring storage provisioning:
    no free tier option
    beyond micro instances, aurora offers better value:
    compute charges are hourly charge, per second, 10 minute minimum:
    storage charges are gb per month consumer, and io cost per request
    100% db size in backups are included
  access method:
    use endpoint, dns addresses which are used to connect to cluster, they have many available and each primary and replica isntances have their own endpt, allows customization
    cluster endpt: 
      always points to primary instance:
    reader endpt: 
      if there are replicas, will load balance reads amongst the replicas:
    custom endpts: 
      point to specific subsets of instances within cluster, they allow you to provide connections based on criteria other than read only or write-only capability of the DB instances:

-----------------------------------------------------------------------------------------------:
AURORA SERVERLESS:
  removes admin overhead of managing database instances, is easier to use:
  same resilience as aurora (6 copies across azs):
  provides a version of the aurora database product where you don't need to statically provision database instances of a certain size or need to manage those database instances
  aurora capcacity units (acus):
    represent a certain amount of compute and a corresponding amount of memory:
    aurora serverless cluster has a min and max acu:
    cluster adjusts based on load, can go to 0 and be paused after consecutive minutes of inactivity
  architecture:
    uses acus that are allocated from a shared warm pool used by aws, stateless, no local storage:
    acus access shared storage in same way that provisioned instances would
    connections are a little bit more complex:
      shared proxy fleet of instances managed by aws
      user connects to application via proxy fleet, and fleet brokers connection between acus
      scaling is fluid
    use cases:
      infrequently used applications:
      new applications, or unpredictable workloads: 
      multi tenant applications:
      development and test databases

-----------------------------------------------------------------------------------------------:
AURORA GLOBAL DATABASE:
  allow you to create global level replication using aurora from a master region to up to 5 secondary aws regions:
  secondary clusters are read only, up to 16:
  allows extremely good rpo and rto:
  1s replication:
  no impact on db performance

-----------------------------------------------------------------------------------------------:
AURORA MULTI-MASTER:
  feature that allows aurora to have multiple instances which are capable of performing both reads and writes:
  default aurora mode is single-master:
  in multi-master mode all instances are read/write:
  no concept of a load balancer, the application connects to one or all of the instances in cluster directly:
  benefit is that failover process is much quicker, because it does not need to switch endpts because it is already connected to multiple instances
  when a write occurs:
    an instance proposes a write change to all of the shared storage nodes and all other instance nodes have to agree for write to occur:
    the write is then written to all of the shared storage nodes and then to each instance node's memory cache:

-----------------------------------------------------------------------------------------------:
RDS PROXY:
  problem it fixes:
    opening and closing connections consume resources and often take bulk of operation time
    with serverless every lambda opens and closes
    handling failure of database instances is hard, doing it within your application adds risks
    db proxies help but managing them is somewhat hard 
  what it does:
    maintains a pool of connections that remain open for the database and applications:
    multiplexes, so there is a smaller number of connections open to the database than the number of connections open to the applications:
    EXAM: 
      each lambda function connection to proxy is much quicker to establish vs direct connection to db and no load on db:
    EXAM: 
      abstracts clients away from db failure or failover events, bc proxy connection is established and waits even if target db is unresponsive:
  use cases:
    too many connections errors, db instances using t2/t3, smaller/burst instances
    aws lambda, time saved via connection reuse and iam auth reuse
    long running connections for SAAS apps
    where resilience to db failure is a priority and make it transparent to application
  key facts:
    auto scaling, highly available by default
    fully managed db proxy for rds/aurora:
    only accessible from a vpc via proxy endpt
    can enforce ssl/tls connections
    can reduce failover time by 60% and abstracts failure:

-----------------------------------------------------------------------------------------------:
DATABASE MIGRATION SERVICE (DMS):
  managed database migration service:
  source and destination endpts point at source and target databases and one endpt must be on aws:
  runs using a replication instance
  replication instance:
    can define replication tasks, define all of the options relate to migration
  three types of jobs:
    full load migration: 
      migrates existing data, outage until migrated:
    full load + CDC, change data capture: 
      migrates existing data and replicates any ongoing changes from source to target:
    CDC only: 
      replicates only data changes: 
  implements schema conversion tool (SCT): 
    used when db engines are not compatible:
    used when converting one database engine to another
    use for larger migrations
  when converting larger migrations:
    dms can ultilize snowball products:
    whenever you utilize snowball products you can use sct when db engines are compatible also bc technically the db is changing

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
EFS, ELASTIC FILE SYSTEM:
*******************************************
  provides network based file systems which can be mounted within linux ec2 instances and used by multiple instances at once:
  EFS is an implementation of NFSv4:
  access via mount targets inside a VPC, or from outside vpc via hybrid networking if you enable vpn or direct connect (dx):
  different from EBS because ebs is block storage and EFS is file storage
  private service 
  architecture:
    posix permissions, standard for interoperability which is used in linux:
    mount targets in multiple azs
  key takeaways:
    linux only:
    general purpose and max io performance+ models, general purpose is default for 99.99% of uses
    bursting(default) and provisioned throughput modes
    standard(default) and infrequent access classes

-----------------------------------------------------------------------------------------------:
AWS BACKUP:
  fully managed data protection (backup/restore) service:
  implements a wide range of products that perform central management of backups: 
    consolidate management into one place, across accounts and across regions:
  backup plans:
    frequency, window, lifecycle, vault, region copy
  resources: 
    what resources are backed up
  vaults: 
    backup destination (container), you can assign kms key for encryption
  vault lock: 
    enables write once, read many, 72 hour cool off, then even aws can't delete, designed for compliant style situations
  PITR: 
    point in time recovery, you can restore state to a certain point of time within retention window

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
HIGH AVAILABILITY AND SCALING:
*******************************************
REGIONAL AND GLOBAL AWS ARCHITECTURE:
  global components:
    global service location and discovery: 
      how does your machine discover correct reference?
    content delivery and optimization: 
      how does data get to users globally?
    global health checks and failover: 
      determining if endpoints are healthy and if not how to implement failover?
  regional components:
    scaling and resilience
    application services and components

  tiers: 
    high level groupings of functionality or different zones of your application
  web tier: 
    communications from applications will generally enter web tier first, ex application load balancer or api gateway
  compute tier: 
    provides functionality for web tier, ex lambda ec2, ecs
  storage tier: 
    provides file storage, ex ebs, s3, efs
  caching tier: 
    improve performance and reduce costs, ex elasticache, dax
  db tier: 
    provides data storage, ex rds, dynamodb, redshift
  app services tier: 
    provide some type of functionality to applications, sqs, kinesis,

-----------------------------------------------------------------------------------------------:
EVOLUTION OF ELASTIC LOAD BALANCER (ELB):
  3 types of load balancers available within aws
  there is one v1 type and two v2 types, avoid using v1
  v2 is faster, cheaper, support target groups and rules, truly layer 7 devices
  classic load balancer: 
    never use:
    v1, 2009, not really layer 7, lacking features, 1 sll cert. per clb limit which limits ability to scale
  application load balancer:
    v2, truly layer 7 device, supports HTTP/ HTTPS / WebSocket
  network load balancer:
    v2, truly layer 7 device, supports TCP/ TLS/ TLS & UDP

-----------------------------------------------------------------------------------------------:
ELB ARCHITECTURE:
  accepts connections from customer and distributes those connections across any registered backend compute:
  the physical infrastructure is abstracted away from the customer
  without load balancers, everything would have to be tied to everything in a sequential flow and if there is failure, service would be disrupted
  elb's allow each tier to scale independently
  EXAM POWER UPS (ALL):
    configured to run in 2+ AZs:
    1+ elb nodes are placed into a subnet in each AZ and scale with load:
    a record: 
      each elb is configured with an A record DNS name that resolves to the ELB node:
    need 8+ IPs per subnet and a /27 or larger subnet to allow for scale
  two different type of elb nodes:
    internet-facing: 
      have public IPs and private IPs, can access public and private ec2 instances
    internal: 
      only have private IPs, generally used to seperate different tiers of applications
  listeners: 
    nodes are configured with listeners which accept traffic on a port and protocol and communicate with targets on a port and protocol
  cross zone elb:
    feature that allows elb node to distribute connections across all AZs:

-----------------------------------------------------------------------------------------------:
APPLICATION AND NETWORK LOAD BALANCER (ALB VS NLB):
  v2 load balancers support rules that contain 1 ssl per rule, each rule targets a target group, which contains resources corresponding to ssl:
  application load balancer:
    listens on HTTP and/or HTTPS:
    can understand l7 content type, custom headers, user location, app behavior:
    can't understand any other layer 7 protocols and no TCP/ UDP/TLS listeners:
    ALBs are slower than NLBs, bc there are more levels of the network stack to process:
    ALBs can perform application health checks:
    have to be in public subnet:
    HTTP HTTPS always terminated on the ALB, no unbroken SSL connection from customer through to your application instances, then a new connection is made from load balancer to the application
    ALBs must have SSL certs if HTTPS is used
    rules:
      direct connections which arrive at a listener
      processed in priority order, there is a default rule 
        rule conditions: 
          like host-header, http-header, http-request-method, path-pattern, query-string, source-ip
        actions: 
          forward, redirect, fixed-response, authenticate-oidc, authenticate-cognito
  network load balancer:
    layer 4 load balancer, TCP, TLS, UDP, TCP_UDP:
    SMTP, SSH, game servers, apps that do not use http:
    no visibility or understanding of HTTP or HTTPS, no headers, no cookies, no session stickiness:
    really really really fast, 75% faster than ALBs:
    health checks just check icmp/tcp handshake, they do not provide application health checks:
    can forward TCP to instances, so can you do unbroken end to end encryption:
    NLB's can have static IPs which are useful for whitelisting
    used with private link to provide services to other VPCs
  
  NLB or ALB?:
    NLB: unbroken encryption:
    NLB: static ip for whitelisting, you can't associate EIPS with ALB:
    NLB: fastest performance:
    NLB: protocols not HTTP or HTTPS:
    ALB: any other scenario:
    NLB: privatelink

-----------------------------------------------------------------------------------------------:
LAUNCH CONFIGURATIONS AND LAUNCH TEMPLATES:
  allow you to define the configuration of an ec2 instance in advance:
  launch configs not editable, launch templates have versions:
  you can define anything you normally define instance type, storage and key pair, networking and security groups, userdata, IAM role
  launch configurations have one purpose: 
    provide configuration of ec2 isntances of an auto scaling group:
  launch templates purpose: 
    same as launch configuration and also can be used to provision ec2 instances from the console UI or CLI:
    also provide T2/T3 unlimited, placement groups, capacity reservations, and elastic graphics:

-----------------------------------------------------------------------------------------------:
AUTO SCALING GROUPS:
  allows ec2 to scale automatically based on demand placed on the system:
  generally used with elbs and launch templates to deliver elastic architectures:
  EXAM: 
    has a minimum size, desired capacity, and maximum size:
    self healing for ec2:
    cooldown periods:
      ensures asg does not launch or terminate additional ec2 instances before previous scaling activity takes effect: 
      default value 300 seconds: 
    more, smaller instances provide granularity vs having bigger instances:
    ASG defines when and where, uses launch templates or launch configurations to define what
    free to use, only resources created are billed
  goal: 
    to keep running instances at the desired capacity by provisioning or terminating instances
  scaling policies: 
    automatically adjust the desired capacity between the min and max values based on metrics like cpu load:
    don't need scaling policies, they can have none
    manual:
      manually adjust the desired capacity
    scheduled:
      scale based on schedule, performs a scaling action at a certain time which states new minimum, maximum, and desired sizes:
    predictive:
      scale based on prediction of workload, but diff from scheduled because all instances need to be homogenous:
    dynamic (know all): 
        simple: 
          simple if then adjustment
        stepped: 
          bigger +- based on difference, recommended by aws bc more flexible
        target tracking: 
          maintains a desired aggregate metric like keep cpu at 40%
        scaling based on SQS queue: 
          ApproximateNumberOfMessagesVisible, scale base on approximate number of messages visible
  self healing:
    implements ec2 status checks:
      monitors health of ec2 instances, and if they do not pass status check, terminates the instance and provisions a new one:
  default policy on which instance to terminate when scaling in:
    terminate the ec2 instance launched from oldest configuration that is not protected:

  scaling processes:
    different process or functions to be performed by asg
    LAUNCH and TERMINATE: if set to suspend then won't launch/terminate
    AddToLoadBalancer: add instance to lb on launch
    alarmNotification: determins if asg reacts to alarms from CW
    AZRebalance: balances instances evenly across all AZs
    Healthcheck: whether instance health checks across group are on/off
    replaceUnhealthy: terminate unhealthy and replace
    scheduledActions: scheduled on/off
    standby: use this for instances that you don't want to be affected by anuthing the asg does

-----------------------------------------------------------------------------------------------:
ASG + LOAD BALANCERS:
  ELBs give ability to app to dynamically point to many instances via target group, and ASG provides dynamic number of instances as needed:

-----------------------------------------------------------------------------------------------:
ASG LIFECYCLE HOOKS:
  allow you to configure custom actions on instances during ASG actions:
  actions are either instance launch or instance terminate transitions:
  whenever you create  scale out (creating instance) lifecycle hook: 
    instance moves to pending:wait state, performs lifecycle hook then moves to a pending:proceed state, then instantiated 
  whenever you create scale in (terminating instance) lifecycle hook: 
    instance moves to a terminating:wait state, then moves to terminating:proceed state when timeout expires or you can resume with CompleteLifecycleAction code, then performs lifecycle hook, then terminated 
  lifecycle hooks can be integrated with SNS so that notifications for lifecycle hooks can be sent to a SNS topic
  lifecycle hooks can be integrated with EventBridge so that other processes can be initiated based on the hooks

-----------------------------------------------------------------------------------------------:
ASG HEALTH CHECKS:
  ASG can use ALB health checks rather than ec2 status checks:
  three different types of health checks: 
    EC2(default), ALB(can be enabled), and custom :
    EC2 unhealthy:
      stopping, stopped, terminated, shutting down, or impaired(not 2/2 status checks):
    ELB healthy:
      running and passing ELB health check
      they can be more application aware because has knowledge of layer 7:
    custom:
      instances marked healthy or unhealthy by an external system
      health check grace period: 
        default is 300s, delay before starting checks, allows system launch, bootstrapping, and application start.  make sure you enable a suitable grace period so instances have enough time to be provisioned. 

-----------------------------------------------------------------------------------------------:
SSL OFFLOAD AND SESSION STICKINESS:
3 ways an elb can handle secure connections: bridging, pass-through, or offload
  bridging mode:
    default mode of an application load balancer:
    connection is terminated (DECRYPTED) on the ELB and needs a ssl certificate for the domain name the application uses:it's a
    elb decrypts the http then routes to backend based on that http, but before connecting to backend it encrypts data and initiates a new ssl connection to backend instances.:
    1 or more clients makes 1 or more connections to a load balancer
    listener is configured for HTTPS
    aws does have some level of access to certificate
    instances need ssl certificates and compute required for cryptographic operations
    benefits:
      elb gets to decrypt http data and perform actions based on that data, flexible:
    disadvantages:
      certificate stores on elb and that's a risk and ec2 instances need a copy of certificate which adds admin overhead:
  pass-through: 
    network load balancer performs this architecture:
    each instance needs to have certificate installed but elb doesn't so no certificate exposure to aws:
    client connects to elb, but elb just passes that connection to backend instances:
    listener is configured for tcp.  no encryption or decryption happens on the NLB.
  offload:
    connection is terminated (DECRYPTED) on the ELB and needs a ssl certificate on ELB for the domain name the application uses:
    connects to elb via https
    elb to instance connections uses HTTP so not certificate or cryptographic requirements
    benefits:
      encrypted on public internet to aws via https:
    disadvantages:
      not encrypted on aws network via http:

session stickiness:
  first time user makes a request, alb generates a cookie, AWSALB, which locks the device to a single backend instance for a duration:
  has to be enabled on an application load balancer for a target group:
  with no stickiness, connections are distributed fairly equal across all in service backend instances
  if application is stateful, then user would lose session details if resuming on different instance
  if application is stateless, then application could operate ok without stickiness connections

-----------------------------------------------------------------------------------------------:
GWLB, GATEWAY LOAD BALANCERS:
allows us to implement transparent security appliances in a scalable way:
  transparent security appliance: 
    allow to deploy, scale, and manage 3rd party virtual appliances such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems that scans data after it leaves and before it enters the application instance:
  flow:
    traffic source => GWLBE => GWLB => appliances => GWLB => GWLBE => aws destination 
    it combines a transparent network gateway (GWLB endpoint that is single entry and exit point for all traffic) that distributes traffic to GWLB that distributes traffic via a Geneve encapsulation tunnel to your 3rd party appliances on demand, and scales this load,
    then the appliances filter traffic, send back to GWLB, that sends back to GWLB endpt, who then sends to aws destination

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
EVENT-DRIVEN ARCHITECTURE:
*******************************************
DIFFERENT TYPES OF ARCHITECTURE (know all lines):
  monolithic:
    all of the different components of application are coupled together, they fail together, scale together, and bill together because full capacity is needed at all times in case they are all running even if certain components are not running
  tiered:
    different components can be run on different servers
    still tightly coupled because specific instances are coupled to other specific instances
  tiered w/ internal load balancers:
    different components can be run on different servers
    lb in between tiers allows instances to connect to any other instance
    still coupled bc there is still dependency from one tier to another that expects other tier to be there to answer its request, impossible to scale individual tiers down to 0 bc the communication is synchronous and needs another tier to be running at same time request is sent
  tiered with queues, asg:
    different components can be run on different servers
    queues in between tiers removes coupling because asynchronous communication can occur, 
    tiers are not tightly coupled anymore because a tier transacts with queue, not tier, then is done
    auto scaling group, provisions instances in a processing component based on queue length
    queue: 
      system which accepts messages, messages can be ordered in a FIFO order, but not always the case
  microservice architecture:
    collection of microservices
    microservices:
      a tiny self sufficient application
      has its own input, output components, logic, and its own store of data 
      are small units that do things individually very well, they can be unique services or copies of same service
      producer, consumer, or both (microservices): 
        producer produces data, consumer consumes data, and some do both
        Microservices produce and consume events
  event driven architecture:
    implements microservices that allow you to only consume resources while handling events:
    event producers produce events in reaction to something
    event consumers wait for events to occur and do something with event or in response to event
    components can be both producers and consumers
    both producers and consumers are not running or consuming resources unless they have been triggered and resources are required
    event routers: 
    event bus:
      like a constant flow of information, producers send events here and event router routes them to consumers
    takeways:
      only consumes resources while handling events


-----------------------------------------------------------------------------------------------:
AWS Lambda:
  function as a service that allows you to run code without needing to manage servers:
  you are billed for the duration that a function runs:
  key part of serverless architectures
  runtime environment: 
    functions are loaded and run in a runtime environment with defined runtime:
    you directly control the memory allocated for lambda functions whereas CPU is allocated indirectly, 128MB to 10240 MB:
  lambda function :
    contains code, wrappings, and configuration:
    deployment package service lamba executes
  docker:
    does not support running docker containers, however you can use docker image to spin up a lambda image
  custom runtimes: 
    allow people to create custom runtimes using layers:
  common uses:
    serverless applications... s3, api gateway, lambda:
    file processing... s3, s3 events, lambda:
    database triggers... dynamodb, streams, lambda:
    serverless CRON... eventbridge/cwevents + lambda:
    realtime stream data processing... kinesis + lambda:
    EXAM:
      lambda functions have a timeout of 900s, or 15 mins:

-----------------------------------------------------------------------------------------------:
LAMBDA FUNCTION PERMISSIONS:
  execution permissions and resource-based policies
  execution permissions: 
    define what your lambda functions can do:
    use IAM role based access:
    do not embed credentials into lambda execution environment
    also needs basic execution permissions in order to do things like posting logs in cloudwatch, AWSLambdaBasicExecutionRole
  resource based policies:
    define who can invoke or manage your lambda function:
    you can allow or deny certain entities from interacting with your lambda function
    can only be changed via cli or api

-----------------------------------------------------------------------------------------------:
LAMBDA ENVIRONMENT VARIABLES:
  are automatically encrypted by default with kms default encryption key:
  to have more control over the data, use your own key (encryption helper), to create, rotate, disable, and define access controls:

-----------------------------------------------------------------------------------------------:
LAMBDA NETWORKING MODES: 
  public and VPC networking 
  public:
    default, can access public aws services and the public internet:
    best performance because no customer specific vpc networking is required
    lambda functions have no access to VPC based services unless public IPs are provided and security controls allow external access
  private:
    configured to run in private subnet, obey all of the vpc networking rules:
    vpc gateway endpts can provide access to public aws services from private subnet
    nat gateway and internet gateway are required to access public internet from public subnet
    creates ENI to run:
      needs permission
      creates one ENI per unique set of security group and subnets, for lambda fns
      takes 90s for initial setup

-----------------------------------------------------------------------------------------------:
LAMBDA LOGGING:
  lambda uses cloudwatch, cloudwatch logs, and xray for logging:
  logs from lambda executions: 
    cloudwatchlogs
  metrics (invocations, success/failure, retries, latency): 
    cloudwatch
  distributed tracing: 
    x-ray:

-----------------------------------------------------------------------------------------------:
LAMBDA INVOCATIONS (need to know all lines):
Three types of lambda invocations: synchronous, asynchronous, and event source mappings
  synchronous: 
    cli/api invokes a function, passing in data and waiting for a response
    EXAM: 
      lambda returns response data, whether success or failure
    EXAM:
      errors or retries have to be handled within the client
  asynchronous:
    typically used when aws services invoke lambda functions
    asynchronous communication, so not waiting for response
    EXAM:
     error handling: 
      incoming events are placed in the lambda service queue before being sent to the actual fn, if the fn returns an error to lambda service then lambda will retry fn twice by default or keep event in the lambda service queue for a certain time period.  If either are reached, then you can have error sent to DLQ, or Lambda destinations
    EXAM:
     need to be idempotent: reprocessing a result should have the same end state
    EXAM:
     lambda supports destinations (sqs, sns, lambda, eventbridge) where successful or failed events can be sent
  event source mapping: 
    EXAM:
      event source mapping within lambda service polls queues or streams for new data and gets batches of source data and sends into lambda functions as event batches
      typically used on streams or queues which don't support event generation to invoke lambda (sqs, kinesis, Dynamodb streams) 
    EXAM: 
      permissions from the lambda execution role are used by the event source mapping to interact with event source
    EXAM: 
      lambda supports destinations (sqs, sns, lambda, eventbridge) where successful or failed batches can be sent

-----------------------------------------------------------------------------------------------:
ALIASES AND VERSIONS:
  Versions: 
    help manage the deployment of functions, enabling the ability to publish a new version of a function for testing, without affecting stable prod version:
    with every version there are two ARNS, (references), one that points to the published fn and another that points to the copy:
    published fn is immutable:
    includes:
      code and dependencies,  runtime setting and env variables, and ARN
  alias: 
    like a pointer to a specific lambda function version:
    services within serverless architecture point to alias so that when a lambda fn changes, you only have to update alias to point to new published version istead of changing all references for your services to point to new published version:
    has ARN reference

-----------------------------------------------------------------------------------------------:
LAMBDA EXECUTION CONTEXT RESUSE(know all lines):
  execution context: 
    container that holds all code and objects declared outside the handler method, like libraries, runtime, etc
  cold start: 
    there is added latency when a function is executed because execution context needs to be "bootstrapped", (created and configured)
  warm start: 
    after a function is executed, lambda maintains the execution context for 15 mins, "warming" the context for reuse so reduced latency when called again because context is reused
    After 15 mins, lambda will terminate the execution context, and if called again will need to be bootstrapped again
  provisioned concurrency:
    feature that allows you to launch the amount of execution contexts you have specified and will keep them warm for you.  can be less expensive than running a Lambda on demand if you are using the capacity you reserved.

-----------------------------------------------------------------------------------------------:
CLOUDWATCH EVENTS AND EVENTBRIDGE:
  cloudwatch events: 
    allows a near real time stream of system events that describe changes in aws services:
    has visibility of cloudwatch:
  eventbridge: 
    a default event bus(stream of events which occurs from any supported service) for the account:
    super set of cloudwatch events functionality:
    allows us to handle third party events as well as events from custom applications:
    rules match incoming events, or schedule based rules, and route events to 1+ targets, ex lambda, ecs:
  overview:
    if X happens, or at Y time(s), ... do Z,
    Eventbridge is basically CloudWatch Events v2,
    Cloudwatch events has only one implicit bus, eventbridge can have additional event busses
  flow:
    Eventbridge sits over the top of event bus listening for events
    rules are linked to event bus, match incoming events, and route events to 1+ targets

-----------------------------------------------------------------------------------------------:
SERVERLESS ARCHITECTURE:
  main focus is you do not have to manage the servers, or if you do very few:
  event driven, billing occurs only when being used:
  FaaS is used where possible for compute functionality:
  applications are smaller than microservices, they are a collection of small and specialized functions
  run in stateless and ephemeral (short-lived) environment, implements duration billing,
  managed services are used where possible

-----------------------------------------------------------------------------------------------:
SNS, SIMPLE NOTIFICATION SERVICE:
  helps coordinate the sending and delivery of messages across aws for notifications:
  messages are <=256KB payloads
  public aws service
  topics: 
    endpts, they contain permissions and configurations:
  publisher: 
    sends messages to a topic:
  subscribers: 
    receive messages from a topic, ex HTTP, email, sqs, mobile push, sms messages, lambda:
  characteristics:
    by default, each message published to a topic is received by every subscriber, but you can enable filter on which messages a subscriber receives:
    delivery status: 
      confirm receipt of message by subscribers
    delivery retries: 
      reliable delivery
    regionally resilient service
    SSE, server side encryption
    Cross-acount via TOPIC policy: 
      resource policy for sns topics

-----------------------------------------------------------------------------------------------:
AWS STEP FUNCTIONS:
  state manager that allows serverless workflow, has startpt, states, and endpt:
  problems/limitations with Lambda:
    15 min max execution time
    lambda chaining gets messy at scale, and not possible to transfer state
  state machine:
    states: 
      are things which occur inside state machine, input data, modify data, output data,
  maximum duration:
    1 year:
  two different types of workflows:
    standard workflow:
      default, has a 1 year maximum duration:
    express workflow:
      designed for high volume event processing workloads and high transactions:
      has a 5 minute maximum duration:
  execution triggered via: 
    api gateway, iot rules, eventbridge, lambda:
  implements ASL, amazon states language
  IAM role is used for permissions
  states:
    are things which occur inside state machine, input data, modify data, output data,
    succeed and fail states: 
      process succeeded or failed,
    wait state: 
      pauses processing for a certain period of time,
    choice state: 
      takes a different path based on input,
    parallel: 
      allows you to create parallel branches to perform multiple processes at same time,
    map: 
      accepts a list of things, performs actions for each item,
    task: 
      represents a single unit of work, coordinates with other services to actually perform that work, can be integrated with Lambda, batch, dynamodb, ecs, sns, sqs, glue, sagemaker, emr, step functions

-----------------------------------------------------------------------------------------------:
API GATEWAY:
  allows you to create and manage APIs:
  sits between applications and integrations(services which provide the functionality of API gateway): 
    http endpts, lambda, step fns, sns, ddb:
  highly available, scalable, handles authorization, authentication, data transformations, throttling, caching, CORS, OpenAPI spec, direct integration, and more:
  HTTP APIs, REST APIs, and Websocket APIs:
  can connect to services/endpts in aws or on premises
  integrates with cloudwatch
  authentication: 
    can natively integrate with cognito:
  gateway cache:
    helps reduce the number of calls made to backend integrations and improves latency:
    configured per stage
    cache ttl default is 300 seconds, but can be configured to be 0-3600
    can be encrypted
    cache size is 500MB to 237GB
  endpt types:
    edge-optimized: 
      incoming requests are routed to nearest cloudfront POP,
    regional:
      used when clients are in the same region,
    private:
      accessible only within a VPC via interface endpt
  resource: 
    data that can be manipulated by an application, Resources are typically exposed by an API (application programming interface) or other interface that allows the application to interact with them.:
  stage:
    is a reference to a deployment, or snapshot.  Helps you manage and optimize a particular deployment. every time you make a change to your api, you must deploy to a stage for it to go live:
    each stage has its unique url as well as its own settings
    canary deployments: 
      Deployments are made to the canary (sub part of stage), not stage.
      can be configured so a certain percentage of traffic is sent to the canary.  This can be adjusted over time, to eventually promoting canary to be new base stage (consuming all of traffic)
  3 phases in most api gateway interactions:
    request phase: 
      client makes a request to api gateway:
      authorizes, validates, transforms to data integration can handle:
    integration phase:
      request is moved through api gateway to integration:
    response phase:
      where response is sent to client:
      transforms to data client can handle, prepares, returns to client:

  facts and figures to remember:
    4xx errors: client error, invalid request on client side:
      400: bad request, generic
      403: access denied:
      429: exceeded configured throttling amount
    5xx errors: server error, valid requests are sent but there is backend issue:
      502: bad gateway exception, bad output returned by lambda
      503: service unavailable, major service issues
      504: integration failure/timeout, 29s timeout for api gateway

-----------------------------------------------------------------------------------------------:
SQS, SIMPLE QUEUE SERVICE:
  message queieng service that allows you to build loosely coupled systems:
  clients can send messages to queue, and other clients poll the queue:
  all sqs queues have enqueue timestamps (time when message is received) and retention period:
  retention period can be from 1 minute to 14 days, default 3 days, messages are then automatically deleted:
  can hold an unlimited number of messages:
  public, fully managed, highly available queues
  messages up to 256 kb in size, if you need bigger message you can store link inside queue
  encryption at rest via KMS can be enabled, in transit encryption is default
  EXAM POWER UP: 
    REMEMBER SNS AND SQS FANOUT ARCHITECTURE SLIDE:
  visibilityTimeout: 
    time period that a message is hidden from queue after is it received from client.  If client (does not delete message or processing fails) after this time period is over, then message reappears back in the queue to be reprocessed:
  Dead Letter Queues (DLQ): 
    can be used for problem messages
  implementations:
    decouple components: 
      one component sends messages to queue, other reads from the queue, that way they do not have to communicate directly:
    scaling: 
      ASGs can scale based on queue length and lambdas can be invoked when messages appear on a queue:
  billing:
    based on requests, request is a single request you make to sqs
    1 request = 1-10 messages up to 64KB total
  two types of polling:
    short: 
      immediate, if 0 messages in queue then returns 0 messages:
    Long:(waitTimeSeconds): 
      can be as long as 20s, if 0 messages in queue then waits for a message to appear before returning:
  two types of queues: 
    Standard:
      at least once delivery, no guarantee on order:
      much faster performance, near unlimited TPS:
      used for decoupling, worker pools, batch for future processing:
    FIFO:
      exactly once delivery, guaranteed order:
      slower performance, 3000 TPS with batching, 300 TPS without batching:
      used for workflow ordering, command ordering, price adjustments:
      must have a .fifo suffix in name
  Policies:
    Queue policies: 
      (resource policy) grant access to queue only from external accounts
    identity policies: 
      grant access within your account

-----------------------------------------------------------------------------------------------:
SQS DELAY QUEUES:
  allow you to postpone the delivery of messages to consumers:
  messages are hidden for a set period of time, DelaySeconds, as soon as they are added to queue
  can set up to 15 minutes
  you can set per message setting of delay on standard queues but not FIFO queues

-----------------------------------------------------------------------------------------------:
SQS DEAD LETTER QUEUES:
  help you handle reoccuring failures when processing messages which are within an SQS queue:
  specifically you can analyze data being sent to DLQ, perform seperate processing, or testing:
  messages are automatically deleted from DLQ if retention period of DLQ > enqueue timestamp:
  each message contains a ReceiveCount, which is number of times it has entered queue
  allows you to configure an alarm when a message is delivered to a dead letter queue
  redrive policy:
    specifies the source queue, the DLQ, and conditions where messages will be moved from one to the other, it defines the maxReceiveCount: 
    if maxReceiveCount < ReceiveCount message is deleted

-----------------------------------------------------------------------------------------------:
KINESIS DATA STREAMS:
  scalable streaming service- allows you to ingest lots of data from a lot of different applications:
  multiple producers can send data into a kinesis stream and multiple consumers can access data from the same stream: 
  streams can scale from low to near infinite data rates:
  public service and highly available
  kinesis data record:
    allows streams to store data for 24 hours by default but up to 365 days for cost: 
  shards:
    streams have 1 to n shards:
    the more shards the more performance and cost:
    each shard has 1 MB/s ingestion capacity and 2 mb/s consumption capacity
  kinesis data firehose:
    connects to a stream and moves data from a stream into another aws service, ex s3

KINESIS DATA FIREHOSE:
  connects to a stream and moves data from a stream into another aws service, ex s3:
  near real time delivery, approximately 60s or when buffer is filled with 1 MB of data:
  fully managed delivery service to load data for data lakes, data stores, and analytics services
  automatic scaling, fully serverless, resilient
  use case:
    use when you need data to be persisted past the set interval for storing data:
    to pass data to the available services natively:
    or when kinesis analysis isn't enough:
  billing: 
    volume through firehose
  architecture:
    EXAM: 
      kinesis data firehose or kinesis data stream can deliver to: 
        HTTP endpts (third party providers):
        splunk:
        redshift (uses s3 as intermediate, then copied to redshift, process is handled by firehose):
        elasticSearch:
        s3:
    lambas can be used to transform data to expected format for delivery

-----------------------------------------------------------------------------------------------:
KINESIS DATA ANALYTICS:
  real time processing of data using SQL:
  ingests data from from kinesis data streams or firehose, or optionally static source like s3:
  destinations: 
    firehose(s3, redshift, elasticSearch, splunk), lambda, kinesis data streams:
  good visual representation of architecture on KINESIS DATA ANALYTICS slide
  billing: 
    pay for data you process, not cheap
  use cases:
    streaming data needing real time SQL processing:
    time series analytics, elections/ e-sports:
    real time dashboards, leaderboards for games:
    real time metrics, security and response teams:

-----------------------------------------------------------------------------------------------:
KINESIS VIDEO STREAMS:
  allows you to ingest live video data from producers:
  consumers can access data frame-by-frame or as needed
  can persist and encrypt data in transit and at rest as a managed service
  can't access directly via storage, access only via APIs 
  integrates with other aws services like Rekognition and Connect
  ex: 
    security cameras, smartphones, cars, drones, time serialized audio, thermal, depth, radar data:

-----------------------------------------------------------------------------------------------:
SQS VS KINESIS?:
  SQS: 1 production group, 1 consumption group:
  SQS: decoupling and asynchronous communications:
  SQS: no persistence of messages, no window
  Kinesis: huge scale ingestion:
  Kinesis: mulitple consumers, rolling window:
  Kinesis: data ingestion, analytics, monitoring, app clicks:

-----------------------------------------------------------------------------------------------:
AMAZON COGNITO:
  allows authentication, authorization, and user management for web/mobile apps:
  USER POOLS:
    goal is for you to sign in and get a JSON web token, JWT:
    imagine a db of users which can include external identities:
    about login and about managing user identities
    JWTs can grant access to APIs via Legacy Lambda custom authorizers and API gateway, but not aws credentials
    features:
      user directory management and profiles, sign up and sign in (customizable UI), MFA, and other security features:
  IDENTITY POOLS:
    allow you to offer access to temporary aws credentials in exchange for token:
    unauthenticated identities: 
      guest users
    federated identities:
      swap external identities for short term aws credentials, ex would be google, facebook, twitter, saml2.0, user pool:
    process of Identity pool and federated identity:
      token, which can be different types, is passed to identity pool:
      authenticated or unauthenticated role is granted to cognito, who grants aws temporary credentials to user:
    process: Identity pool and user pool:
      jwt token is passed to identity pool:
      authenticated or unauthenticated role is granted to cognito, who grants aws temporary credentials to user:

  EXAM: 
    web identity federation: 
      swapping of any external identity provider token for aws credentials:
-----------------------------------------------------------------------------------------------:
AWS GLUE:
  serverless ETL, extract, transform, and load system:
  etl systems combine multiple sources of data into one:
  similar to datapipeline but datapipeline uses servers
  glue jobs:
    moves and transforms data between source and destination: 
    crawls data sources and generates the aws glue data catalog
    can be initiated manually or triggered by eventBridge
  data sources include:
    stores: 
      s3, rds, jdbc compatible and dynamodb:
    streams: 
      kinesis data stream and apache kafka:
  data targets include:
    s3, rds, jdbc databases:
  crawlers:
    connect to data stores, determine schema, and create metadata in data catalog
  data catalog:
    persistant metadata about data sources in region
    improves visbilitiy of data within organization because it is publicized and accessible
    one unique catalog per region per account
    some services that use data catalog:
      athena, redshift spectrum, EMR, Lake Formation
  AWS warm pool:
    aws managed compute resources to perform glue jobs

-----------------------------------------------------------------------------------------------:
AMAZON MQ:
  helps orgs migrate from an existing system that uses topics or queues into AWS with little to no application change:
  SNS and SQS won't work out of the box
  need standards compliant solution for migration
  what is it?:
    open source message broker based on managed Apache ActiveMQ(popular in enterprise)
    EXAM: 
      supports JMS API, protocols such as AMQP, MQTT, OpenWire, and Stomp:
  provides queues (one to one communication) and topics (one to many comm)
  characteristics:
    single instance available or HA pair
    EXAM: 
      no aws native integrations...delivers activeMQ product which you manage:
    EXAM: 
      vpc based, not a public service, private networking is required:
-----------------------------------------------------------------------------------------------:
AMAZON APPFLOW:
  fully managed integration service:
  like middleware, you can exchange data between applications using flows:
  syncs data across applications or aggregrate data from different source
  public endpts, but works with privateLink(privacy)
  appflow custom connector sdk, you can build your own

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
GLOBAL CONTENT DELIVERY AND OPTIMIZATION:
*******************************************
CLOUDFRONT ARCHITECTURE:
  origin:
    the source location of your content:
    s3 origin or custom origin(HTTP server):
  distribution:
    the base configuration entity of cloudfront:
    EXAM: 
      security policy is defined on the distribution:
    EXAM: 
      cache invalidation defined on the distribution:
    contains behaviors
    behaviors:
      like a sub-configuration within a distribution:
      if path pattern of behavior is matched, then it is used:
      link to origins
      have default behavior, but you can add more
  edge locations:
    local cache of your data:
  regional edge cache:
    larger version of an edge location:  
    generally supports a number of local edge locations:
    provides another layer of caching.
  cloudfront integrates with aws certificate manager for HTTPS so you can use SSL certificates with cloudfront:
  cloudfront performs read-only caching, does not ever write to origin

-----------------------------------------------------------------------------------------------:
CLOUDFRONT BEHAVIORS:
  EXAM: 
    a single distribution can have multiple behaviors:
  defines:
    origin and origin groups
    viewer protocol policy: 
      which policy is used between viewer and edge location
      allowed http methods 
    cache policies:
    restrict viewer access to a behavior: 
      via trusted key groups or trusted signer, need signed cookies or signed urls to access
    function associations: 
      lambda@edge

-----------------------------------------------------------------------------------------------:
CF TTL AND INVALIDATIONS:
  influence how long cf objects are cached and when rejected:
  basic process:
    TTL: 
      determines how long object is valid in cache:
    user requests object from edge location, if object is valid then it is returned: 
      if versioned file name of object in edge location is same as versioned file name in origin, then object is returned
    user requests object from edge location, if not valid then fetched from origin:
      if versioned filed name of object in edge location is not same as version file name in origin, then a 304 not modified is returned
  EXAM: 
    default TTl is 24 hours: 
    you can change TTL values, minimum TTL and maximum TTL,  for all or per object values:
    the minimum and maximum override a per object value if the per object value is not within the constraints
  EXAM: 
    you can set custom per object TTL values via object metadata:
    EXAM: Origin Header: cache-control max-age (seconds), 
    EXAM: Origin Header: cache-control s-maxage (seconds), 
    EXAM: Origin Header: Expires (Date and time)
  invalidations:
    invalidates objects based on a path pattern that you specify:
    applies to all edge locations immediately but takes time to invalidate objects:
    a way to corrects errors and has a cost of using
    EXAM: 
      use versioned file names rather than relying on invalidations:

-----------------------------------------------------------------------------------------------:
AWS CERTIFICATE MANAGER, ACM:
  allows you to easily provision, deploy, manage, public or private SSL/TLS certificates for use with aws services and your internal connected resources:
  ACM lets you run a public or private CA, certificate authority
  Private CA: 
    applications need to trust your private CA
  Public CA: 
    browsers trust a list of providers issued within OS, which can trust other providers
  characteristics:
    can generate or import existing certificates:
    EXAM: 
      if generated, can automatically renew:
    EXAM: 
      if imported, you are responsible for renewal:
    EXAM: 
      can be deployed out to supported services, cloudfront, albs, api gateway:
    EXAM: 
      regional service, cannot leave the region they are generated or imported in, so the certificate needs to be in same region by service accessing it, however cloudfront operates as though its within us-east-1 so use us-east-1 for cloudfront and other global services

-----------------------------------------------------------------------------------------------:
CLOUDFRONT AND SSL:
  EXAM: 
    SSL supported by default as long as you use *.cloudfront.net default domain name:
  to use SSL on custom domain name:
    verify ownership using a matching certificate, HTTPS optional:
  EXAM: 
    generate or import certificate in ACM in us-east-1:
  two SSL connections when using cloudfront:
    viewer protocol: 
      viewer and cloudfront:
    origin protocol: 
      cloudfront and origin:
    EXAM: 
      both need valid public certificates!:
      ALB can use ACM, custom origin needs to use an imported generated cert:
  billing:
    SNI is free, dedicated host costs around 600$/mo per distribution
    SNI, server name identification: 
      allows a client to tell a server which domain name its attempting to access, happens at TCP layer, modern browsers support this:
      allows one IP for edge location to host many HTTPS websites that need their own certificate
      old browsers don't support SNI
      you can established a dedicated host ip address for edge location within CF

-----------------------------------------------------------------------------------------------:
ORIGIN TYPES AND ARCHITECTURE:
  s3 buckets:
    EXAM: 
      does not include website hosted on a s3 bucket:
    EXAM: 
      protocol for origin protocol and viewer protocol are always the same:
      custom headers available:
      you can restrict access to only CF, via origin access control.  access is granted via trusted key groups or trusted signer:
    aws media package channel endpts
    aws media store container endpts
  everything else are custom origins (web servers):
    EXAM: 
      includes website hosted on a s3 bucket:
    minimum origin ssl protocol: 
      minimum ssl protocol that cf uses with the origin can differ from viewer protocol:
      customize http port and https port between origin and edge location
    custom headers available: 
      custom origins does not have origin access control like s3 buckets so if you want to ensure connections only from cf you can do so via custom headers:

-----------------------------------------------------------------------------------------------:
SECURING CD AND S3 USING OAI:
securing connection between edge locations and origins:
  for s3 origins:
    oac, origin access control (new version of oai, origin access identity):
      type of identity that allows us to control access to s3 origins:
      oac allows cf to sign requests going to s3 origin and need to update s3 bucket policy to allow oac:
      requires users to access content via cloudfront url instead of using direct url for file:
    legacy: 
      cf becomes oai when accessing s3 origin and the oai can be used in s3 bucket policies to restrict everything but that or more OAIs
  for custom origins (2 ways):
    require custom header: 
    use firewall that only allows IPs of edge locations:

-----------------------------------------------------------------------------------------------:
PRIVATE DISTRIBUTION AND BEHAVIORS:
  securing connection between edge locations and viewers:
    behaviors are either public or private
    EXAM: 
      private behaviors require signer, which is entity that provides signed cookie or url:
  
  2 ways of configuring private behaviors:
    EXAM: 
      old way, a cf key is created by an account root user, given to account, the account is added as a trusted signer
    EXAM: 
      new way, create trusted key groups and add those as signers, gets put on the distribution:

  Signed URLS vs signed cookies:
    EXAM: 
      urls provide access to one object only:
      use urls if your client doesn't support cookies:
    EXAM: 
      cookies provide access to groups of objects, all files of a type:
      for members, send the required 'set-cookie' headers to the viewer which will unlock the content only to them:
      use cookies if you want to keep application url the same, signed urls change url format:

-----------------------------------------------------------------------------------------------:
Lambda@Edge:
  feature of CF that allows you to run lightweight lambda functions at edge locations:
  allows lambda to adjust data between the viewer and origin:
  you cant access any vpc based resources since it runs in aws public space:
  currently supports node.js and python
  layers are not supported
  different limits vs normal lambda fns
  architecture:
    lambda fns can run at four different events of architecture (origin req/res, viewer req/res):
    has a 30s timeout:
  use cases:
    perform a/b testing with viewer request function
    migration between s3 origins, origin request
    different objects based on device, origin request
    content by country, origin request
    many more

-----------------------------------------------------------------------------------------------:
GLOBAL ACCELERATOR:
  comprised of anycast IP addresses: 
    allow a single IP to be in multiple locations.  routing moves traffic to closest location:
    traffic initially uses public internet and enters a global accelerator edge location, from the edge, data transits globally across the aws global network leading to significantly better performance:
    moves the aws network closer to customers
  EXAM: 
    customer connections enter at edge, using anycast IPs:
  EXAM: 
    transits over aws backbone to 1+ locations:
  
  CLOUDFRONT VS GLOBAL ACCELERATOR:
    GA used for non http/s (TCP/UDP) where CF can cache HTTP/HTTPS content only:
    GA moves your connection/data faster to desired location whereas CF aims to cache content closer to you:
    GA does not cache any content:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
ADVANCED VPC NETWORKING:
*******************************************
VPC FLOW LOGS:
  helps provide details of traffic flow within private network:
  captures metadata, not contents, ex srcip, dstip, srcport, dstport, protocol, etc, they can be configured to capture accepted, rejected, or all metadata

  can be attached to:
    can be attached to a vpc, which monitors every eni in that vpc:
    can be attached to a subnet, which monitors every eni in that subnet:
    can be attached to an eni directly:
 
  not real time:
  can be logged to s3 or cloudwatch logs, or athena for querying

-----------------------------------------------------------------------------------------------:
EGRESS-ONLY INTERNET GATEWAY:
  allows connections to be initiated inside vpc to outside only, and allows response traffic back in:
  like NAT for ipv4, but egress only is for ipv6:
  architecture:
    similar to NAT gateway

-----------------------------------------------------------------------------------------------:
VPC ENDPTS - GATEWAY endpt:
  allows a private only resource within a VPC to access dynamodb and s3:
  used in lieue of providing infrastructure like NAT:
  implemented via PrivateLink:
 
  regional, can't access cross-region services
  s3 buckets can be set to private only by allowing access only from a gateway endpt 
  gateway endpts are not accessible outside the VPC
  implementation:
    does not get deployed to subnet, instead prefix list added to route table of subnet that points traffic to gateway endpt.  
    gateway endpt is HA across all AZs in a region by default:
    endpt policy: 
      controls what things can be connected to by that gateway endpt:

-----------------------------------------------------------------------------------------------:
VPC ENDPTS - INTERFACE endpt:
  allows a private only resource within a VPC to any service but dynamodb:
  used in lieue of providing infrastructure like NAT:
  added to specific subnets, to a single ENI, so not HA:
  network access controlled via security groups
  endpoint policies: 
    controls what things can be connected to by that gateway endpt
  only support TCP and IPV4 protocols
  implemented via PrivateLInk:
    allows external services to be injected into your VPC:
  implementation:
    endpt provides a new service dns name so services within vpc can access new service via dns name via endpt
    regional dns
    zonal dns
    privateDNS: 
      overrides the default DNS for services so the services can still retain same dns name, however when services within vpc try to access, they do so via endpt interface rather than accessing service directly

-----------------------------------------------------------------------------------------------:
VPC PEERING:
  allows you to create a private and encrypted network link between two VPCs:
  works between same/cross region and same/cross account:
  option of public hostnames resolve to private IPs
  if in same region, security groups can reference peer security groups
  EXAM: 
    does not support transitive peering, vpcs cannot connect via intermediate vpcs:
    routing configuration is needed on both sides, SGs and NACLs can filter:
  EXAM: 
    peering connections cannot be created where there is overlap in the VPC CIDrs, so don't use the same address ranges in multiple VPCs:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------: 
*******************************************
HYBRID ENVIRONMENT AND MIGRATION:
*******************************************
BGP, BORDER GATEWAY PROTOCOL:
  routing protocol that controls how data flows from different points, based on shortest path of interconnected networks:
  operates over TCP/179, it's reliable:
  not automatic, peering is manually configured:
  made up of many autonomous sytems (AS): 
    routers controlled by one entity, a network in BGP, details abstracted away from BGP
    allocated unique ASN via IANA, 16-32 bit numbers, allows BGP to identify networks
  path vector protocol:
    exchanges the best path (shortest) to a destination between peers, called the ASPATH
    by default exchanges the shortest ASPATH, but you can influence via path prepending, which can be used to make a certain path look like multiple paths if the connection is slow or unfavorable
  iBGP: 
    internal BGP, routing within an AS
  eBGP: 
    external BGP, routing between AS's, used most often with AWS

-----------------------------------------------------------------------------------------------:
IPSEC VPN FUNDAMENTALS:
  group of protocols that allows you to set up secure tunnels across insecure networks between two routers (peers):
  provides authentication and data traveling through tunnels is encrypted:
  interesting traffic:
    traffic that matches certain patterns to be transported via vpn tunnels
  two main phases:
    IKE phase one: 
      slow and heavy, protocol for how keys are exchanged within vpn
      authenticate with certifice or pre shared authentication
      uses asymmetric encryption to create a shared symmetric key
      creates SA, security association, phase 1 tunnel
    IKE phase two:
      fast and agile, uses the symmetric keys agreed in phase 1
      agrees on encryption method, and IPSEC key used for bulk data transfer
      creates IPSEC SA, phase 2 tunnel running over phase 1
  policy based VPNS:
    rules created to match traffic, 
    traffic for each rule is sent over a pair of SAs with unique IPSEC key
  route-based VPNS:
    do target matching based on prefix
    traffic is sent over a single pair of SAs
    one IPSEC key
    provides less functionality but simpler to set up

-----------------------------------------------------------------------------------------------:
AWS SITE TO SITE VPN (KNOW EVERY LINE) (SEE SLIDE):
  logical connection between a VPC and on premises network encrypted using IPSec, running over the public internet
  EXAM: 
    highly available, if you design and implement correctly
  EXAM: 
    can be provisioned in less than an hour
  EXAM: 
    speed limitations around 1.25Gbps
  EXAM: 
    latency inconsistency possible over the public internet
    can be used as a backup for Direct Connect, DX
    can be used with Direct Connect, DX
  cost: 
    aws hourly cost, GB out cost, data cap (on premises)
  components:
    virtual private gateway, VGW:
      has same functionality as internet gateway, except
      has endpoints located in different AZs
    customer gateway, CGW:
      logical gateway that corresponds to customers router
    VPN connection between the VGW AND CGW:
      vpn tunnels connect the endpoints and the CGW
  architecture:  
    partially highly available design:
      single point of failure lies with the customer gateway
    highly available design:
      having two customer gateways, with two different VPN internet connections and seperate buildings
  
  static vs dynamic vpn:
    static:
      static routes of the other network are manually configured for each network
      no load balancing and multi connection failover
    dynamic:
      BGP is configured between VGW and CGW using ASN, network info is exchanged via BGP
      multiple VPN connects are possible providing HA and traffic distribution
      route propogation: 
        if enabled means routes are added to route tables automatically

-----------------------------------------------------------------------------------------------:
DIRECT CONNECT, DX:
  a physical connection (1, 10, or 100 Gbps):
  low and consistent latency, highest speeds, high throughput:
  can access VPCs (private services) and aws public services, not public internet though:
  aws allocates you a port and authorizes you to use port at a DX location, you need to arrange connection to that port
  no resilience for physical connection
  EXAM: 
    not encrypted, a private encryption can be created by using a site to site VPN over public VIF:
  connection: 
    business premises => dx location => aws region
  cost: 
    port hourly cost and outbound data transfer
  DX location:
    EXAM: 
      is a large regional data center not owned by aws, aws rents out space and equipment, the DX endpts called DX routers:
      customer or comms partner cage is space rented out by you or comms partner, your router is called customer DX router:
    EXAM: 
      if you don't have customer cage, then a comms partner can extend the DX port into your business premises:
  Cross connect: 
    connection between AWS DX Router port and customer/partner dx router port

-----------------------------------------------------------------------------------------------:
DX RESILIENCE AND HA:
  see slide:
  multiple dx routers connect to multiple customer dx routers:
  provision several customer premises with routers:

-----------------------------------------------------------------------------------------------:
TRANSIT GATEWAY, TGW:
  network transit hub that helps connect VPCs to on premise networks:
  allows you to significantly reduce network complexity
  signle network object,  highly available and scalable
  allows attachments to other network types:
    VPC, Site to site VPN, and Direct Connect Gateway:
  benefits:
    supports transitive routing, very helpful between VPCs:
    can be used to create global networks via connecting to different TGW in different regions and different accounts:
    share between accounts using AWS RAM

-----------------------------------------------------------------------------------------------:
STORAGE GATEWAY:
  virtual machine on premises that helps provide low latency access to data by caching frequent data while storing archive data securably and durably in aws:
  integrates with EBS and S3:
  think integration:
  presents storage using iSCSI, NFS, or SMB
  EXAM: 
    helps with extensions of a data center into AWS, storage tiering, DR and replacement of backups systems:
    you keep your applications and migrate your on premise file share network to aws:
    small datasets at a time:

-----------------------------------------------------------------------------------------------:
STORAGE GATEWAY- VOLUME:
  EXAM: 
    iSCSI raw block devices:
  architecture:
    cached mode:
      EXAM: 
        main location for data is not on premises, rather in s3 in aws: 
        on premise has local cache that stores frequently accessed data:
      EXAM: 
        allows data center extension:
    stored mode: 
      presents volumes over iSCSI to servers running on premises
      source of storage is on premise storage:
      upload buffer asynchronously copies data to SG endpt in aws, data then sent to EBS snapshots
      EXAM: 
        great for full disk backups of servers and for disaster recovery:
        doesn't improve datacenter capacity, storage gatweay VM is source of storage
        32 volumes per gateway, 16TB per volume, 512TB per gateway
-----------------------------------------------------------------------------------------------:
STORAGE GATEWAY- TAPE (VTL, virtual tape library):
  EXAM: 
    allows you to do tape backups in aws:
  some large backups are written to tape, up to 60TB compressed
  the tape medium allows write as a whole or read as a whole:
  slower times to access:
  architecture:
    has upload buffer and local cache on premise
    on premise server implements iSCSI to modify virtual tapes, which  sends instructions to endpt, which communicates with virtual tape library in aws running on s3, and archives from s3 into tape shelf with unlimited storage in glacier
-----------------------------------------------------------------------------------------------:
STORAGE GATEWAY - FILE GATEWAY:
  EXAM: 
    allows you to link local on premise file storage to s3:
  EXAM: 
    mount points (shares) available via NFS or SMB:
  EXAM: 
    primary data is held in s3:
  on premise files map directly onto an s3 bucket, are visible as objects in an s3 bucket and vice versa:
  read and write caching ensures LAN-like performance
  bucket share:
    one aws s3 bucket linked to on premise file share
  10 bucket shares per file gateway
  use the NotifyWhenUploaded API to notify other gateways when objects are changed 
  file gateway doesn't support object locking, so if two rewrites occur at same time there can be data loss.  To prevent this, use read only mode on all shares besides one or tightly control file access:
  when a file share modies a file on premise, it gets replicated to s3, but s3 doesn't notify other fileshares automatically of change

-----------------------------------------------------------------------------------------------:
SNOWBALL/ EDGE / SNOWMOBILE:
  allow you to move large amounts of data in and out of aws:
  physical storage devices range from suitcase or truck:
  snowball:
    ordered from aws, log a job, device delivered
    any data on device is encrypted using KMS
    50TB or 80TB capacity
    1 Gbps or 10 Gbps network
    EXAM: 
      economical range when to use is 10TB to 10PB:
    EXAM: 
      you can order multiple snowballs to multiple premises:
    EXAM: 
      helps only with storage:
  snowball edge:
    EXAM: 
      both storage and compute capability:
    larger capacity vs snowball and faster networking
    10 Gbps, 10/25 SFP, 45/50/100 Gbps
    Storage optimized option: with EC2, 1TB SSD
    Compute optimized option: 100TB + ...
    Compute with GPU optimized option: same but with a GPU
    EXAM: 
      ideal for remote sites or where data processing on ingestion is needed:
  snowmobile:
    portable datacenter within a shipping container on a truck:
    EXAM: 
      economical range when to use is for one location when data is 10PB - 100PB:

-----------------------------------------------------------------------------------------------:
AWS DIRECTORY SERVICE:
  allows you to provide multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other aws services:
  runs within a VPC
  HA by deploying into multiple AZs
  supports SAML 2.0, as AD does:
  some aws services need a directory, ex Amazon Workspaces:
  can be isolated, independent of any other directories, or integrated with existing on premises system:
  can act as proxy back to on premises:
  directory:
    stores objects( users, groups, computers, servers, file shares) with a structure (domain/tree) and uses this information to manage access to information and resources:
    commonly used in windows environments, Microsoft Active Directory Domain Services, AD DS:
    sign in to multiple devices with same credentials provides centralized management for assets
    multiple trees can be grouped into a forest
  architecture:
    simple AD mode:
      standalone directory which uses Samba 4, open source:
      not designed to integrate with any existing on premises: 
      use case:
        default. simple requirements. a directory in aws.:
      small mode is up to 500 users, large mode is up to 5000 users
      integrates with aws services
    managed microsoft AD:
      full microsoft AD DS running in AWS:
      primary running location in aws, resilient if the vpn fails:
      use case:
        applications in aws which need AD DS, or you need to trust on premise AD DS:
        optional direct connect or vpn connection set up between aws and on premises directory:
    AD connector:
      when you want to use only one aws service that requires you to implement AD service:
      only a proxy to integrate your aws service with your on premise directory via VPN
      use case:
        use aws services which need a directory without storing any directory info in the cloud, is a proxy to your on premises directory:

-----------------------------------------------------------------------------------------------: AWS DATASYNC:
  data transfer service to and from aws for large datasets:
  think migration:
  helps with manual tasks of migrations, data processing transfers and then back out, archival/cost effective storage:
  designed to work at huge scale:
  keeps metadata
  built in data validation
  features:
    scalable, around 100TB per day:
    bandwidth limiters
    incremental and scheduled transfer options
    supports compression and encryption
    automatic recovery from transit errors
    aws service integration s3, EFS, FSx:
  architecture:
    communicates beween storage and datasync agent via NFS or SMB:
    connection between on premises DataSync agent and aws datasync endpt is encrypted via TLS:
    from there it can be stored in s3, EFS, or FSx for Windows server, public or VPC
    schedules can be set for certain time periods
    customer impact can be minimized by setting a bandwidth limit
    task: 
      a job within datasync, what is being synced, how quickly, source and destination locations
    agent:
      software used to read or write to on premises data stores using NFS or SMB
    location:
      every task has from and to locations

-----------------------------------------------------------------------------------------------:
FSx FOR WINDOWS FILE SERVER:
  fully managed native windows file servers/shares (windows shared file system product):
  integrates with Directory Service or self managed AD in aws or on premises:
  can be deployed into a single AZ or Multi-AZ in VPC:
  designed for integration with windows environments
  can perform on-demand and scheduled backups
  accessible using VPC, peering, VPN, direct connect
  encryption at rest via KMS and enforced encryption in transit
  supports volume shadow copies (file level versioning)
  EXAM:
    VSS, user driven restores: 
    allows user to restore files and folder:
    Native file system accessible over SMB:
    Windows permission model:
    supports DFS, distributed file system, way you natively scale out file systems in windows:
    managed, no file server admin:
    integrates with DS or your own directory:
    use when you want to migrate your on premise application and file share to aws:

-----------------------------------------------------------------------------------------------: FSx FOR LUSTRE:
  managed implementation of Lustre file system (fs designed for high performance workloads):
  supports Linux based instances, and POSIX style permissions:
  accessible over VPN or direct connect
  data is lazy loaded from s3 to file system, can be exported back to s3 at any point
  use cases:
    machine learning, big data, financial modeling:
  deployment types:
    persistent:
      longer term, HA in one AZ, self healing
    scratch:
      optimized for high performance, not HA or resilient 

-----------------------------------------------------------------------------------------------:
AWS TRANSFER FAMILY:
  managed file transfer service that supports transferring to s3 and EFS:
  provides managed servers which support protocols other than s3 and efs, would be useful when integrating with existing workflows or using MFTW to create new workflows:
  multi az, resilient and scalable
  cost: 
    provisioned server per hour + data transferred
    protocols (KNOW ALL OF THESE):
      FTP, file transfer protocol: 
        unencrypted file transfer
      FTPS, file transfer protocol secure: 
        file transfer with TLS encryption
      Secure Shell File Transfer protocol, SSH: 
        file transfer over SSH
      Applicability Statement 2, AS2: 
        structured b2b data
      Identities: 
        service managed, directory service, custom (lambda/api gateway)
      managed file transfer workflows, MFTW: 
        serverless file workflow engine
      *with FTP and FTPS only Directory Service or Custom IDP are supported
  architecture:
    transfer family service uses IAM roles to access s3 and efs
    think of it like the front end access points to backend storage (efs and s3)
  access via:
    public: 
      no configuration required, does not support FTP or AS2:
    VPC<->INTERNET:
      supports SFTP and FTPS protocols:
      SG and NACL can be used to control access
    VPC<->INTERNAL:
      supports SFTP, FTP, and FTPS protocols:
      SG and NACL can be used to control access

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
SECURITY, DEPLOYMENT, AND OPERATIONS:
*******************************************
AWS SECRETS MANAGER:
  EXAM: 
    designed for secrets (passwords, API keys...), focuses on storage and rotation:
  it does share functionality with parameter store
  usable via console, CLI, API, or SDKs
  secrets are encrypted using KMS:
  EXAM: 
    supports automatic rotation, this uses lambda, invoked periodically:
  EXAM: 
    directly integrates with some aws services like RDS:

-----------------------------------------------------------------------------------------------:
APPLICATION ( LAYER 7 ) FIREWALLS (KNOW ALL):
  EXAM: 
    layer 7 firewalls are aware of the layer 7 protocol, ie HTTP, headers, data, DNS, rate
  EXAM: 
    can identify normal or abnormal requests that focus on protocol specific attacks
  EXAM: 
    if connection is encrypted, (HTTPS), then it is terminated (decrypted) on the firewall, inspected, then new encrypted connection is sent between l7 firewall and backend
  EXAM: 
    data can be blocked, replaced, or tagged 

-----------------------------------------------------------------------------------------------:
WEB APPLICATION FIREWALL:
  aws service that implements layer 7 firewall:
  can protect both global and regional services:
  architecture:
    EXAM: 
      designed to create a feedback loop between logs generated by WAF, Applications (Cloudfront, APIGateway, ALB), or ecosystem data to update WEB ACL:
    Web ACL:
      main unit of configuration within WAF, associated with a set of resources:
      rule groups/rules: 
        allow different reactions to different types of traffic:
      EXAM: 
        eventbridge can integrate with and implement IP list parser:
  
    rule groups/rules: 
      processed in order
      rule groups:
        contain rules
        actions of WAF get matched to most specific rule, then to rule group rules
        rule group can be referenced by multiple WEBACL
        WCU max capacity 1500, but can raise ticket for more              
      rules:
        EXAM: 
          each rule contains a statement that defines inspection criteria and an action to take if a web request meets that criteria:
          can match based on body(first 8192 bytes ONLY), origin country, IP, label, header, cookies, query parameter, URI path, query string, HTTP method:
          have a compute requirement based on their complexity:
          WEB ACL CAPACITY UNITS (WCU): 
            general measurement of complexity, max default 1500, but can be increased via support ticket: 
        syntax:
          contain type, statement, action
        two types:
          rate based rules:
            tracks the rate of requests for each originating ip address and triggers rule action on IPs with rates that go over a limit, protects from excessive requests:
          regular rule:
            matches the statement defined in rule:
  price: 
    charged 5$ monthly per WEBACL
    charged 1$ monthly per rule
    charged per requests per WEBACL, 0.60$/1 million  

  AWS FIREWALL MANAGER:
    integrates with WAF to centrally and simplify defining your rules and resuse across all the web apps you wish to protect:

-----------------------------------------------------------------------------------------------:
AWS NETWORK FIREWALL:
  stateful, managed network firewall for your VPC that filters traffic at perimeter of your VPC:
  enable in VPC service:

-----------------------------------------------------------------------------------------------:
AWS SHIELD 101:
  Two tiers:
    both offer protection against DDoS attacks:
    Shield Standard:
      free, automatically works in the background:
      protects against common Network (L3) or Transport (L4) attacks:
      protects R53, Cloudfront, Global accelerator:
      protection at the perimeter, ie at your region/VPC or the AWS edge of cloudfront
    Shield Advanced: 
      cost: 3,000 per month per organization, 1 year lock in + data out:
      protects R53, Cloudfront, Global accelerator, and anything associated with EIPS (ec2), ALBs, CLBs, NLBs:
      access to proactive engagement and aws shield response team (SRT):
      not automatic, must be explicitly enabled
      cost protection for unmitigated attacks
      features:
        health based detection, application specific:
        real time visibility:
        WAF integration protects layer 7 attacks:
        protection groups

-----------------------------------------------------------------------------------------------:
CLOUDHSM (KNOW ALL LINES):
  service that creates, manages, and secures keys
  different from KMS because not shared between customers (with KMS, aws manages keys and service is shared between customers but seperated accounts)
  EXAM: 
    true "single tenant" hardware security module, HSM
  EXAM: 
    aws provisioned, fully customer managed
  EXAM: 
    fully FIPS 140-2 Level 3, (KMS is L2 overall, some L3)
  EXAM: 
    implements industry standard APIs, PKCS#11, Java Cryptography Extensions (JCE), Microsoft CryptoNG (CNG) libraries
    can integrate with KMS, KMS can use CloudHSM as a custom key store
  use cases:
    EXAM: 
      no native integrations between HSM and any aws service needed
    EXAM: 
      offload the SSL/TLS processing for web servers
    EXAM: 
      enable transparent data encryption TDE for Oracle databases
    EXAM: 
      protect the private keys for an issuing certificate authority (CA)

-----------------------------------------------------------------------------------------------:
AWS CONFIG:
  records configuration changes over time on resources:
  ensures tracking for auditing of changes, compliance with standards but does not prevent changes from happening:
  regional service, supports cross region and cross account aggregation
  configuration items: 
    are like snapshots of resource at that time
  config rule changes: 
    changes can generate sns notifications and near realtime events via eventbridge and lambda:

-----------------------------------------------------------------------------------------------:
AMAZON MACIE:
  allows you to discover, monitor, and protect data stored in s3 buckets:
  automated discovery of data ie PII, PHI, finance:
  aws managed data identifiers, built in, ML/patterns:
  you can build custom identifiers, regex based:
  data security and data privacy service
  integrates with security hub and eventbridge

-----------------------------------------------------------------------------------------------:
AMAZON INSPECTOR:
  EXAM: 
    scans ec2 instances and the instance OS and container workloads to identify any security vulnerabilities and deviations against best practice:
  EXAM: 
    provides a report of findings ordered by priority:
    rule packages determine what is checked:
  two different assessments:
    network assessment: 
      agentless
    network and host assessment: 
      agent:
      checks end to end reachability, ie EC2, ALB, DX, ELB, ENI, IGW, ACLs, RTs, SGs, subnets, VPCs, VGWs and VPC peering:
      Packages (agent required):
        EXAM: 
          CVE, common vulnerabilities and exposures:
        EXAM: 
          CIS, center for internet security benchmarks:
        EXAM: 
          security best practices for amazon inspector:

-----------------------------------------------------------------------------------------------:
AMAZON GUARDDUTY:
  continuous security monitoring service:
  analyses supported data sources (dns logs, vpc flow logs, cloudtrail event logs, CT management events, and CT s3 data events):
  identifies unexpected and unauthorized activity that is irregular of account:
  implements AI/ML plus threat intelligence feeds
  can notify or perform event driven protection/remediation
  supports multiple accounts via master and member structure

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
INFRASTRUCTURE AS CODE (CLOUDFORMATION):
*******************************************
cloudformation: 
  allows you to create templates to provisions AWS resources using IAC:

CLOUDFORMATION PHYSICAL AND LOGICAL RESOURCES:
  template: 
    contains logical resources and their properties and allows us to build a stack with corresponding physical resources:
    written either in YAML or JSON:
    should be portable, ie allow you to create resuable templates, avoid naming things explicitly and avoid using region specific values like AMIs:
      contains:
        resources: 
          contains at least one logical resource, each resource has type and properties
        description: 
          gives some details about the template, decription has to immediately follow AWSTemplateFormatVersion if there is one
        metadata: 
          controls how resources look in the UI, and other things
        parameters: 
          fields that prompt user for more information
        mappings: 
          optional, it allows you to create lookup tables
        conditions: 
          allow decision making in template that only occur if conditions are met
        outputs: 
          once the template is finished, it can produce outputs
    
  stack: 
    created by template: 
    builds a physical resource in your account for each corresponding logical resource in template:
    keeps resources in sync:
      if you update the stack, it updates the corresponding physical resources:
      if you delete the stack, it deletes the physical resources:

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION TEMPLATE AND PSEUDO PARAMETERS (KNOW ALL LINES):
  template and pseudo parameters allow input via console, CLI, API when a stack is created or updated
  can be referenced from within logical resources
  psuedo and template parameters can be used together
  template parameters:
    default, or explicit input values
  pseudo parameters:
    parameters injected by aws, they are always available, and are populated based on environment when creating stack
    ex, aws::region

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION INTRINSIC FUNCTIONS:
  allow you to gain access to data at runtime:
  your template can take action based on how things are when template is creating stack:

  Ref:
    !Ref parameterName: 
      returns value of template or pseudo parameters
    !Ref logicalResourceName: 
      returns physical ID of logical resource 
  Fn::GetAtt:
    !GetAtt LogicalResource.Attribute: 
      can be used to retrieve any attribute associated with the resource

  Fn::GetAZs:
    !GetAZs "us-east-1" or !GetAZs "": 
      returns a list of AZs of explicit region or if "" gets list of AZs in current region
  Fn::Select:
    !Select [0, !GetAZs '']: 
      returns an object from a list of objects

  Fn::Join:
    !Join["delim", ["string1, ..., stringn"]]: 
      joins strings with delimiter in between
  Fn::Split: 
    !Split["delim", "string"] : 
      splits string into list based on delimiter

  Fn::Base64:
    Base64: plaintext : 
      accepts plaintext and outputs base64 encoded text
  Fn::Sub: 
    !Sub | inputText : 
      substitutes variables in the input text, with their actual runtime values, format is:  
      ${Parameter}
      ${LogicalResource}
      ${LogicalResource.AttributeName}

  Fn::Cidr:
    !Select [ 0, !Cidr[ !GetAtt VPC.CidrBlock, "16", "12" ]]: 
      references the cidr range of a vpc, you can input how many subnets to generate, and bits per CIDR, this select statement gets first item in list of available cidr ranges

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION MAPPINGS:
  templates can contain a Mappings object which maps keys to values, allowing lookup:
  can have one key, or top and second level
  !FindInMap [ MapName, TopLevelKey, SecondLevelKey]: 
  common use case:
    retrieve AMI for given region and architecture
  EXAM: 
    help you improve template portability: 

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION OUTPUTS:
  help provide status information or how to access physical resources created by stack:
  EXAM: 
    values can be declared in this section which will be visible as ouptuts when using the CLI or console UI and can be accessed from a parent stack when using nesting:
  EXAM: 
    values can be exported, allowing cross stack references:
  optional within a template

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION CONDITIONS (KNOW ALL):
  allow you to provide conditions that are evaluated as true or false and processed before resources are created
  When processing resources, if a condition key/value is present, the resource is only created if the condition is true
  located in optional conditions section of a template
  common condition use cases:
    how many AZs to create resources in
    size of instances depending on PROD or DEV environment
  
-----------------------------------------------------------------------------------------------:
CLOUDFORMATION DependsOn:
  allows you to explicitly define formal dependencies between resources within templates:
  when creating stack:
    cf does things in parallel
    tries to determine a dependency order via references or functions or you can explicitly state dependencies with DependsOn:
  EXAM: 
    an elastic IP requires an IGW attached to a VPC in order to work, but since there is no explicit !Ref in template defn then there could be an error when provisioning stack:
    solution:
      implement DependsOn property to explicitly state the dependency

-----------------------------------------------------------------------------------------------:
CF WaitCondition, Creation Policy, and cfn-signal:
  configures cf to hold to process a resource:
    wait for X number of success signals within defined timeout period for those signals, 12 hr max and if signals received, then status of resource moves to CREATE_COMPLETE:
    if failure signal or timeout is reached, creation fails:
  
  resources being created implement either a CreationPolicy signal or WaitCondition signal:
    CreationPolicy: 
      allows us to appply signal requirement and timeout:
      used for simple creation of ec2, auto scaling groups, and simple creation requirements
    WaitCondition:
      is its own resource:
      used to pass data back to cloudformation or implement general wait states in template that does not proceed until after signal has been received:
      can depend on other resources, and other resources can depend on the waitCondition
      generates a PreSigned URL for resource signals, JSON passed back in signal response

-----------------------------------------------------------------------------------------------:
NESTED STACKS AND CROSS REFERENCE STACKS:
  problem of single stack:
    500 resource limits per stack:
    can't easily reuse resources, ex a VPC that you want to use in diff stacks:
    can't easily reference other stacks:

  NESTED STACKS:
    architecture:
      EXAM: 
        whole templates can be reused in other stacks!:
        root stack:
          only stack to be created manually by entity
          has parameters and outputs just like normal stack
        parent stack:
          parent of any stack that it immediately creates, contains its own nested stack
    benefits:
      use when you want to overcome the 500 resource limit of one stack:
      code reuse via modular templates, nested stacks can reference resources:
      installation process is easier because nested stacks are created by root stack:
      EXAM: 
        use only when everything is lifecycle linked: 
    
  CROSS STACK REFERENCES:
    allow one stack to reference another stacks outputs:
    EXAM: 
      outputs can be exported, making them visible from other stacks:
    EXAM: 
      exports must have a unique name in the region:
    EXAM: 
      !ImportValue resourceName:  
        can be used instead of Ref to reference region exported values
    export attribute: used to export the output of a stack to that region's export list
    Use case:
      EXAM: 
        service oriented, different lifecycles, stack outputs reuse:

-----------------------------------------------------------------------------------------------:
CLOUDFORMATION STACK SETS:
  allow you to use CF to deploy CF stacks across many accounts and regions:
  stacksets are containers in an admin account that contain stack instances, which are references to actual stacks
  stack instances remain even when stack fails
  target accounts: 
    contain stack instances and stacks 
  each stack runs in 1 region in 1 account
  security:
    self managed roles: 
      manually create roles and use self managed roles to get permissions
    service managed roles: 
      use CF with aws organizations 
  terms:
    concurrent accounts: 
      how many individual aws accounts can be used at same time
    failure tolerance: 
      amount of individual deployments can fail before stackset as a whole fails
    retain stacks: 
      option to retain stacks even after stack instances have been deleted
  use cases:
    enable aws config: 
    aws config rules:
    create iam roles for cross account access:

-----------------------------------------------------------------------------------------------:
CFN DELETION POLICY:
  DeletionPolicy attribute: 
    allows you to preserve or backup a resource when its stack is deleted:
    you specify a deletionPolicy attribute for each resource you want to control
    only applies to delete operations, not replace operations
      delete: 
        default behavior without deletionPolicy
      retain: 
        preserves a resouce if stack is deleted
      snapshot: 
        if supported, remember that snapshots continue on past stack lifetime so you will have to clean up manually to avoid recurring charges

-----------------------------------------------------------------------------------------------:
CFN STACK ROLES:
  EXAM: 
    by default, cfn uses the permissions of the logged in identity so in order to create, update, or delete resources the identity needs those permissions:

  stack roles: 
    allow an IAM role to be passed in stack via PassRole which allows role seperation:
  PassRole: 
    EXAM: 
      allows stack to use this role rather than the identity interacting with the stack to create, update, and delete resources:

-----------------------------------------------------------------------------------------------:
CFN-INIT:
  allows you to provide configuration information to an ec2 instance, alternative to user data:
  allow a desired state configuration management system to be implemented within CFN:
  AWS::CloudFormation::Init : 
    located in template, this part of logical resource allows you to include metadata on an EC2 instance for the cfn-init helper script:
    states desired state, leaves implementation details up to system:
    defines which configkeys to use and in which order to apply
  cfn-init script:
    helper script that is installed on ec2 OS and runs during bootstrapping (user data):
    looks for resource metadata rooted in cfn stack
    supports all metadata types for linux and some for windows

-----------------------------------------------------------------------------------------------:
CFN-HUP:
  daemon which can be installed that helps detect changes in resource metadata, so when a change in cfn::Init is updated, then cfn-init will be ran again:
  without cfn-hup:
    cfn-init is run only once, it doesn't rerun if cfn::init in stack is updated:

-----------------------------------------------------------------------------------------------:
CFN CHANGE SETS:
  allow you to preview how proposed changes to a stack might impact your running resources:
  you can create many different change sets for a stack
  when you execute a change stack, the template is executed and changes are made

-----------------------------------------------------------------------------------------------:
CFN CUSTOM RESOURCES:
  allow to write custom provisioning logic in templates that runs anytime CFN creates, updates custom resource, or delete stacks:
  EXAM: 
    they are logical resources that allow CFN to integrate with anything it doesn't yet, or doesn't natively support:
  EXAM: 
    CFN passes event data to an endpoint that you define within custom resource, then custom resource passes some data back, called ResponseURL:

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
NOSQL DATABASES AND DYNAMODB:
*******************************************
  public service
  capable of handling key/value and document databases
  no self managed servers or infrastructure
  manual or automatic provisioned performance, or on-demand
  highly resilient, across AZs and optionally global:
  really fast, single digit milliseconds for processing of small data that dynamically grows and changes:
  event driven integration, allow you to do things when data changes:
  backups, point in time recovery, encryption at rest:
  EXAM: 
    if you see NoSQL or key/value on exam, likely answer is DynamoDB:
  access via console, CLI, API, but not SQL!
  Billed based on RCU, WCU, storage, and features

  tables:
    grouping of items with the same primary key attribute:
    partion key can be primary key or composite key comprised of partition and sort key:
    each item must have unique primary or composite key and can have none, all, same, or different attributes
    item max 400KB
    capacity:
      speed:
      WCU: 
        write capacity units:
        EXAM: 
          1 wcu = 1kb per second:
      RCU:
        read capacity units:
        exam: 
          1 rcu = 4kb per second:
      on demand capacity: 
        capacity is managed for you
        use when you have unknown, unpredictable, low admin workloads
        price per million R or W units
      provisioned capacity:
        you explicitly set capacity values on a per table basis

  
  backups:
    on demand backups:
      EXAM: 
        manual restore and delete required:
      full copy of table retained until removed:
      restores can be same or cross region, with/out indexes, and you can adjust encryption:
    Point in time recovery, PITR:
      not enabled by default, enabled on a table basis:
      continuous record of changes allows replay to any point in the 35 day recovery window:
      1 second granularity:

-----------------------------------------------------------------------------------------------:
DYNAMODB OPERATIONS, CONSISTENCY, AND PERFORMANCE:
  EXAM: 
    every operation consumes at least 1 RCU/WCU:
  every table has a RCU and WCU burst pool, 300 seconds, that is available if you run out of designated RCU or WCU, if you exceed this pool your table will output an error if R or W operation is performed:

  query operation:
    requires you to query via partion key OR partition key and sort key, allows range for sort key:
  scan operation:
    least efficient operation, but most flexible way to query data:
    consumes the capacity of every item in table:
    allows you to scan range for all attributes that are not partition key:

  consistency:
    deals with when data is read from database, is the data consistent with the most recent version of write:
    architecture:
    two types: eventually consistent and strongly consistent
      one leader storage node is selected out of all storage nodes
      writes are written to leader storage node, leader node replicates data to other nodes
      eventually consistent reads:
        read from any of the storage nodes, 50% price because it uses less RCU, data could be stale, faster read:
      strongly consistent reads:
        reads from leader storage node, not as fast reads:

-----------------------------------------------------------------------------------------------:
DDB LOCAL SECONDARY INDEX AND GLOBAL SECONDARY INDEX, LSI, GSI:
  indexes: 
    allow for more efficient data retrieval operations:
    allow you to provide alternative views on table data(allows alt key to search with, other than primary key):

  LSI (LOCAL SECONDARY INDEX) and GSI (GLOBAL SECONDARY INDEX) allow for an alternative presentation of data stored in a base table
  LSI (LOCAL SECONDARY INDEX):
    EXAM: 
      allow for alternative SKs: 
    EXAM: 
      must be created with a table:
    EXAM: 
      shares the RCU and WCU with the table:
      ensures strong consistency:
      indexes are sparse, capacity is only consumed on corresponding indexes
  GSI (GLOBAL SECONDARY INDEX):
    EXAM: 
      allow for alternative PK AND SKS:
    EXAM: 
      can be created at any time:
    EXAM: 
      are allocated their own RCU and WCU allocations:
    indexes are sparse, capacity is only consumed on corresponding indexes
    are always eventually consistent:
    use as default, use LSI only when strong consistency is required:
  EXAM: 
    for both, some or all attributes can be projected onto view, KEYS_ONLY, INCLUDE, ALL:

-----------------------------------------------------------------------------------------------:
DYNAMODB STREAMS AND LAMBDA TRIGGERS (KNOW ALL):
  streams:
    time ordered list of item changes in a ddb table
    24 hour rolling window
    enabled on a per table basis
    4 view types:
      keys_only, new_image, old_image, new_and_old_images

  triggers:
    item changes added to stream automatically generate an event, triggers lambda fn that takes action using the event data

-----------------------------------------------------------------------------------------------:
DDB GLOBAL TABLES (KNOW ALL):
  EXAM: 
    help provide multi-master cross-region replication
    tables are created in multiple regions and added to the same global table, the tables in the regions become replica tables
  EXAM: 
    last writer wins is used for conflict resolution
    reads and writes can occur to any region
  EXAM: 
    generally sub second replication between regions
  EXAM: 
    strongly consistent reads only in the same region as writes, everything else eventually consistent
  HA, fast performance, and global disaster recover/business continuity (DR/BC)

-----------------------------------------------------------------------------------------------:
DDB ACCELERATOR, (DAX):
  in memory cache designed for dynamo db:
  dax sdk reduces admin overhead, client can now treat cache and ddb as one entity, perform one api call:
  very fast, cache hits are returned in microseconds, misses returned in milliseconds:
  Primary node supports writes, replicas support reads:
  supports write through, (you can write via dax sdk):
  nodes are HA
  you can scale up and scale out
  dax deployed within a VPC

-----------------------------------------------------------------------------------------------:
DYNAMODB TTL:
  EXAM:
    allows you to define a per item timestamp to determine when an item is no longer needed:
    deletes item from table without consuming any throughput or cost:

-----------------------------------------------------------------------------------------------:
AMAZON ATHENA:
  serverless querying service that allows for ad-hoc sql like queries on s3:
  billing is based on amount of data consumed:
  capable of working with unstructured, semi-structured, or structured data
  original data never changed, remains on s3
  schema on read:
    table like translation where schema translates data when read
    output can be sent to other services like Quicksight:
    you can also query other data sources other than s3 w/ athena federated query:
    good for querying aws logs, glue data catalog that have been exported to s3
    demo implementation:
      use DDL to define database, table schema, and source of data
      perform query on table

-----------------------------------------------------------------------------------------------:
ELASTICACHE:
  in memory database (cache) designed for high performance:
  not persistent:
  EXAM: 
    supports redis or memcached engines:
  use cases:
    EXAM: 
      read heavy workloads with low latency:
    EXAM: 
      scaling reads in a cost effective way:
    EXAM: 
      allows externally hosted user session state, allowing stateless servers:
      session state is written to cache and loaded when needed:
    EXAM: 
      using elasticache requires application code changes:
  redis vs memcached:
    redis:
      advanced data structures:
      multi AZ replication:
      replication (scale reads):
      backup and restore:
      transactions, multiple operations are treated as one:
      authenticate via using Redis AUTH command:
    memcached:
      simple data structures:
      no replication:
      multiple nodes (sharding)
      no backups:
      multi threaded:

-----------------------------------------------------------------------------------------------:
REDSHIFT ARCHITECTURE:
  column based, petabyte scale, OLAP data warehousing product implemented in aws or on premises:
  server based, not serverless:
  sql like interface JDBC/ODBC connections:
  integrates with aws tooling like quicksight, firehose, and dms:
  redshift spectrum:
    allows you to directly query s3 without loading it into redshift in advance
  federated query:
    allows you to directly query other DBs without loading it into redshift in advance
  
  architecture:
    one AZ in a VPC, due to network cost and performance:
    automatic snapshots to s3 with option of manual snapshots:
    has all of the integrations a service in vpc has like security, IAM permissions, KMS, CW
    enhanced vpc routing:
      EXAM: 
        when enabled, allows redshift to be routed with vpc networking configurations:
    leader node:
      query input, planning, and aggregation
    compute node:
      contains slices that query data assigned by leader node
    
-----------------------------------------------------------------------------------------------:
REDSHIFT RESILIENCE AND RECOVERY:
  to overcome single AZ risk:
    automatic snapshots to s3 every 8 hours or 5GB, default 1 day retention period, up to 35d:
    optional manual snapshots can be created and deleted at any time
    snapshots can be to same region or cross region

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
MACHINE LEARNING :
*******************************************
AMAZON COMPREHEND:
  NLP service that helps identify valuable insights and connections in text:
  input: 
    document, think text:
  output: 
    entities, phrases, language, PII, sentiments:
  real time analysis for small workloads
  async jobs for larger workloads
  implement via console and CLI, or use APIs to build into apps

-----------------------------------------------------------------------------------------------:
AMAZON KENDRA:
  intelligent search service that helps mimic interacting with a human expert:
  provides data to apps and also integrates with aws services:
  supports wide range of question types:
    factoid: who, what, were
    descriptive: how do I get my cat to stop being a jerk?
    keyword: what time is the keynote address?  address can have multiple meanings. kendra will determine intent
  components:
    index: indexed data is organized in an efficient way
    data source: 
      where your data lives, kendra connects and indexes from this location, many options
    documents:
      data is either structured like in FAQs or unstructered like HTML, PDFS, text, etc

-----------------------------------------------------------------------------------------------:
AMAZON LEX: 
  helps you build conversational interfaces via recognizing speech and understanding language:
  utterance: 
    configured phrase that invokes a given intent:
  intent: 
    performs an action based on gathering user input to fulfill all required slots:
  fullfillment: 
    how the request is completed (business logic) after intent has been fullfilled with all required slots.:
  slot: 
    like a variable, it is input data required to fulfill an intent
  slot type: 
    like a variableName, it is a common phrase or descriptor
  slot value: 
    like a variableValue, it is value provided by customer 
   use case: 
    chatbots, voice assistants, q&a bots, info/enterprise bots:

-----------------------------------------------------------------------------------------------:
AMAZON POLLY:
  allows you to turn text into lifelike speech:
  allows you to create entirely new categories of speech enabled products
  modes:
    standard TTS, text to speech: 
      concatenative (phonemes)
    neural TTS: 
      more compute but more natural sounding
      different output formats
  allows you to implement SSML:
    allows different emphasis, pronounciation, whispering, etc 

-----------------------------------------------------------------------------------------------:
AMAZON REKOGNITION:
  allows extraction of information and insights from images and videos:
  can even analyze video live streams via kinesis streams:
  can identify objects, people, text, activities, content moderation, face detection, face analysis, face comparison, pathing, and much more
  per image or per minute pricing
  integrates with applications and event driven

-----------------------------------------------------------------------------------------------:
AMAZON TEXTRACT:
  helps automatically extract text, handwriting, and data from scanned input documents:
  it can identify, understand, and extract data from forms and tables:
  input: 
    jpeg, png, pdf, or tiff
  output: 
    extracted text, structure, analysis
  use cases:
    document analysis, like names, address, birthdate
    receipt analysis, like prices, vendor, line items, dates
    identity documents, like abstract fields

-----------------------------------------------------------------------------------------------:
AMAZON TRANSCRIBE:
  allows you to convert audio to text:
  you can implement as a standalone transcription service or add speech to text to apps
  features:
    custom vocabularies and language models
    language customization, filters for privacy, audience appropriate language, speaker identification
  use cases:
    full text indexing of audio, allows searching:
    subtitles/captions, transcripts:
    call analytics:
    meeting/gathering notes
    integration with other apps/ aws ml services

-----------------------------------------------------------------------------------------------:
AMAZON TRANSLATE:
  allows you to perform real time language translation:
  fast, high quality, affordable, and customizable 
  allows you to offer a multilingual user experience 
  commonly integrates with other services/apps/platforms

-----------------------------------------------------------------------------------------------:
AMAZON FORECAST:
  allows you to deliver highly accurate time-series forecasts (predicts future demand based on historical data):
  (retail demand, supply chain, staffing, energy, server capacity, web traffic)
  output: 
    forecast and forecast explainability:

-----------------------------------------------------------------------------------------------:
AMAZON FRAUD DETECTOR:
  allows you to automate the detection of potentially fraudulent activities online:
  model types: 
    online fraud, transaction fraud, account takeover
  actions are scored and rules/decision logic allows a reaction based on business activity

-----------------------------------------------------------------------------------------------:
AMAZON SAGEMAKER:
  allows developers to quickly build and train machine learning models, and then deploy into a production ready hosted environment:
  sage maker studio: 
    ide for entire ML lifecylce
  domain: 
    container for a particular projects, ex efs volume, users, apps, policies, VPCs
  containers:
    docker containers deployed to ML EC2 instance
  hosting:
    deploy endpoints for your models
  cost:
    sagemaker has no cost, but resources it creates does

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
OTHER SERVICES AND FEATURES, EXAM TIPS :
*******************************************
AWS LOCAL ZONES:
  type of infrastructure deployment that allows you to place compute, storage, database, and other aws services close to large population and industry centers:
  allow really low latencies bc they are closer to you:
  1 additional zone, so no built in resilience, like an AZ but near your location:
  not all products support them, many are opt in with limitations:
  see slide:
  DX to a local zone is supported
  utilize parent region for certain services like ebs snapshots
  use case: 
    when you need the highest performance:

-----------------------------------------------------------------------------------------------:
EXAM TECHNIQUES:
  exam is comprised of 25% easy, 50% medium, 25% hard
  phase 1: 
    answer questions that take you 20 secs or less
  phase 2:
    identify hard qs and skip, answer medium qs
  phase 3:
    answer difficult qs, finish up with these and if you run out of time guess
  2 minutes per question, assume you will run out of time
  don't guess until end
  use the mark for review feature
  take all the practice tests you can, aim for 90%+ before you do real exam

-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
-----------------------------------------------------------------------------------------------:
*******************************************
TUTORIAL DOJO PRACTICE EXAMS :
*******************************************
KNOW ALL LINES:
AWS COMPUTE OPTIMIZER:
  recommends optimal compute allocation:
  reduces costs and improve performance by using machine learning to analyze historical utilization metrics
  generates recommendations for the following resources:
    ec2 instances, auto scaling groups, ebs volumes, and lambda functions

AMAZON MANAGED SERVICE FOR PROMETHEUS:
  serverless, monitoring service for container metrics that makes it easier to securely monitor containers at scale
  monitor containers running on ec2, ecs, and eks
AMAZON MANAGED GRAFANA:
  secure data visualization service that allows you to instantly query, correlate, and visualize operational metrics, logs, and traces
  use it together with amazon managed service for prometheus

AWS APPLICATION MIGRATION SERVICE:
  highly automated lift and shift solutions that helps migrate your on premises and/or cloud servers to serverless architecture on aws
  once your applications are running on aws, you can leverage aws services and capabilities to easily replatform or refactor applications
  different that datasynce or snowball because it migrates applications, not just raw data

AWS APPLICATION DISCOVERY SERVICE:
  helps you plan your migration to the aws cloud by collecting usage and config data on your on premise servers
  integrates with aws migration hub to aggregrate migration statuses into a single console
AWS MIGRATION HUB:
  helps provide a single place to discover your existing servers, plan migrations, and track status of each application migration

AMAZON QUANTUM LEDGER DATABASE (QLDB):
  fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority

ELASTIC BEANSTALK:
  allows you to upload your application, and automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring
  when you deploy your application, it builds the selected supported platform version and provisions one or more aws resources to run your application
  reduces management complexity without restricting choice or control

AWS LAKE FORMATION:
  service that makes it easy to set up a secure data lake in days
  data lake:
    centralized, curated, and secured repository that stores all of your data, both in its original form and prepared for analysis
  this was solution to combining multiple sources of data from different aws accounts s3 buckets to a single centralized data lake

AWS ARTIFACT:
  central resource for compliance related information
  includes SOC reports, PCI reports, and certifications that validata the implementation and operating effectiveness of aws security controls

AWS RESOURCE ACCESS MANAGER, RAM:
  allows you to share aws resources with any aws account or within your aws organization
  you can share aws transit gateways, subnets, aws license manager configurations, and route 53 resolver rules

IAM DB AUTHENTICATION:
  feature of RDS that allows you to authenticate to your DB instance using IAM 
  works with MySQL and PostgreSQL
  uses an authentication token 

AURORA NATIVE FUNCTIONS OR STORED PROCEDURES:
  you can invoke a lambda function when data changes from amazon aurora with a native function or a stored procedure
  different from rds events because rds events can only capture operational events such as db instance events that deal with a db instance is created, modified, or deleted

S3 ACCESS POINTS:
  network endpts that simplify data acess for any aws service or customer application that stores data in s3
  has distinct permissions and network controls that s3 applies for any request that is made through that access point

best way to track billing:
  enable tags on resources, activate tags in Billing and Cost management console
  aws then generates a cost allocation report as a CSV file
  gives you a breakdown on individual department or units based on tags
AWS Budget:
  allows you to run custom actions if your budget thresholds are exceeded
Cost and Usage report:
  gives you breakdown of costs based on aws services

AWS PROTON:
  allows you to deploy any serverless or container based application with increased efficiency, consistency, and control
  you can define infrastructure standards and effective cd pipelines with standardized service templates
  components allows developers to add supplemental resources
  exam:
    application specific container standardizations
  
AWS SWF, simple workflow service:
  web service that makes it easy to coordinate work across distributed application components

AWS EMR:
  managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, to process and analyze vast amounts of data
  performs ETL

AWS DLM, DATA LIFECYCLE MANAGER:
  automates the updation of snapshots taken to back up your amazon ebs volumes
  provides a complete backup solution for ebs volumes at no additional cost
  
AWS SECURITY TOKEN SERVICE:
  allows you to create and provide trusted users with roles that create temporary security credentials
  works almost identically to long term access key credentials that IAM users can use

AWS IAM IDENTITY CENTER (NEWER VERSION OF SSO):
  provides single sign on access for all ofyour aws accounts and cloud apps
  connects with AD through aws Directory Service to allow users in that directory to sign in to a personalized aws access portal using their existing AD user names and password





  