IAM access keys: long term credentials, used with IAM users only,
use access keys when using CLI,
dont change automatically or regularly,
similar to user name and password, but differences are:
an IAM user can have two access keys, either 0, 1, or 2,
can be created, deleted, made inactive or active,
formed from two parts: access key ID, and secret access key
access key id is like username, while secret acces key is like password

aws configure: allows us to configure the default configuration for CLI
aws configure --profile namedProfile: named profiles, allow us to configure multiple aws accounts to our CLI instead of just using one account, whenever running commands append '--profile nameOfProfile' to run from NameOfProfile account


NIST definition of cloud computing: 5 characteristics of cloud computing 
On demand self service: allows you to provision capabilities as needed without requiring human interaction
broad network access: capabilities are available over the network and accessed through standard mechanisms
Resource pooling: about abstraction, there is a sens of location independence and resources are pooled to server multiple consumers using a multi tenant model
Rapid elasticity: resources can be elastically provisioned and released to scale rapidly in real time with no human interaction
Measured service: Resource usage can be monitored, controlled, reported, and billed 


Public vs Private vs Hybrid vs Multi Cloud Models:
multi-cloud: using multiple public clouds for higher levels of availability and durability
private: aws outposts, is on premise and still meets 5 requirements of cloud computing, most on premise traditional datacenters do not meet 5 requirements
hybrid: using both public cloud and private cloud cooperating together as a single environment

hybrid environment: using public cloud on your on premise infrastructure


YAML: language for defining data or configuration
characteristics:
unordered collection of key-value pairs, (dictionary)
indentation matters, spaces are like brackets



Encryption:
  

Encryption at rest: helps to secure data from physical access, example would be encrypting data stored on a hard drive, and decrypting when it reads.  Secret(password) is used to decrypt the data.
In cloud computing, data is encrypted while in s3
encryption at transit: helps to secure data that is being transferred between two place, encrypted with an encryption tunnel

Plaintext: un-encrypted data, data that you can load into an app and use
algorithm: code that takes plain text and an encryption key and generates encrypted data
ciphertext: encrypted data

key: "password", decrypts ciphertext
  different types: symmetric key
    symmetric key: same key is used for both encryption and decryption processes, used by a single party
    asymmetric key: comprised of public and private keys, public key is used to encrypt, the private key is used to decrypt, used by two or more parties

signing: allows verification of who sent the encrypted message,
        requires the sender to sign with private key,
        requires the receiver to decrypt signature with sender's public key

steganography: allows you to embed data inside other data, like hiding ciphertext in an image



NETWORKING:


OSI 5 LAYER MODEL: networking stack, that includes seven software layers for transporting data
  physical, data link, network, transport, session, presentation, application layers
  layer x understands its layer and below, built on top of other layers

  media layers: how data is moved between point A and B
  physical, data link, network, 
  
  host layers: how data is chopped up and reassembled for transport and formatted to be understood by both sides of a network connection,
  transport, session, presentation, application layers

Physical Layer: Layer 1: defines the transmission and reception of raw bit streams between a device and a shared physical medium.

can be different physical mediums, copper(electrical), fibre(light), or WIFI(RF)
no device addressing at layer 1, all data is processed by all devices
if multiple devices transmit at once a collission occurs
no media access control and no collision detection
tend to not scale very well

  hub: anything received on any port, is transmitted on every other port, including errors and collisions


Data Link Layer: Layer, 2: packages frame to be sent on layer 1, the physical layer
  ethernet: most popular l2 protocol used generally for local networks 

  MAC address: unique hardware address

  frames: format for sending information over layer 2 network, can be addressed to MAC address or broadcast to all
    payload: the data the frame carries from source to destination.  It's generally provided by layer 3 and the ET attribute defines which l3 protocol is used

  CSMA/CD: provides controlled access to physical medium via:
  carrier sense multiple access: checks carrier(to see if anything is transmitting on layer 1), and if no carrier then passes frame to layer 1 to transmit, if there is carrier it waits to send until a carrier is not detected
  collision detection: detects if there was collision, if there was collision reattempts collision after a certain amount of time

  switch: intelligent layer 2 device that works similarly to a hub but much more efficient because it understands layer 2 
  HOW IT WORKS:
  learns which devices are connected at layer 2 and populates a MAC address table with that MAC address's port
  if it receives frame for MAC address in table, it orders that frame to be sent to that port only on layer 1, unlike a hub which would send to every single port
  if it receives frame for MAC address not in table, it sends to all ports
  every x port has X collision domains, meaning if there is a collision on a port, it only occurs on that single port

Network Layer, Layer 3: gets data from one location to another
uses packets which are similar to frames,
generally an ip packet doesnt change, while a frame does between each lvl2 destination because a frame has address of next intermediary address and ip packet has source ip address and destination ip address,

contains l4 protocol in packet in protocol field and data received from layer 4 

ipv4 and ipv6 packets, ipv6 packets have larger addresses

ipv4 addressing:
DHCP: machine that assigns IP addresses
Ip addresses have a network part and a host part, if they have same network part they are on same network
subnet masks: divides the host and network part via shorthand prefix(starts from left), letting an IP device know if another device is on same network or not, which influences if it communicates directly with device or default gateway

Every router has one or more route tables, letting router know where to send packet or destination.  Packets are routed, hop by hop across the internet.  

default gateway: IP address on local network that packets are forwarded to if intended desination is not a local ip address, typically a router

ARP, Address Resolution Protocol: finds Mac address for a given IP address,
a protocol that allows us to encapsulate layer 3 packet in a l2 frame, broadcasts to all devices on the ip address network and asks for mac adress, the corresponding device then sends mac address and frame is created
ipv6: 


How to convert decimal to binary for IPv4 addressing: 
133.33.33.7 <=> 10000101.00100001.00100001.00000111
Each number in dotted decimal notation maps to 8 bits


The Transport Layer, Layer 4 and the Session Layer, Layer 5: provides most of the functionality that supports most of the networking of the internet

Transmission Control Protocol, TCP
User datagram Protocol, UDP
packages data in segments, encapsulated within IP packets,
used in same way but TCP adds more reliable architecture,

TCP/IP: TCP is layer 4 protocol running on top of IP, used for most of the important Application Layer protocols like HTTP, HTTPS, SSH, and so on, connection oriented protocol
has source and destination ports, sequence number
connection between random port on client and known port on the server, flags set up connection, makes error checking, ordering, and retransmission possible because everything is connection based

statefull firewall: extension of security group, understands TCP/IP connection so only one rule is needed, initiating request source port and destination port and response traffic is automatically allowed

stateless firewall, (Network ACL): two rules will be required, one to define the outbound traffic source and destination port and the other to allow the inbound traffic source and destination port

UDP/IP: faster because it doesn't have the TCP overhead but less reliable, 


NAT, Network Address Translation: helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse, takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table

static NAT: 1 private to 1 fixed public address, use when a private IP needs access to public internet by using a public IP and where these IPs need to be consistent 
dynamic NAT: 1 private to 1st available public ip in Nat table pool, use when you have less public IPs than private IP addresses and you want to be efficient with how they're used, if there are no available in the pool then public access can fail
port address Translation: many private to 1 public address, use for home router, use when you have many devices that use a single public address, NAT gateway with AWS use this way, uses ports to identify devices by using a source and destination port for each packet, 

NAT table: maps one to one private Ip to allocated public ip address, not configured though


ipv6 addressing:
340 sextillion address spaces vs 4 billion ipv4 address spaces
ipv6 doesn't need any network translation because there are so many more addresses


CIDR : defines a way of expressing a size of a network,
subnetting: taking a larger network and breaking down into smaller networks, each of which has a higher prefix
entire internet: /0
class a network: /8
class b network: /16
class c network: /24
one ip address: /32
takeway: larger the subnet mask prefix, the smaller the network
every number (x) you increment in prefix, the number of networks it splits that prior network into goes up by 2^x



DDOS attacks: attacks designed to overload websites, 
compete against legitimate connections,
distributed and therefore hard to block individual IPs/ranges
involve large armies of compromised machines (botnets)
3 types:
application Layer - HTTP flood: take advantage of imbalance of processing between client and server, send countless requests
protocol attack - SYN flood: takes advantage of the connection based nature of requests, spoof a source ip address and server can't complete stage 2 of handshake and waits an amount of time before it stops trying consuming resources
volumetric- DNS amplification: takes advantage of how certain protocols like DNS take only a small amount of data to make request and require large response



SSL, Secure Sockets Layer AND TLS, Transport Layer Security: provide privacy and data integrity between client and server
both do the same thing but TLS is newer and more secure version,

provides private communications that are encrypted first asymmetric and then symmetric
provides identity verification two ways possible, but generally client verifying server
provides reliable connection, detects data alteration

TLS phases that occur after TCP connection is made between client and server:
Cipher suites: set of protocols used by TLS that are agreed on for communications, server sends server certificate that contains public key
authentication: client needs to validate server certificate and public key is valid,  verifies via the CA, verifies the server has the private key
key exchange: moves from assymetric encryption to symmetric encryption



Hash Functions and Hashing:
hashing: process where algorithm is used to turn any kind of data into a fixed length representation of that data
hash function: input is data and output is fixed length representation called hash, unique data results in unique hash value, is one way because you can't use hash as input for hash function to get original data
collision: when different inputs result in same hash value, if there are collisions encryption is unreliable and less secure
MD-5: not reliable because data can be manipulated to cause collisions
SHA-256: modern and reliable hashing algorithm


Public vs Private Services:
refers to networking only
public aws service: something which is accessed using public endpoints, located in the AWS public zone and anyone can connect, but permissions are required to access the service
private aws service: something which located within a VPC and is accessed within a VPC or endpoints in a VPC 

Three Different Network Zones:
public internet zone: internet services, anyone can access
aws public zone: aws public services like s3, operates in between public internet zone and aws private zone, users access public aws zone using internet as transit
aws private zone: where VPCs reside,  VPCs are isolated unless configured otherwise, nothing from the internet can access the VPC unless you allow it, can be accessed via AWS VPN or Direct Connect
  internet gateway: 
  allows VPC service access to public internet with an allocated public ip address, 
  allows VPC service access to public aws service as long as data does not touch public internet at any point, 
  allows VPC service to be accessed via public internet by projecting VPC service into AWS public zone and makes part of or whole VPC service accessible as long as VPC service has an allocated public ip address 



AWS GLOBAL INFRASTRUCTURE:
region: 
a physical geographic location that is a collection of availability zones,
geographic separation allows isolated fault domain,
geopolitical seperation allows different governance that is dicted by that region and data stays within that region,
reference by: region code, ap-southeast-2, or region name, Asia Pacific (Sydney)
location control allows increased performance

edge locations: 
generally only have content distribution services, and edge computing, allow us to cache content and lower latency 

availability zone: 
single data center or a collection of data centers within a region
provides redundant networking, low latency, bc if one availability zone is affected the others most likely wont

service resilience:
globally resilient: 
service operates globally with a single database and its data is replicated across several regions, is highly available because it is always available unless every single region fails, ex route53 and IAM
region resilient:
service that operates in a single region with one set of data per region.  replicates data across multiple availability zones within that region.
AZ resilient:
service that operates in a single availability zone, if availability zone fails then that service fails



VIRTUAL PRIVATE CLOUD BASICS:
virtual private cloud, allows you to create a secure virtual private network in the AWS cloud where you secure your services and resources
connects:
allows you to connect your on premise data center to your AWS VPCs, and also allows you to connect to other cloud platforms when you're creating a multi cloud deployment

is within 1 account and 1 region, regionally resilient
private and isolated unless you decide otherwise

Default VPC: 
max of 1 per region, come preconfigured by aws, a lot less flexible than custom VPCs, you can remove and redeploy if you want
DEFAULT VPC CIDR: 172.31.0.0/16
to replicate across availabity zones: the CIDR range is broken into subnets so that there is a subnet for each availability zone
Subnets assign public IPv4 addresses: anything deployed in their subnets get deployed with public addresses
preconfigured with: IGW, SG, & NACL

Custom VPCs: 
can have many in 1 region, configure them in any way that you want, require you to configure everything end to end and private by default,



EC2 BASICS:
default compute service within AWS
allows you to rent and manage virtual servers in the cloud elastically, ec2 instances are the virtual servers running on the physical servers
instance: operating system configured in a certain way with a certain set of allocated resources
private service by default: uses VPC networking
resiliency: AZ resilient
features:
different instance sizes and capabilities, you can set when you provision or some settings can be changed even after deployment,
on demand billing, per second,
different types of storage: many, including on host storage or Elastic Block Store, EBS
states:
running and stopped:can toggle between states, like off/on switch
terminated:fully deleted once you move to termination state
charges:
cpu, memory, disk, and networking, charged per second,
charged for all four while running, 
charged for disk only while stopped,

AMIS:
AMI => EC2 => AMI:
ec2 instance can be generated from an amazon machine image, or generate an ami from ec2

ami: like a server image which can be used to create virtual machines or a usb device to install OS
permissions: which allow or deny access to the ami,
public: everyone allows
owner: private, implicitly allows the owner to create ami from ec2,
explicit: owner explicitly allows access to specific aws accounts

root volume: the c drive in windows, the drive that boots the OS
block device mapping: determines which volume is root volume and which volume is a data volume

CONNECTING TO AN EC2:
since we have different OS available for instances, we connect to different ports based on the OS
3389: Remote Desktop Protocol, windows instances
22: SSH Protocol, Linux instances



S3 Basics:
global object storage service that is regional based because data is stored inside a specific region and never moves unless you explicitly move it,
default storage service for AWS,
Regional resilient,
public service, unlimited data and multi-user,
economical
accessed via: UI, CLI, API, HTTP

objects: 
data that s3 stores, like a file
composed of key: like a fileName
composed of value: data or value of the object, can range from 0 bytes to 5TB
other components: version ID, metadata, access control, and subresources

buckets: 
containers that store objects
deployed in a specific aws region
blast radius: region
name needs to be globally unique
unlimited number of objects
structure: flat, all objects are stored at the root level.  However, displayed like there is a file structure in UI
configurations: most configurations for s3 are set at bucket level

exam powerup: 
bucket names are globally unique
3-63 characters, all lower case, no underscores
start with a lowercase letter or a number
can't be IP formatted
buckets have 100 soft limit, 1000 hard limit per account
unlimited objects in bucket, 0 bytes to 5 TB
key=name, value = data

Patterns and Antipatterns:
s3 is an object store, not file or block
you can't mount an s3 bucket as K:\ or /images, block storage has a single user limitation and s3 does not have that limitation
great for large scale data storage, distribution, or upload
great for offload, which deals with storing data on s3 bucket rather that ec2 instance
should be default input for any aws services or output to most aws products


Cloudformation Basics:
allows you to create templates to provisions AWS resources using IAC
written either in YAML or JSON

template: tells stack what logical resources to contain and their properties 
contains:
  resources: containes at least one logical resource, they have type and properties
  description: gives some details about the template, decription has to immediately follow AWSTemplateFormatVersion if there is one
  metadata: controls how resources look in the UI, and other things
  parameters: fields that prompt user for more information
  mappings: optional, it allows you to create lookup tables
  conditions: allow decision making in template that only occur if conditions are met
  outputs: once the template is finished, it can produce outputs

!Ref fnReferenceName : references another part of the Cloudformation template
!GetAtt : references another part of the Cloudformation template, and gets attibutes as well 

stack: created by template and contains all of the logical resources the template tells it to contain, then builds a physical resource in your account for each corresponding logical resource.
if you update the stack, it updates the corresponding physical resources
if you  delete the stack, it deletes the physical resources


Cloudwatch Basics:
what is it: collection of services that help you monitor and observe your cloud resources
and it allows you to collect logs, metrics, events, and set alarms
characteristics:
Metrics:AWS Products, apps, on premises
Logs:AWS Products, apps, on premises
Events:AWS services and schedules, 
Namespace:
container for monitoring data
metric:
time ordered set of data points, ex CPUUtilization
Datapoint:
measurement that contains time stamp and value
dimension:
separate datapoints for different things or perspectives with the same metric, ex one ec2 instance cpu utilization
alarms:
takes an action based on a specific metric
ok state and alarm state: once in alarm state it sends trigger to event


High Availability vs Fault Tolerance vs Disaster Recovery:

High Availability:
highly available systems are designed so that when it fails, it is designed to be fixed as quickly as possible.  
maximizes the system's online time, generally measured in percent of uptime of year, like 99.9%
fault tolerance: 
a fault tolerant system operates properly even while faults, which are failures of a system, are present.  Much more expensive than high availability because redundancies are put into place.
disaster recovery:
multiple stage of processes designed to keep the crucial and non replaceable parts of your system safe so when disaster occurs you don't lose anything irreplaceable and can rebuild after disaster


DNS Fundamentals:
discovery service
allows us to translate machine ip addresses into readable language and vice versa
www.amazon.com => 104.98.34.131
it's a huge database and has to be distributed
zone: 
a part of the DNS database that corresponds to a domain, think of it as logical domain, ex amazon.com, contains zone files
zone file:
physical database for a zone that contains the DNS info for a domain, contains the DNS records which has the mapping of website name to IP address
DNS SERVER:
nameserver (NS), server that hosts the zonefiles
DNS resolver Server:
located on your client, or router, or server within your IP, finds the NS you are looking for, then queries the server for the zone file
ROOT OF DNS, or DNS ROOT ZONE:
starting point of the DNS lookup
database hosted on 13 Root servers, 12 large companies manage the servers but not database
delegates authority to top level domains authoritative servers
Root Hints file:
installed on OS'S, pointer to root servers which allows your device to trust root servers

authoritative: trusted
delegated: authoritative entity passes trust to other entity so now other entity is trusted 
top level domains:
part of domain that is immediately left of last . ex:.com .org .uk
registries: organizations that are delegated from IANA to help manage top level domains
registrars: organizations that have relationship with registry that allow you to create a domain registration with them

DNS Record Types:
nameserver: allow delegation to occur in DNS
A, AAAA: map hostnames to IP, a maps to ipv4, aaaa maps to ipv6
CNAME: host to host records, lets you create the equivalent of DNS shortcuts, reduces admin overhead by pointing various services to same A record, so you will only have to update single A record, can only point to names not ips
MX: finds a mail server (SMTP) for a domain
TXT: allow you to add arbitrary text to a domain allowing further functionality, in some cases used to prove domain ownership

TTL: numerical value that can be set on DNS records, once a DNS record from zonefile is recovered by NS, records are cached on resolver server for TTL seconds, caching it so resolver server does not have to perform lookup again for TTL seconds


Route 53 Fundamentals:
allows us to register domains
allows us to host zones files on managed nameservers which it provides
global service with a single database
globally resilient

when registering a domain:
checks with TLD to see if domain is available
creates a zone file for registered domain, known as hosted zone
allocates 4 nameservers for the zone,
adds nameserver records to TLD that indicate that the namerservers are authoritative for domain

hosted zone:
zone files in AWS
hosted on four managed NS
can be public or private linked to VPCs
stores records, known as recordsets



*******************************************
IAM, ACCOUNTS, AND AWS ORGANIZATIONS:
*******************************************

IAM policies:
JSON document that allows or denies permissions for IAM users, groups, and roles
1 or more statements
Sid: statement id, optional field, allows you to provide summaryName of statement to quickly identify what it does, 
Action: can provide a wildcard * which means "all", list of multiple independent actions, or a specific individual action 
["service: operationName"]
Resource: same options above as action, 
["arnResourceNames"]
Effect: either allow or deny, controls what aws does if action and resource match

Every interaction you have with AWS, is a combination of resource you're interacting with and the actions that your attempting to perform on that resource
Policy precedence when two policies overlap permissions(ordered in highest precedence vs least):
1. Explicit deny 
2. explicit allow
3. default deny (implicit), aws identities have no access to any resources (besides root)

Inline Policies:
applying JSON to each account individually
use only when you need a special or exceptional allow or deny for a single user
Managed Policies: 
create policy, then attach to any identity that wishes to use it
reusable, low management overhead
  aws managed policies: managed and created by AWS
  customer managed policies: created and managed by you specific to your needs


IAM Users and ARNs:
entities you create in IAM to represent the person or application needing to access your aws resources
principal: 
represents an entity trying to access aws account
authenticates via username and password or access keys
authenticated identity:
principal that's proved it is who it says it is, aws performs authorization once the authenticated identity tries to access a resource

ARNS:
uniquely identify resources within any AWS accounts
formats differ depending on what your trying to do:
arn:partition:service:region:account-id:resource-id
arn:partition:service:region:account-id:resource-type/resource-id
arn:partition:service:region:account-id:resource-type:resource-id

fields are split by a colon, if you see ::: the field does not have to be specified, like for s3 buckets region or account id do not have to be specified because buckets are globally unique

ex of similar looking arn accessing different things:
arn:aws:s3:::catgifs : accesses bucket
arn:aws:s3:::catgifs/* : objects in the bucket

EXAM POWERUP:
5000 IAM users per account
iam user can be a member of 10 groups
therefore: if you have a system which requires more than 5000 identities then you cant use one IAM user for each identity, IAM roles and Identity Federation fix this 


IAM Roles:
roles allow user or group to temporarily assume permissions via policies,  
two types of policies:
trust policy:
controls which identities can assume that role, can ref identities in same or diff acount, 
if idenity has access to role then temporary security credentials are made available that expire after a given amount of time
permissions policy:
allows or denies permissions for aws resources and services
sts:AssumeRole is policy given to access temporary security credentials and perform whatever permissions it was granted
WHEN TO USE?:
generally used when an unknown number of users is uncertain
use when a set of credentials are needed, bc we dont want to hard code into code or on service because it is security risk as well as problematic if we ever need to change or rotate keys (roles use temporary security credentials)
emergency or out of the usual situations
when you're adding aws into an existing corporate network, external accounts or external identities can't be used to interact with AWS directly, instead assign the external identities a  role for them to interact with aws
users > 5000, use Web identity federation that uses roles
give users from one account access to resources in another aws account via roles
service linked roles:
it is a IAM role linked to a specific aws service
predefined by a service that provide permissions that a service needs to interact with other AWS services on your behalf, service might create or delete the role or allow you to during setup or within IAM
you can't delete the role until it's no longer required!
ListRoles and passRole permissions: 
when you want a group the ability to use roles but not create them


Organizations: allows you to centrally manage multiple AWS accounts under one umbrella,
benefits:
consolidated billing, cost effective with little to no overhead (has volume discounts), account governance
comprised of:
member accounts and management account
oranizational root:
top level of hiearichal structure, container which contains member accounts, management account, and Organizatinal Units, OUs
OUs: organization units, contains member accounts or other OUs
SCPs: service control policies, help you restrict permissions you want everyone in org to follow
best practice:
have single account to handle logins with identities or identity federation, then identities use roles to access the other member accounts

SCPS:
policy document that limits what the account can do, including root users.  They do not grant permissions.
they grant access to what can and cant be allowed, but identities still need identity policies in to grant that permission.  So an identity needs SCPs to allow and also identity policies to allow to access resources.
They can be attached to individual accounts, OUs, and the organizational root.  they inherit down the organization tree.
management account is never affected by SCPs
Allow list:
block everything by default, and allow certain services
Removes FullAWSAccess policy and add any services that can be allowed access for users for account
more overhead but more secure
Deny List:
allow everything by default via FullAWSAccess policy, and deny certain services
default, when you enable SCPS applied to entire organization


Cloudwatch Logs:
public service
store, monitor, and access logging data
many aws integrations
can generate metrics based on logs via metric filter
contains log events, log streams, log groups put defn in from building demo

CloudTrail:
logs api actions which effect aws accounts
not realtime, there is a delay!
cloudtrail event:
logged api call/activity
event history:
events stored for 90 days by default, no cost
Different types of events:
management, data, and insight events
by default only logs management events
management: logs management operations performed on resource 
data: logs the resource operations performed on or within a resource
insight: logs any unusual activity, errors, or user behavior in your account

Trails:
customize the service
regional service, logs events for the region it's created in
two types: 
one region trail: only ever in region it was created in and logs these events only
all regions trail: collection of trail in every region but managed as single logical trail
global service events:
some global services like IAM, STS, Cloudfront log their events only to one region, need to have this enabled to log these events
event storage:
stores events in a definable s3 bucket, can be stored indefinately
can store events in cloudwatch logs as well to use metric filter or search through data
organizational trail:
single management point for every api call made across the organization


AWS Control Tower:
allows the quick and easy setup of multi-account environments
orchestrates other AWS services to provide its functionality including: 
Organizations, IAM Identity Center, CloudFormation, Config, and more...
Landing zone:
multi-account environment, what most people interact with
provides SSO/ID federation, centralized logging and auditing
home region:
region you initially provision into, always available
contains two OUs: the foundational OU named Security and the custom OU named sandbox
foundational: 
contains two accounts, the Audit Account and Log Archive account
custom:
generally used for testing and for account factory
Account factory:
automates creating, updating, and deleting AWS accounts as your business needs them
changes based on templates and implements Cloudformation to provision 
automates: account provisioning, identities with appropriate permissions, guardrails, account and network standard configurations, and can be fully integrated with a businesses SDLC

other features:
guard rails: 
detect and mandate rules/standards across all acounts
rule categories:
mandatory, strongly recommended, or elective
preventative functional type of rule: stops you from doing things via Organization SCPs
detective functional type of rule: performs compliance checks via config rules
dashboard:
single page oversight of the entire environment
you can create other OU's and accounts in a real world environment




*******************************************
S3:
*******************************************
private, by default, account root user is only idenity which can access the bucket

bucket policies:
form of resource policy
resource policy:
like identity policies, but are attached to resources instead of identities.  You are controlling who has access to that resource. 
resource policies have principal field, whereas an identity policy does not have one because it is implied
benefit:
allow/deny access to identies in same or DIFFERENT accounts (diff than identity policies)
allow/deny anonymous principals

ACLS, Access Control Lists:
allow a way to provide security on objects and bucket, basically limited resource policies
used less often, aws recommends you do not use them and instead use bucket policies
subresources
inflexible and simple permissions because you cannot apply conditions like bucket policies and you cannot apply a certain ACL on a group of objects, you would need to apply several ACLs

Block Public Access:
adds a further level of security so that if you do accidentally allow public and anonymous access to bucket via resource policy, then access is still blocked

EXAM PowerUP:
when to use resource policies, identity policies, or ACLs:
identity: controlling access to multiple resources because not every service supports resource policies, you have a preference for a single place to control permissions, working with permissions in same account
resource policies: managing permissions on a single service, allow anonymous or cross account access
ACLs: never, unless you must!


static website hosting:
normal access is via AWS APIs
this feature allows access via HTTP
index and error documents are set
website endpoint is created
you can have custom domains via R53 and bucket name matters
use cases for s3 static website hosting:
offloading and out of band pages
offloading:
store static data from website on s3, have compute service handle dynamic content and then retrieve static data from s3 to save money on storage
out-of-band-pages:
show error or other notification pages on static s3 site in case server is offline or ec2 service as a whole

pricing:
cost to store data on s3 expressed as GB/month fee
data transfer fee from out of s3 expressed in GB
cost for requesting data, diff operations have different costs per 1000 operations

Object Versioning and MFA Delete:
versioning:
lets you store multiple versions of objects within a bucket.  operations which would modify objects generate a new version.
states: disabled, enabled, and suspended
originally disabled, once you enable you can never disable again but can suspend and reenable
implementation:
each object has a key(name) and an id attribute.  
disabled: all ids are set to null.  Whenever changes are made to object, the original object is overwritten.
enabled: ids are given to each object.  Whenever changes are made to object, the original object stays in bucket with same id, while a new object with same key but diff id is generated.  When you access objects from bucket with key the current version is returned.  You can specify which version you want returned with key and id.   
When deleting, if you specify key and id the corresponding object is deleted and the current version marker is reset to corresponding object.  If you do not specify id, then all objects are hidden under delete marker.  To undo delete you can delete the delete marker.  

MFA delete:
enabled in versioning configuration of a bucket
MFA is required to change bucket versioning state or to delete versions
implementation:
serial number of MFA + code are passed with API calls


S3 Performance Optimization:
problem for Single PUT upload:
single data stream to s3 requires full restart if any part of stream fails, then upload fails
speed and reliability is limited in a limit of 1 stream
limited to transferring up to 5GB
solution for Single PUT upload:
Multipart Upload
data is broken up, and each individual part is treated as a single stream, if parts fails, then those individual parts are restarted
minimum data size is 100MB

How global transfer of data to s3 buckets work:
without s3 accelarated transfer, we have to use public internet to route data from source to destination, doesn't usually take an optimal route, 
with s3 transfer acceleration:
transfers data to closest edge location, then transfers data from there directly to closest edge location to s3 destination bucket, 
much faster because the aws network is built for performance between regions
by default, disabled, so you have to enable if you wish to use


KMS, Key management service:
allows you to create, store, and manage keys
both symmetric and asymmetric keys
can perform cryptographic operations, including encrypt and decrypt and others
keys never leave KMS or by default, region, provides FIPS 140-2 L2
regional and public service
support rotation
can create aliases
aws owned & customer owned
customer owned:
aws managed or customer managed keys
aws managed: service default key
customer managed keys:
created explicitly by customer to use in application or service
more configurable, 

KMS keys:
logical key that has ID, data, policy, desc, and state
backed by physical key material (backing key) that is managed by KMS service, we use logical key to have KMS generate, import, or encrypt or decrypt data up to 4 kb in size
implementation:
identity performs call, provides data, KMS creates key and performs operations and always returns data only, not key
role separation:
individual permissions are needed for different operations, like creating key, managing KMS, encrypt, decrypt

DEKs, Data Encryption Keys:
generated by KMS keys via GenerateDataKey call and can be used to encrypt and decrypt data on data greater than 4KB in size
KMS doesn't store the DEK in any way, provides to you or service using KMS to perform crypto operations\
implementation:
when DEK is created, KMS provides you with two versions of that key
plaintext version:
used to encrypt data then discarded
ciphertext version:
encrypted via KMS key
then the encrypted key is stored with encrypted data
when decrypting, you send encrypted key to KMS, it decrypts it and then you decrypt data with the key and then discard key

Key Policies and Security:
key policy:
type of resource policy that has to explicitly grant permission to account owner access to KMS
every key has one
typical way of accessing key:
broad key policy for account + IAM policies
granular key policies
or key policies + grants


s3 Encryption:
buckets aren't encrypted, objects are
each object inside bucket could be using different encryption settings
encryption at rest: client side encryption and server side encryption:
client:
data is encrypted by client, sent as ciphertext to s3 endpoint, then sent to s3 storage
provides you a lot of control, but you do everything when it comes to management
server:
data is sent as plaintext to s3 endpoint, then encrypted by s3 endpoint, and sent to s3 storage 
implementation:
2 components, 
encryption and decryption
generation and management of cryptographic keys
three types available:
differences referenced in slides
SSE-C: ss encryption with customer provided keys
SSE-S3: ss encryption with s3 managed keys, uses AES256 encryption
SSE-KMS: ss encryption with KMS keys stored in KMS, best 

Bucket Default Encryption:
allows you to set default encrytion method used if not specified on object level,
you can set bucket default at object level when uploading object via header x-amz-server-side-encryption or you can set bucket default in the console at bucket level


S3 object storage classes:
s3 standard: 
default
objects are stored across at least 3 AZs
when objects are stored a HTTP/1.1 200 OK respose is provided via s3 api endpoint
no minimums, you are charged per GB for data transferred out and per 1000 requests
recommended for frequently accessed data which is important and non replaceable
s3 standard infrequent access: 
architecture is mostly the same as s3 standard except that it is cheaper to store data but more expensive to retrieve the data
has minimum duration of 30 days for storage and min billing size per object, 128KB
recommended for long lived data, which is important but where access is infrequent
s3 one zone-infrequent access: 
cheaper than s3 or s3-IA
architecture mostly the same as s3-IA, but data is only stored in one AZ
recommended for long lived data, which is non critical and replaceable and infrequently accessed
s3 glacier- instant retrieval:
architecture mostly the same as s3-IA, but cheaper storage, more expensive retrieval costs, and longer minimums
recommended for long lived data, accessed once per quarter with fast, millisecond access
s3 glacier flexible retrieval: 
architecture mostly the same as s3-glacier instant retrievel, but cheaper storage, more expensive retrieval costs, and longer access times
think of objects as chilled state, unable to access immediately
cannot be made publicly accessible, and requires a retrieval process
expedited: 1-5 minutes
standard: 3-5 hours
bulk: 5-12 hours
recommended for archival data, accessed once per year, and minutes-hours retrieval
s3 glacier deep archive: 
cheapest storage option
think of objects as frozen state, unable to access immediately
architecture mostly the same as s3-glacier flexible retrievel, but cheaper storage, much longer access times
standard: 12 hours
bulk: up to 48 hours
recommended for archival data, accessed very rarely if ever, and hours-days retrieval
s3 intelligent tiering: 
helps you automatically move data to most cost effective storage tier based on access patterns, tiers are like classes we have frequent access, infrequent access, archive instant access, archive access, and deep archive
recommended for long lived data with changing or unknown patterns

s3 lifecycle configuration:
set of rules that consist of actions that can automatically transition or delete objects in the bucket
transition actions: change storage classes of objects after x amount of time
can only transition down classes
expiration actions: can delete objects, or versions after x amount of time


s3 replication: 
allows you to configure replication of objects between a source and destination s3 bucket
cross-region replication, CRR:
same-region replication, SRR:
architecture differs depending whether same account or different account
same account:
replication rule is applied to source bucket, dest bucket and IAM role to use
the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
different account:
replication rule is applied to source bucket, dest bucket and IAM role to use
the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
additionally, a bucket policy is needed on destination bucket to allow role from different account access
s3 replication options:
all objects or subset
storage class, default is to maintain same class but you can change
ownership, defualt is source account but you can change
RTC, replication time control, keeps buckets in sync within 15 mins of each other
s3 replication considerations:
not retroactive and versioning for both buckets needs to be on
one way replication only from source to destination
can handle unencrypted objects, SSE-S3, AND SSE-KMS with extra config
source bucket owner needs permissions to objects that will be replicated
will not replicate system events, glacier, or glacier deep archive objects
delete markers are not replicated by default
why use replication?:
SRR- log aggregation
SRR- PROD and TEST sync
SRR- Resilience with strict sovereignty
CRR- global resilience improvements
CRR- latency reduction

s3 presigned urls:
allow you to give an unauthenticated user access to an object inside an s3 bucket using generated credentials from an IAM user in a safe and secure way
credentials are included in URL
see slides for architectures
used when offloading media and access to a private s3 bucket needs to be controlled and you don't want to run application servers to broker that access
EXAM POWERUPS:
you can create a URL for an object you have no access to, (which will also have no access lol)
when using the URL, the permissions match the identity's which generated it CURRENTLY, so if they change so do the URL's permissions
do not generate URL's with a role!  URL stops working when temporary credentials expire


S3 select and Glacier select:
SQL-like statements that allow you to retrieve parts of object rather than whole object
formats of data: CSV, JSON, Parquet, BZIP2 compression for CSV and JSON
why use?:
you might want to retrieve only parts of object to save on computing costs and for faster performance


S3 Events:
allows you to create event notifications when events occur in a bucket
can be delivered to SNS, SQS, and Lambda Functions, all need resource policy allowing s3 principal access
Object created events
Object Delete events
Object restore events
Replication events
EventBridge:
alternative and supports more types of events and more services
would use this instead of s3 events unless you had specific reason not to do so

S3 Access Logs:
Enable logging on source bucket and sends logs to target bucket
s3 log delivery group reads permissions you set, the buck acl on target group allows access to s3 log delivery group
why use?:
provides detailed information about requests which are made to a source bucket that are useful for
security functions
access audits for employees
customer access patterns
understand charges

s3 object Lock:
group of related features that enables Write-Once-Read-Many (WORM) architecture, which means no deletes, no overwrites
Requires versioning, individual versions are locked
Bucket can have default object lock settings
Retention Period and Legal Hold: object lock can have both, one, or none
Retention Period:
specifies days and years which object is locked
compliance mode:can't be adjusted, deleted, overwritten for duration of retention period
governance mode:special permissions can be granted allowing lock settings to be adjusted
Legal Hold:
you set legal hold to be on or off, there is not retention period
no deletes or changes until removed


VPC sizing and structure:
overview:
what size should the VPC be?
are there any networks we can't use, do not overlap networks of VPC's, cloud, on-premises, partners, and vendors
try to predict the future situation
vpc structure: tiers and resiliency availability zones
tiers: 
seperate application components and allow different security to be applied 

VPC minimum /28 (16 IP), maximum /16 (65536)
Personal preference for the 10.x.y.z range
avoid common ranges, like 10.1-10.15
reserve 2+ networks per region being used per account
in our scenario, we have 3 US, 1 Europe, 5 australia, x 2 and assuming max 4 accounts = 40 IP
VPC sizing in depth:
how many subnets will you need?
how many IPs total?  How many per subnet?
formula?:
a subnet is located in one AZ, so determine how many AZs you will be using, he mentions by default he uses 4, 3 for AZs and one spare
each tier has its own subnet in each AZ, by default 4 tiers, web, app, db, and spare so now we have 16 subnets by default


Custom VPCs:
regional service, operates from all of the AZs in that region
allows you to create isolated networks
nothing IN or OUT without explicit configuration
flexible configuration, simple or multi-tier
hybrid networking to other cloud and on premises
default or dedicated tenancy, allows resources created inside VPC to be provisioned on shared or dedicated hardware

can use IPv4 public and IPv4 private addresses, by default uses private
public addresses are used when you want resource to communicate with public internet, or aws public zone, or allow access to them from public internet
allocated one primary private IPv4 CIDR block, configured when you create VPC, optional secondary ipv4 blocks, think of it as a VPC has a pool of private addresses and optionally it can use public addresses
optional signle assigned ipv6 /56 CIDR block

fully featured DNS:
provided by r53
vpc address is base IP + 2
enableDnsHostnames: gives instances DNS names, if set to true then instances with public ip addresses get public DNS hostnames
enableDnsSupport: enables DNS resolution in VPC, indicates whether enabled or disabled


VPC subnets:
a subnetwork of a VPC, within a particular AZ
1 subnet can have only one AZ, one AZ can have zero to many subnets
allow you to add structure, functionality, and resilience to VPCs
AZ resilient 
ipv4 CIDR is a subset of the VPC CIDR, and cannot overlap with any other subnets within VPC
optional ipv6 CIDR
subnets can communicate with other subnets in the VPC
5 reserved ip addresses:
network address: first starting address in available range
network + 1: address used for VPC router
network + 2: address used for Reserved DNS
network + 3: address used for future requirements
broadcast address: last IP in subnet

DHCP Options set: dynamic host configuration protocol
configuration object applied to VPC that allows computing devices to receive IP addresses automatically
one applied to a VPC at one time
if you want to change settings you need to create a new one
two options:
auto assign public ipv4:
automatically assigns public ipv4 address to resource that has private ipv4 address
auto assign ipv6:
automatically assigns ipv6 address to resource


VPC ROUTING AND INTERNET GATEWAY:
internet gateway: 
region resilient gateway attached to a vpc
1 VPC = 0 or 1 IGW, if 0 then it is entirely private
1 IGW = 0 or 1 VPC
Runs from within AWS Public zone, or "border between public zone and VPC"
Gateways traffic between the VPC and internet or between VPC and public aws zone
Managed by aws for performance
maintains mappings of ipv4 public addresses for private instances within VPC:
instances that have ipv4 addressing enabled, have only private ipv4 address
the igw maintains a public ipv4 address mapping to private ip address
when private instance sends packet, the igw replaces the private ip address with public ip address
ipv6 addresses for private instances are natively routable:
instances that have ipv6 addressing enabled, have both private ipv6 address and public ipv6 address
igw passes traffic, it doesn't need to do mapping


VPC ROUTER:
routes traffic between subnets
every vpc has a vpc router, highly available
in every subnet, the router uses network+1 address
controlled by route tables, each subnet has one only
a VPC has a main route table, it is default table for subnet if there hasn't been an explicit association with custom table, when subnet does get associated with custom table the main route table is dissasociated
route table:
basically just a list of routes, matches destination ip address on packet with destination field in route table
if multiple routes match, the most specific, higher prefix route is selected
target field:
point to an aws internet gateway or local, which means destination is within vpc itself


Bastion Host/Jumpbox:
is instance in a public subnet inside a VPC, generally used as an entry point for private only VPCs
historically was only way to implement entry point for private only VPCs but not there are better alternatives
allow incoming management connections and then access internal VPC resources


STATEFUL VS STATELESS FIREWALLS:
two things to think about when dealing with firewall rules:
inbound and outbound:
request and response
inbound and outbound rules can be both requests and responses, just depends on client/server relationship

stateless firewalls:
need rule for each request/response, 2 total
request is always to well known port, and response is always to ephemeral port and since it does not have state, you have to allow traffic to all ephemeral ports
stateful firewall:
recognize connection and treats each request/response as 1 rule, 1 total rule allows/denies request and response is automatically allowed/denied
lower admin overhead

NACL, NETWORK ACCESS CONTROL LISTS:
traditional stateless firewall available within aws vpc
each subnet has one only, NCLS filter traffic crossing the subnet boundary inbound or outbound
connections between things inside subnet are not affected by ACLs
contain rules grouped into INBOUND and OUTBOUND
match IP, port, and allow explicit denies or allow for that match
matches first rule found in order then stops processing, so first rule matched takes effect
default NACL is implemented in a VPC with all traffic allowed
you cannot reference any logical resources, only IPS/CIDR, ports, and protocols
good practice is to implement together with security groups to allows traffic with security groups and deny traffic with NACLS

VPC SECURITY GROUPS:
traditional stateful firewall 
if request is allowed, then response is automatically allowed 
no explicit deny, only allow or implicit deny by not allowing
can't block specific bad actors
support IP/CIDR rules and logical resource references, which includes other security groups and itself, both help scale, you could use self reference to allow intra app communcation 
attached to ENI's, network interfaces, not instances

NETWORK ADDRESS TRANSLATION, NAT, AND NAT GATEWAYS:
NAT, Network Address Translation: set of processes that remap source or destination IPs
static nat: what igw does
ip masquerading: hiding CIDR blocks behind one IP, helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse, takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table
how to implement IP masquerading in AWS, will refer to as ip masquerading as nat:
nat gateway:
provision nat gateway into public subnet with route table, diff route table for private subnet, so that packets sent from private go to nat gateway, then get sent to internet gateway
uses eleastic IP (static ipv4 public address)
az resilient service, deploy a nat gateway in each az for region resilience!
2 charges, one per hour and another per gb of data consumed
nat instance:
implementing ec2 instance as NAT gateway, not really all that important to know because nat gateways are mostly used, however for exam know that if you do implement nat instance, you need to disable source/destination checks
differences between two:
nat instance is managed by you, so you can do other things than just performing nat service, with nat gateway you can only do nat service
nat gateway is highly available within az, nat instance isn't
nat gateways only support NACLS, while nat instances support security groups and NACLS
ipv6:
NAT isn't required for ipv6 and nat gateways don't work with ipv6
all ipv6 addresses in aws are publicly reoutable, the igw works with all ipv6 ips directly
you can enable default ipv6 address route + igw for bi directional connectivity or with egress only igw for outbound only



VIRTUALIZATION 101:
virtualization: process of running more than one operating system on a piece of physical hardware or server
user mode and privileged mode
application needs to make calls to OS(kernel)(privileged state) that makes calls to hardware, apps cannot make system calls directly to hardware

allows multiple privileged programs(os) to make calls to hardware:
emulated virtualization:
host os, which runs in privileged mode, has hypervisor, means it has knowledge of the different guest os and can allow privileges 
virtual machine:
container that contains applications and guest operating system, logical hardware is allocated by hypervisor which has access to physical hardware
binary translation:
guest os not modified, whenever the virtual machine makes call to physical hardware, the hypervisor steps in and makes the call to physical hardware
Para-virtualization:
similar architecture to emulated virtualization, but guest os is modified, instead of making calls to physical hardware, it makes calls to hypervisor 

hardware assisted virtualization:
hardware itself is virtualization aware,
guest os make calls to physical hardware and hardware does not halt execution, but redirects these calls back to hypervisor
SR-IOV:
hardware is split up into several physical hardware pieces available for guest os so no translation has to happen by hypervisor
Enhanced Networking:
this process occurs in EC2 with network cards and improves latency, less cpu usage


EC2 architecture and resilience:
ec2 instances are virtual machines running on the ec2 hosts, which is physical hardware 
ec2 hosts are either shared hosts or dedicated hosts
az resilient, host runs on a single AZ
instance store: local storage on ec2 host
storage networking:
remote storage can attach to elastic block store service
EBS, elastic block store: storage device, a volume, that can be attached to or removed from your instance,
data persists when instance is not running, 
can only be attached to one instance in same AZ,
recommended for quickly accessible data, running a database on an instance, long term data storage

data networking:
elastic network interface, ENI, is provisioned in subnet and maps to physical hardware on ec2 host when an instance is provisioned inside subnet

ec2 instances remain on same host unless host fails or host is stopped and started

What's ec2 good for?:
when you've got a traditional os+application compute
long-running compute needs
server style applications, that are waiting for incoming connections
either bursts or steady state load requirements
monolithic application stacks
migrated application workloads or disaster recovery
tends to be default compute service within aws

ec2 instance types:
characteristics:
raw resources you get, cpu memory, local storage capacity, and type
resource ratios
storage and data network bandwidth 
system architecture/vendor
additional features and capabilities, like gpus, fpgas

five main categories:
general purpose:
default, diverse workloads, equal resource ratio
compute optimized: 
have higher cpu than memory ratio
media processing, HPC, scientific modeling, gaming, machine learning,
memory optimized:
higher higher memory than cpu ratio
processing large in memory datasets, some database workloads
accelerated computing:
have additional features and capabilities
hardware gpu, FGPAs
storage optimized:
large amounts of super fast local storage
sequential and random IO, scale out transacitonal databses, data warehousing, elasticsearch, analytic workloads

Decoding EC2 types:
check out slide
[instance family] [instance generation] [additional capabilities]. [instance size]

ec2 instance connect:
need identity policy permissions 
your browser connects to aws, then it redirects to connect to ec2 instance
using the GUI can only connect to public ec2 instances, can connect to private using CLI
ec2 ssh client:
need access key


Storage Refresher:
ephemeral storage: temporary storage, instance store volumes
  direct(local) attached storage: storage on the ec2 host, called instance store volumes
persistent storage: permanent storages, lives on past the lifetime of the instance, ebs
  network attached storage: volumes delivered over the network (EBS)

3 main categories of storage available in AWS:
block storage: 
Volume presented to the OS as a collection of blocks.  no structure provided. mountable. bootable.  
implemented via SSDs, (physical storage) or by logical volumes backed by SSDs
ideal for booting storage, high performance storage inside os
file storage:
presented as a file share. has structure. mountable. not bootable.
ideal for sharing files among multiple servers, clients, services, etc
object storage:
collection of objects.  no structure (flat). Not mountable.  Not bootable.  
ideal for large access to read and write to data at scale

storage performance:
IO, or block size:
size of blocks of data that your writing to disk
think size of wheels of car
IOPS:
number of io operations the storage system can support in a second
think speed of engine of car, power
throughput: 
rate of data storage system can store on a particular piece of storage, mb/sec
think average speed of car
io x iops = throughput:

EBS, ELASTIC BLOCK STORE:
provides block storage that can be encrypted using KMS
instances see block device and create file system on this device
provisioned in one az, resilient in that AZ
attached to one ec2 instance over a storage network
persistent, they can be detached and reattached to same or different instances
you can perform backups, or snapshots, of data into s3.  You can migrate the data and create volumes from that snapshot.  
can be provisioned with different physical storage types, different sizes, and different performance profiles
billed based on gb-month, and in some cases performance

EBS VOLUME TYPES GENERAL PURPOSE SSD:
up to 16000 IOPS per volume
GP2:
default general purpose ssd storage
high performance storage for fairly low price
can be as small as 1GB or as large as 16TB
io credits and baseline performance found on slide
used for boot volumes, low latency apps, and dev and test environments
GP3:
cheaper than GP2
io credits and baseline performance found on slide
benefits of GP2 and IO1 
used for boot volumes, low latency apps, and dev and test environments, virtual desktops, medium sized single instance databases

EVS VOLUME TYPES PROVISIONED IOPS SSD:
designed for super high performance situations, IOPS can be adjusted independently of size
low latency and consistent low latency are characteristics
faster than gp3
there is a max per instance performance, found on slide
up to 64,000 IOPS per volume
io1:
io2:
up to 256,000 IOPS per volume
io2 block express:
used for high performance, latency sensitive workloads, IO intensive NoSQL and relational databases.  also used when you have small volumes and you need high performance.

EBS VOLUME TYPES HARD DISK DRIVE (HDD) -BASED:
have moving bits, slower but good for specific situations
used for larger, cheaper storage that is frequently accessed for throughput intensive sequential workloads.  think big data, data warehouses, and log processing.
cannot boot
st1:
fast hard drive, not agile
sequentially accessed data
sc1:
cold HDD
designed for infrequently accessed workloads where cheaper storage is priority

INSTANCE STORE VOLUMES:
provide block storage devices
physically connected to one EC2 host
instances on that host can access them
highest storage performance in aws by much higher levels
included in instance price
attached at launch of ec2 instance!
number of and size of instance store volumes depend on type of instance
EXAM POWER-UPS:
local on ec2 host
add at launch only
lost on instance move, resize, or hardware failure
price included in instance
temporary data

CHOOSING BETWEEN INSTANCE STORE AND EBS:
EBS: persistant data
EBS: higher resilience
EBS: storage isolated from instance lifecycle
depends: when you need resilience and app supports in built replication of data, instance stores are an option because you can have many ec2 instances with instance store to provide the resilience and the data is persisted within app
depends: high performance needs
  RAID0 + EBS: max number of IOPS per instance is 260,000 IOPS, used by combination of high number of volumes and efficient architecture
instance store: super high performance needs, more than 260,000 IOPS in millions
instance store: when cost is a primary concern

EBS SNAPSHOTS:
incremental volume copies to s3, improves az resilience to region resilient
the first is a full copy of data on the volume, future snaps are incremental
volumes can be created or restored from snapshots, and can be copied to another region
billing: GB/month of used data, not allocated data

when you create new EBS volume w/out snapshot: full performance immediately
when you create new EBS volume w/ snapshot: restores lazily, the blocks are fetched gradually
to force blocks to be fetched immediately you can implement a forced read of all data immediately with CLI tools or OS tools or you can implement Fast snapshot Restore(FSR)
FSR: 
immediately restores data, up to 50 snaps per region
extra cost