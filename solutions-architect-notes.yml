IAM access keys: long term credentials, used with IAM users only,
use access keys when using CLI,
dont change automatically or regularly,
similar to user name and password, but differences are:
an IAM user can have two access keys, either 0, 1, or 2,
can be created, deleted, made inactive or active,
formed from two parts: access key ID, and secret access key
access key id is like username, while secret acces key is like password

aws configure: allows us to configure the default configuration for CLI
aws configure --profile namedProfile: named profiles, allow us to configure multiple aws accounts to our CLI instead of just using one account, whenever running commands append '--profile nameOfProfile' to run from NameOfProfile account


NIST definition of cloud computing: 5 characteristics of cloud computing 
On demand self service: allows you to provision capabilities as needed without requiring human interaction
broad network access: capabilities are available over the network and accessed through standard mechanisms
Resource pooling: about abstraction, there is a sens of location independence and resources are pooled to server multiple consumers using a multi tenant model
Rapid elasticity: resources can be elastically provisioned and released to scale rapidly in real time with no human interaction
Measured service: Resource usage can be monitored, controlled, reported, and billed 


Public vs Private vs Hybrid vs Multi Cloud Models:
multi-cloud: using multiple public clouds for higher levels of availability and durability
private: aws outposts, is on premise and still meets 5 requirements of cloud computing, most on premise traditional datacenters do not meet 5 requirements
hybrid: using both public cloud and private cloud cooperating together as a single environment

hybrid environment: using public cloud on your on premise infrastructure


YAML: language for defining data or configuration
characteristics:
unordered collection of key-value pairs, (dictionary)
indentation matters, spaces are like brackets



Encryption:
  

Encryption at rest: helps to secure data from physical access, example would be encrypting data stored on a hard drive, and decrypting when it reads.  Secret(password) is used to decrypt the data.
In cloud computing, data is encrypted while in s3
encryption at transit: helps to secure data that is being transferred between two place, encrypted with an encryption tunnel

Plaintext: un-encrypted data, data that you can load into an app and use
algorithm: code that takes plain text and an encryption key and generates encrypted data
ciphertext: encrypted data

key: "password", decrypts ciphertext
  different types: symmetric key
    symmetric key: same key is used for both encryption and decryption processes, used by a single party
    asymmetric key: comprised of public and private keys, public key is used to encrypt, the private key is used to decrypt, used by two or more parties

signing: allows verification of who sent the encrypted message,
        requires the sender to sign with private key,
        requires the receiver to decrypt signature with sender's public key

steganography: allows you to embed data inside other data, like hiding ciphertext in an image



NETWORKING:


OSI 5 LAYER MODEL: networking stack, that includes seven software layers for transporting data
  physical, data link, network, transport, session, presentation, application layers
  layer x understands its layer and below, built on top of other layers

  media layers: how data is moved between point A and B
  physical, data link, network, 
  
  host layers: how data is chopped up and reassembled for transport and formatted to be understood by both sides of a network connection,
  transport, session, presentation, application layers

Physical Layer: Layer 1: defines the transmission and reception of raw bit streams between a device and a shared physical medium.

can be different physical mediums, copper(electrical), fibre(light), or WIFI(RF)
no device addressing at layer 1, all data is processed by all devices
if multiple devices transmit at once a collission occurs
no media access control and no collision detection
tend to not scale very well

  hub: anything received on any port, is transmitted on every other port, including errors and collisions


Data Link Layer: Layer, 2: packages frame to be sent on layer 1, the physical layer
  ethernet: most popular l2 protocol used generally for local networks 

  MAC address: unique hardware address

  frames: format for sending information over layer 2 network, can be addressed to MAC address or broadcast to all
    payload: the data the frame carries from source to destination.  It's generally provided by layer 3 and the ET attribute defines which l3 protocol is used

  CSMA/CD: provides controlled access to physical medium via:
  carrier sense multiple access: checks carrier(to see if anything is transmitting on layer 1), and if no carrier then passes frame to layer 1 to transmit, if there is carrier it waits to send until a carrier is not detected
  collision detection: detects if there was collision, if there was collision reattempts collision after a certain amount of time

  switch: intelligent layer 2 device that works similarly to a hub but much more efficient because it understands layer 2 
  HOW IT WORKS:
  learns which devices are connected at layer 2 and populates a MAC address table with that MAC address's port
  if it receives frame for MAC address in table, it orders that frame to be sent to that port only on layer 1, unlike a hub which would send to every single port
  if it receives frame for MAC address not in table, it sends to all ports
  every x port has X collision domains, meaning if there is a collision on a port, it only occurs on that single port

Network Layer, Layer 3: gets data from one location to another
uses packets which are similar to frames,
generally an ip packet doesnt change, while a frame does between each lvl2 destination because a frame has address of next intermediary address and ip packet has source ip address and destination ip address,

contains l4 protocol in packet in protocol field and data received from layer 4 

ipv4 and ipv6 packets, ipv6 packets have larger addresses

ipv4 addressing:
DHCP: machine that assigns IP addresses
Ip addresses have a network part and a host part, if they have same network part they are on same network
subnet masks: divides the host and network part via shorthand prefix(starts from left), letting an IP device know if another device is on same network or not, which influences if it communicates directly with device or default gateway

Every router has one or more route tables, letting router know where to send packet or destination.  Packets are routed, hop by hop across the internet.  

default gateway: IP address on local network that packets are forwarded to if intended desination is not a local ip address, typically a router

ARP, Address Resolution Protocol: finds Mac address for a given IP address,
a protocol that allows us to encapsulate layer 3 packet in a l2 frame, broadcasts to all devices on the ip address network and asks for mac adress, the corresponding device then sends mac address and frame is created
ipv6: 


How to convert decimal to binary for IPv4 addressing: 
133.33.33.7 <=> 10000101.00100001.00100001.00000111
Each number in dotted decimal notation maps to 8 bits


The Transport Layer, Layer 4 and the Session Layer, Layer 5: provides most of the functionality that supports most of the networking of the internet

Transmission Control Protocol, TCP
User datagram Protocol, UDP
packages data in segments, encapsulated within IP packets,
used in same way but TCP adds more reliable architecture,

TCP/IP: TCP is layer 4 protocol running on top of IP, used for most of the important Application Layer protocols like HTTP, HTTPS, SSH, and so on, connection oriented protocol
has source and destination ports, sequence number
connection between random port on client and known port on the server, flags set up connection, makes error checking, ordering, and retransmission possible because everything is connection based

statefull firewall: extension of security group, understands TCP/IP connection so only one rule is needed, initiating request source port and destination port and response traffic is automatically allowed

stateless firewall, (Network ACL): two rules will be required, one to define the outbound traffic source and destination port and the other to allow the inbound traffic source and destination port

UDP/IP: faster because it doesn't have the TCP overhead but less reliable, 


NAT, Network Address Translation: helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse, takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table

static NAT: 1 private to 1 fixed public address, use when a private IP needs access to public internet by using a public IP and where these IPs need to be consistent 
dynamic NAT: 1 private to 1st available public ip in Nat table pool, use when you have less public IPs than private IP addresses and you want to be efficient with how they're used, if there are no available in the pool then public access can fail
port address Translation: many private to 1 public address, use for home router, use when you have many devices that use a single public address, NAT gateway with AWS use this way, uses ports to identify devices by using a source and destination port for each packet, 

NAT table: maps one to one private Ip to allocated public ip address, not configured though


ipv6 addressing:
340 sextillion address spaces vs 4 billion ipv4 address spaces
ipv6 doesn't need any network translation because there are so many more addresses


CIDR : defines a way of expressing a size of a network,
subnetting: taking a larger network and breaking down into smaller networks, each of which has a higher prefix
entire internet: /0
class a network: /8
class b network: /16
class c network: /24
one ip address: /32
takeway: larger the subnet mask prefix, the smaller the network
every number (x) you increment in prefix, the number of networks it splits that prior network into goes up by 2^x



DDOS attacks: attacks designed to overload websites, 
compete against legitimate connections,
distributed and therefore hard to block individual IPs/ranges
involve large armies of compromised machines (botnets)
3 types:
application Layer - HTTP flood: take advantage of imbalance of processing between client and server, send countless requests
protocol attack - SYN flood: takes advantage of the connection based nature of requests, spoof a source ip address and server can't complete stage 2 of handshake and waits an amount of time before it stops trying consuming resources
volumetric- DNS amplification: takes advantage of how certain protocols like DNS take only a small amount of data to make request and require large response



SSL, Secure Sockets Layer AND TLS, Transport Layer Security: provide privacy and data integrity between client and server
both do the same thing but TLS is newer and more secure version,

provides private communications that are encrypted first asymmetric and then symmetric
provides identity verification two ways possible, but generally client verifying server
provides reliable connection, detects data alteration

TLS phases that occur after TCP connection is made between client and server:
Cipher suites: set of protocols used by TLS that are agreed on for communications, server sends server certificate that contains public key
authentication: client needs to validate server certificate and public key is valid,  verifies via the CA, verifies the server has the private key
key exchange: moves from assymetric encryption to symmetric encryption



Hash Functions and Hashing:
hashing: process where algorithm is used to turn any kind of data into a fixed length representation of that data
hash function: input is data and output is fixed length representation called hash, unique data results in unique hash value, is one way because you can't use hash as input for hash function to get original data
collision: when different inputs result in same hash value, if there are collisions encryption is unreliable and less secure
MD-5: not reliable because data can be manipulated to cause collisions
SHA-256: modern and reliable hashing algorithm


Public vs Private Services:
refers to networking only
public aws service: something which is accessed using public endpoints, located in the AWS public zone and anyone can connect, but permissions are required to access the service
private aws service: something which located within a VPC and is accessed within a VPC or endpoints in a VPC 

Three Different Network Zones:
public internet zone: internet services, anyone can access
aws public zone: aws public services like s3, operates in between public internet zone and aws private zone, users access public aws zone using internet as transit
aws private zone: where VPCs reside,  VPCs are isolated unless configured otherwise, nothing from the internet can access the VPC unless you allow it, can be accessed via AWS VPN or Direct Connect
  internet gateway: 
  allows VPC service access to public internet with an allocated public ip address, 
  allows VPC service access to public aws service as long as data does not touch public internet at any point, 
  allows VPC service to be accessed via public internet by projecting VPC service into AWS public zone and makes part of or whole VPC service accessible as long as VPC service has an allocated public ip address 



AWS GLOBAL INFRASTRUCTURE:
region: 
a physical geographic location that is a collection of availability zones,
geographic separation allows isolated fault domain,
geopolitical seperation allows different governance that is dicted by that region and data stays within that region,
reference by: region code, ap-southeast-2, or region name, Asia Pacific (Sydney)
location control allows increased performance

edge locations: 
generally only have content distribution services, and edge computing, allow us to cache content and lower latency 

availability zone: 
single data center or a collection of data centers within a region
provides redundant networking, low latency, bc if one availability zone is affected the others most likely wont

service resilience:
globally resilient: 
service operates globally with a single database and its data is replicated across several regions, is highly available because it is always available unless every single region fails, ex route53 and IAM
region resilient:
service that operates in a single region with one set of data per region.  replicates data across multiple availability zones within that region.
AZ resilient:
service that operates in a single availability zone, if availability zone fails then that service fails



VIRTUAL PRIVATE CLOUD BASICS:
virtual private cloud, allows you to create a secure virtual private network in the AWS cloud where you secure your services and resources
connects:
allows you to connect your on premise data center to your AWS VPCs, and also allows you to connect to other cloud platforms when you're creating a multi cloud deployment

is within 1 account and 1 region, regionally resilient
private and isolated unless you decide otherwise

Default VPC: 
max of 1 per region, come preconfigured by aws, a lot less flexible than custom VPCs, you can remove and redeploy if you want
DEFAULT VPC CIDR: 172.31.0.0/16
to replicate across availabity zones: the CIDR range is broken into subnets so that there is a subnet for each availability zone
Subnets assign public IPv4 addresses: anything deployed in their subnets get deployed with public addresses
preconfigured with: IGW, SG, & NACL

Custom VPCs: 
can have many in 1 region, configure them in any way that you want, require you to configure everything end to end and private by default,



EC2 BASICS:
default compute service within AWS
allows you to rent and manage virtual servers in the cloud elastically, ec2 instances are the virtual servers running on the physical servers
instance: operating system configured in a certain way with a certain set of allocated resources
private service by default: uses VPC networking
resiliency: AZ resilient
features:
different instance sizes and capabilities, you can set when you provision or some settings can be changed even after deployment,
on demand billing, per second,
different types of storage: many, including on host storage or Elastic Block Store, EBS
states:
running and stopped:can toggle between states, like off/on switch
terminated:fully deleted once you move to termination state
charges:
cpu, memory, disk, and networking, charged per second,
charged for all four while running, 
charged for disk only while stopped,

AMIS:
AMI => EC2 => AMI:
ec2 instance can be generated from an amazon machine image, or generate an ami from ec2

ami: like a server image which can be used to create virtual machines or a usb device to install OS
permissions: which allow or deny access to the ami,
public: everyone allows
owner: private, implicitly allows the owner to create ami from ec2,
explicit: owner explicitly allows access to specific aws accounts

root volume: the c drive in windows, the drive that boots the OS
block device mapping: determines which volume is root volume and which volume is a data volume

CONNECTING TO AN EC2:
since we have different OS available for instances, we connect to different ports based on the OS
3389: Remote Desktop Protocol, windows instances
22: SSH Protocol, Linux instances



S3 Basics:
global object storage service that is regional based because data is stored inside a specific region and never moves unless you explicitly move it,
default storage service for AWS,
Regional resilient,
public service, unlimited data and multi-user,
economical
accessed via: UI, CLI, API, HTTP

objects: 
data that s3 stores, like a file
composed of key: like a fileName
composed of value: data or value of the object, can range from 0 bytes to 5TB
other components: version ID, metadata, access control, and subresources

buckets: 
containers that store objects
deployed in a specific aws region
blast radius: region
name needs to be globally unique
unlimited number of objects
structure: flat, all objects are stored at the root level.  However, displayed like there is a file structure in UI
configurations: most configurations for s3 are set at bucket level

exam powerup: 
bucket names are globally unique
3-63 characters, all lower case, no underscores
start with a lowercase letter or a number
can't be IP formatted
buckets have 100 soft limit, 1000 hard limit per account
unlimited objects in bucket, 0 bytes to 5 TB
key=name, value = data

Patterns and Antipatterns:
s3 is an object store, not file or block
you can't mount an s3 bucket as K:\ or /images, block storage has a single user limitation and s3 does not have that limitation
great for large scale data storage, distribution, or upload
great for offload, which deals with storing data on s3 bucket rather that ec2 instance
should be default input for any aws services or output to most aws products


Cloudformation Basics:
allows you to create templates to provisions AWS resources using IAC
written either in YAML or JSON

template: tells stack what logical resources to contain and their properties 
contains:
  resources: containes at least one logical resource, they have type and properties
  description: gives some details about the template, decription has to immediately follow AWSTemplateFormatVersion if there is one
  metadata: controls how resources look in the UI, and other things
  parameters: fields that prompt user for more information
  mappings: optional, it allows you to create lookup tables
  conditions: allow decision making in template that only occur if conditions are met
  outputs: once the template is finished, it can produce outputs

!Ref fnReferenceName : references another part of the Cloudformation template
!GetAtt : references another part of the Cloudformation template, and gets attibutes as well 

stack: created by template and contains all of the logical resources the template tells it to contain, then builds a physical resource in your account for each corresponding logical resource.
if you update the stack, it updates the corresponding physical resources
if you  delete the stack, it deletes the physical resources


Cloudwatch Basics:
what is it: collection of services that help you monitor and observe your cloud resources
and it allows you to collect logs, metrics, events, and set alarms
characteristics:
Metrics:AWS Products, apps, on premises
Logs:AWS Products, apps, on premises
Events:AWS services and schedules, 
Namespace:
container for monitoring data
metric:
time ordered set of data points, ex CPUUtilization
Datapoint:
measurement that contains time stamp and value
dimension:
separate datapoints for different things or perspectives with the same metric, ex one ec2 instance cpu utilization
alarms:
takes an action based on a specific metric
ok state and alarm state: once in alarm state it sends trigger to event


High Availability vs Fault Tolerance vs Disaster Recovery:

High Availability:
highly available systems are designed so that when it fails, it is designed to be fixed as quickly as possible.  
maximizes the system's online time, generally measured in percent of uptime of year, like 99.9%
fault tolerance: 
a fault tolerant system operates properly even while faults, which are failures of a system, are present.  Much more expensive than high availability because redundancies are put into place.
disaster recovery:
multiple stage of processes designed to keep the crucial and non replaceable parts of your system safe so when disaster occurs you don't lose anything irreplaceable and can rebuild after disaster


DNS Fundamentals:
discovery service
allows us to translate machine ip addresses into readable language and vice versa
www.amazon.com => 104.98.34.131
it's a huge database and has to be distributed
zone: 
a part of the DNS database that corresponds to a domain, think of it as logical domain, ex amazon.com, contains zone files
zone file:
physical database for a zone that contains the DNS info for a domain, contains the DNS records which has the mapping of website name to IP address
DNS SERVER:
nameserver (NS), server that hosts the zonefiles
DNS resolver Server:
located on your client, or router, or server within your IP, finds the NS you are looking for, then queries the server for the zone file
ROOT OF DNS, or DNS ROOT ZONE:
starting point of the DNS lookup
database hosted on 13 Root servers, 12 large companies manage the servers but not database
delegates authority to top level domains authoritative servers
Root Hints file:
installed on OS'S, pointer to root servers which allows your device to trust root servers

authoritative: trusted
delegated: authoritative entity passes trust to other entity so now other entity is trusted 
top level domains:
part of domain that is immediately left of last . ex:.com .org .uk
registries: organizations that are delegated from IANA to help manage top level domains
registrars: organizations that have relationship with registry that allow you to create a domain registration with them

DNS Record Types:
nameserver: allow delegation to occur in DNS
A, AAAA: map hostnames to IP, a maps to ipv4, aaaa maps to ipv6
CNAME: host to host records, lets you create the equivalent of DNS shortcuts, reduces admin overhead by pointing various services to same A record, so you will only have to update single A record, can only point to names not ips
MX: finds a mail server (SMTP) for a domain
TXT: allow you to add arbitrary text to a domain allowing further functionality, in some cases used to prove domain ownership

TTL: numerical value that can be set on DNS records, once a DNS record from zonefile is recovered by NS, records are cached on resolver server for TTL seconds, caching it so resolver server does not have to perform lookup again for TTL seconds


Route 53 Fundamentals:
allows us to register domains
allows us to host zones files on managed nameservers which it provides
global service with a single database
globally resilient

when registering a domain:
checks with TLD to see if domain is available
creates a zone file for registered domain, known as hosted zone
allocates 4 nameservers for the zone,
adds nameserver records to TLD that indicate that the namerservers are authoritative for domain

hosted zone:
zone files in AWS
hosted on four managed NS
can be public or private linked to VPCs
stores records, known as recordsets



*******************************************
IAM, ACCOUNTS, AND AWS ORGANIZATIONS:
*******************************************

IAM policies:
JSON document that allows or denies permissions for IAM users, groups, and roles
1 or more statements
Sid: statement id, optional field, allows you to provide summaryName of statement to quickly identify what it does, 
Action: can provide a wildcard * which means "all", list of multiple independent actions, or a specific individual action 
["service: operationName"]
Resource: same options above as action, 
["arnResourceNames"]
Effect: either allow or deny, controls what aws does if action and resource match

Every interaction you have with AWS, is a combination of resource you're interacting with and the actions that your attempting to perform on that resource
Policy precedence when two policies overlap permissions(ordered in highest precedence vs least):
1. Explicit deny 
2. explicit allow
3. default deny (implicit), aws identities have no access to any resources (besides root)

Inline Policies:
applying JSON to each account individually
use only when you need a special or exceptional allow or deny for a single user
Managed Policies: 
create policy, then attach to any identity that wishes to use it
reusable, low management overhead
  aws managed policies: managed and created by AWS
  customer managed policies: created and managed by you specific to your needs


IAM Users and ARNs:
entities you create in IAM to represent the person or application needing to access your aws resources
principal: 
represents an entity trying to access aws account
authenticates via username and password or access keys
authenticated identity:
principal that's proved it is who it says it is, aws performs authorization once the authenticated identity tries to access a resource

ARNS:
uniquely identify resources within any AWS accounts
formats differ depending on what your trying to do:
arn:partition:service:region:account-id:resource-id
arn:partition:service:region:account-id:resource-type/resource-id
arn:partition:service:region:account-id:resource-type:resource-id

fields are split by a colon, if you see ::: the field does not have to be specified, like for s3 buckets region or account id do not have to be specified because buckets are globally unique

ex of similar looking arn accessing different things:
arn:aws:s3:::catgifs : accesses bucket
arn:aws:s3:::catgifs/* : objects in the bucket

EXAM POWERUP:
5000 IAM users per account
iam user can be a member of 10 groups
therefore: if you have a system which requires more than 5000 identities then you cant use one IAM user for each identity, IAM roles and Identity Federation fix this 


IAM Roles:
roles allow user or group to temporarily assume permissions via policies,  
two types of policies:
trust policy:
controls which identities can assume that role, can ref identities in same or diff acount, 
if idenity has access to role then temporary security credentials are made available that expire after a given amount of time
permissions policy:
allows or denies permissions for aws resources and services
sts:AssumeRole is policy given to access temporary security credentials and perform whatever permissions it was granted
WHEN TO USE?:
generally used when an unknown number of users is uncertain
use when a set of credentials are needed, bc we dont want to hard code into code or on service because it is security risk as well as problematic if we ever need to change or rotate keys (roles use temporary security credentials)
emergency or out of the usual situations
when you're adding aws into an existing corporate network, external accounts or external identities can't be used to interact with AWS directly, instead assign the external identities a  role for them to interact with aws
users > 5000, use Web identity federation that uses roles
give users from one account access to resources in another aws account via roles
service linked roles:
it is a IAM role linked to a specific aws service
predefined by a service that provide permissions that a service needs to interact with other AWS services on your behalf, service might create or delete the role or allow you to during setup or within IAM
you can't delete the role until it's no longer required!
ListRoles and passRole permissions: 
when you want a group the ability to use roles but not create them


Organizations: allows you to centrally manage multiple AWS accounts under one umbrella,
benefits:
consolidated billing, cost effective with little to no overhead (has volume discounts), account governance
comprised of:
member accounts and management account
oranizational root:
top level of hiearichal structure, container which contains member accounts, management account, and Organizatinal Units, OUs
OUs: organization units, contains member accounts or other OUs
SCPs: service control policies, help you restrict permissions you want everyone in org to follow
best practice:
have single account to handle logins with identities or identity federation, then identities use roles to access the other member accounts

SCPS:
policy document that limits what the account can do, including root users.  They do not grant permissions.
they grant access to what can and cant be allowed, but identities still need identity policies in to grant that permission.  So an identity needs SCPs to allow and also identity policies to allow to access resources.
They can be attached to individual accounts, OUs, and the organizational root.  they inherit down the organization tree.
management account is never affected by SCPs
Allow list:
block everything by default, and allow certain services
Removes FullAWSAccess policy and add any services that can be allowed access for users for account
more overhead but more secure
Deny List:
allow everything by default via FullAWSAccess policy, and deny certain services
default, when you enable SCPS applied to entire organization


Cloudwatch Logs:
public service
store, monitor, and access logging data
many aws integrations
can generate metrics based on logs via metric filter
contains log events, log streams, log groups put defn in from building demo

CloudTrail:
logs api actions which effect aws accounts
not realtime, there is a delay!
cloudtrail event:
logged api call/activity
event history:
events stored for 90 days by default, no cost
Different types of events:
management, data, and insight events
by default only logs management events
management: logs management operations performed on resource 
data: logs the resource operations performed on or within a resource
insight: logs any unusual activity, errors, or user behavior in your account

Trails:
customize the service
regional service, logs events for the region it's created in
two types: 
one region trail: only ever in region it was created in and logs these events only
all regions trail: collection of trail in every region but managed as single logical trail
global service events:
some global services like IAM, STS, Cloudfront log their events only to one region, need to have this enabled to log these events
event storage:
stores events in a definable s3 bucket, can be stored indefinately
can store events in cloudwatch logs as well to use metric filter or search through data
organizational trail:
single management point for every api call made across the organization


AWS Control Tower:
allows the quick and easy setup of multi-account environments
orchestrates other AWS services to provide its functionality including: 
Organizations, IAM Identity Center, CloudFormation, Config, and more...
Landing zone:
multi-account environment, what most people interact with
provides SSO/ID federation, centralized logging and auditing
home region:
region you initially provision into, always available
contains two OUs: the foundational OU named Security and the custom OU named sandbox
foundational: 
contains two accounts, the Audit Account and Log Archive account
custom:
generally used for testing and for account factory
Account factory:
automates creating, updating, and deleting AWS accounts as your business needs them
changes based on templates and implements Cloudformation to provision 
automates: account provisioning, identities with appropriate permissions, guardrails, account and network standard configurations, and can be fully integrated with a businesses SDLC

other features:
guard rails: 
detect and mandate rules/standards across all acounts
rule categories:
mandatory, strongly recommended, or elective
preventative functional type of rule: stops you from doing things via Organization SCPs
detective functional type of rule: performs compliance checks via config rules
dashboard:
single page oversight of the entire environment
you can create other OU's and accounts in a real world environment




*******************************************
S3:
*******************************************

private, by default, account root user is only idenity which can access the bucket

bucket policies:
form of resource policy
resource policy:
like identity policies, but are attached to resources instead of identities.  You are controlling who has access to that resource. 
resource policies have principal field, whereas an identity policy does not have one because it is implied
benefit:
allow/deny access to identies in same or DIFFERENT accounts (diff than identity policies)
allow/deny anonymous principals

ACLS, Access Control Lists:
allow a way to provide security on objects and bucket, basically limited resource policies
used less often, aws recommends you do not use them and instead use bucket policies
subresources
inflexible and simple permissions because you cannot apply conditions like bucket policies and you cannot apply a certain ACL on a group of objects, you would need to apply several ACLs

Block Public Access:
adds a further level of security so that if you do accidentally allow public and anonymous access to bucket via resource policy, then access is still blocked

EXAM PowerUP:
when to use resource policies, identity policies, or ACLs:
identity: controlling access to multiple resources because not every service supports resource policies, you have a preference for a single place to control permissions, working with permissions in same account
resource policies: managing permissions on a single service, allow anonymous or cross account access
ACLs: never, unless you must!


static website hosting:
normal access is via AWS APIs
this feature allows access via HTTP
index and error documents are set
website endpoint is created
you can have custom domains via R53 and bucket name matters
use cases for s3 static website hosting:
offloading and out of band pages
offloading:
store static data from website on s3, have compute service handle dynamic content and then retrieve static data from s3 to save money on storage
out-of-band-pages:
show error or other notification pages on static s3 site in case server is offline or ec2 service as a whole

pricing:
cost to store data on s3 expressed as GB/month fee
data transfer fee from out of s3 expressed in GB
cost for requesting data, diff operations have different costs per 1000 operations

Object Versioning and MFA Delete:
versioning:
lets you store multiple versions of objects within a bucket.  operations which would modify objects generate a new version.
states: disabled, enabled, and suspended
originally disabled, once you enable you can never disable again but can suspend and reenable
implementation:
each object has a key(name) and an id attribute.  
disabled: all ids are set to null.  Whenever changes are made to object, the original object is overwritten.
enabled: ids are given to each object.  Whenever changes are made to object, the original object stays in bucket with same id, while a new object with same key but diff id is generated.  When you access objects from bucket with key the current version is returned.  You can specify which version you want returned with key and id.   
When deleting, if you specify key and id the corresponding object is deleted and the current version marker is reset to corresponding object.  If you do not specify id, then all objects are hidden under delete marker.  To undo delete you can delete the delete marker.  

MFA delete:
enabled in versioning configuration of a bucket
MFA is required to change bucket versioning state or to delete versions
implementation:
serial number of MFA + code are passed with API calls


S3 Performance Optimization:
problem for Single PUT upload:
single data stream to s3 requires full restart if any part of stream fails, then upload fails
speed and reliability is limited in a limit of 1 stream
limited to transferring up to 5GB
solution for Single PUT upload:
Multipart Upload
data is broken up, and each individual part is treated as a single stream, if parts fails, then those individual parts are restarted
minimum data size is 100MB

How global transfer of data to s3 buckets work:
without s3 accelarated transfer, we have to use public internet to route data from source to destination, doesn't usually take an optimal route, 
with s3 transfer acceleration:
transfers data to closest edge location, then transfers data from there directly to closest edge location to s3 destination bucket, 
much faster because the aws network is built for performance between regions
by default, disabled, so you have to enable if you wish to use


KMS, Key management service:
allows you to create, store, and manage keys
both symmetric and asymmetric keys
can perform cryptographic operations, including encrypt and decrypt and others
keys never leave KMS or by default, region, provides FIPS 140-2 L2
regional and public service
support rotation
can create aliases
aws owned & customer owned
customer owned:
aws managed or customer managed keys
aws managed: service default key
customer managed keys:
created explicitly by customer to use in application or service
more configurable, 

KMS keys:
logical key that has ID, data, policy, desc, and state
backed by physical key material (backing key) that is managed by KMS service, we use logical key to have KMS generate, import, or encrypt or decrypt data up to 4 kb in size
implementation:
identity performs call, provides data, KMS creates key and performs operations and always returns data only, not key
role separation:
individual permissions are needed for different operations, like creating key, managing KMS, encrypt, decrypt

DEKs, Data Encryption Keys:
generated by KMS keys via GenerateDataKey call and can be used to encrypt and decrypt data on data greater than 4KB in size
KMS doesn't store the DEK in any way, provides to you or service using KMS to perform crypto operations\
implementation:
when DEK is created, KMS provides you with two versions of that key
plaintext version:
used to encrypt data then discarded
ciphertext version:
encrypted via KMS key
then the encrypted key is stored with encrypted data
when decrypting, you send encrypted key to KMS, it decrypts it and then you decrypt data with the key and then discard key

Key Policies and Security:
key policy:
type of resource policy that has to explicitly grant permission to account owner access to KMS
every key has one
typical way of accessing key:
broad key policy for account + IAM policies
granular key policies
or key policies + grants


s3 Encryption:
buckets aren't encrypted, objects are
each object inside bucket could be using different encryption settings
encryption at rest: client side encryption and server side encryption:
client:
data is encrypted by client, sent as ciphertext to s3 endpoint, then sent to s3 storage
provides you a lot of control, but you do everything when it comes to management
server:
data is sent as plaintext to s3 endpoint, then encrypted by s3 endpoint, and sent to s3 storage 
implementation:
2 components, 
encryption and decryption
generation and management of cryptographic keys
three types available:
differences referenced in slides
SSE-C: ss encryption with customer provided keys
SSE-S3: ss encryption with s3 managed keys, uses AES256 encryption
SSE-KMS: ss encryption with KMS keys stored in KMS, best 

Bucket Default Encryption:
allows you to set default encrytion method used if not specified on object level,
you can set bucket default at object level when uploading object via header x-amz-server-side-encryption or you can set bucket default in the console at bucket level


S3 object storage classes:
s3 standard: 
default
objects are stored across at least 3 AZs
when objects are stored a HTTP/1.1 200 OK respose is provided via s3 api endpoint
no minimums, you are charged per GB for data transferred out and per 1000 requests
recommended for frequently accessed data which is important and non replaceable
s3 standard infrequent access: 
architecture is mostly the same as s3 standard except that it is cheaper to store data but more expensive to retrieve the data
has minimum duration of 30 days for storage and min billing size per object, 128KB
recommended for long lived data, which is important but where access is infrequent
s3 one zone-infrequent access: 
cheaper than s3 or s3-IA
architecture mostly the same as s3-IA, but data is only stored in one AZ
recommended for long lived data, which is non critical and replaceable and infrequently accessed
s3 glacier- instant retrieval:
architecture mostly the same as s3-IA, but cheaper storage, more expensive retrieval costs, and longer minimums
recommended for long lived data, accessed once per quarter with fast, millisecond access
s3 glacier flexible retrieval: 
architecture mostly the same as s3-glacier instant retrievel, but cheaper storage, more expensive retrieval costs, and longer access times
think of objects as chilled state, unable to access immediately
cannot be made publicly accessible, and requires a retrieval process
expedited: 1-5 minutes
standard: 3-5 hours
bulk: 5-12 hours
recommended for archival data, accessed once per year, and minutes-hours retrieval
s3 glacier deep archive: 
cheapest storage option
think of objects as frozen state, unable to access immediately
architecture mostly the same as s3-glacier flexible retrievel, but cheaper storage, much longer access times
standard: 12 hours
bulk: up to 48 hours
recommended for archival data, accessed very rarely if ever, and hours-days retrieval
s3 intelligent tiering: 
helps you automatically move data to most cost effective storage tier based on access patterns, tiers are like classes we have frequent access, infrequent access, archive instant access, archive access, and deep archive
recommended for long lived data with changing or unknown patterns

s3 lifecycle configuration:
set of rules that consist of actions that can automatically transition or delete objects in the bucket
transition actions: change storage classes of objects after x amount of time
can only transition down classes
expiration actions: can delete objects, or versions after x amount of time


s3 replication: 
allows you to configure replication of objects between a source and destination s3 bucket
cross-region replication, CRR:
same-region replication, SRR:
architecture differs depending whether same account or different account
same account:
replication rule is applied to source bucket, dest bucket and IAM role to use
the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
different account:
replication rule is applied to source bucket, dest bucket and IAM role to use
the role has trust policy for s3 to assume it, and permission policy gives it permission to read objects from s3 source, and replicate to destination bucket, transfer is encrypted with SSL
additionally, a bucket policy is needed on destination bucket to allow role from different account access
s3 replication options:
all objects or subset
storage class, default is to maintain same class but you can change
ownership, defualt is source account but you can change
RTC, replication time control, keeps buckets in sync within 15 mins of each other
s3 replication considerations:
not retroactive and versioning for both buckets needs to be on
one way replication only from source to destination
can handle unencrypted objects, SSE-S3, AND SSE-KMS with extra config
source bucket owner needs permissions to objects that will be replicated
will not replicate system events, glacier, or glacier deep archive objects
delete markers are not replicated by default
why use replication?:
SRR- log aggregation
SRR- PROD and TEST sync
SRR- Resilience with strict sovereignty
CRR- global resilience improvements
CRR- latency reduction

s3 presigned urls:
allow you to give an unauthenticated user access to an object inside an s3 bucket using generated credentials from an IAM user in a safe and secure way
credentials are included in URL
see slides for architectures
used when offloading media and access to a private s3 bucket needs to be controlled and you don't want to run application servers to broker that access
EXAM POWERUPS:
you can create a URL for an object you have no access to, (which will also have no access lol)
when using the URL, the permissions match the identity's which generated it CURRENTLY, so if they change so do the URL's permissions
do not generate URL's with a role!  URL stops working when temporary credentials expire


S3 select and Glacier select:
SQL-like statements that allow you to retrieve parts of object rather than whole object
formats of data: CSV, JSON, Parquet, BZIP2 compression for CSV and JSON
why use?:
you might want to retrieve only parts of object to save on computing costs and for faster performance


S3 Events:
allows you to create event notifications when events occur in a bucket
can be delivered to SNS, SQS, and Lambda Functions, all need resource policy allowing s3 principal access
Object created events
Object Delete events
Object restore events
Replication events
EventBridge:
alternative and supports more types of events and more services
would use this instead of s3 events unless you had specific reason not to do so

S3 Access Logs:
Enable logging on source bucket and sends logs to target bucket
s3 log delivery group reads permissions you set, the buck acl on target group allows access to s3 log delivery group
why use?:
provides detailed information about requests which are made to a source bucket that are useful for
security functions
access audits for employees
customer access patterns
understand charges

s3 object Lock:
group of related features that enables Write-Once-Read-Many (WORM) architecture, which means no deletes, no overwrites
Requires versioning, individual versions are locked
Bucket can have default object lock settings
Retention Period and Legal Hold: object lock can have both, one, or none
Retention Period:
specifies days and years which object is locked
compliance mode:can't be adjusted, deleted, overwritten for duration of retention period
governance mode:special permissions can be granted allowing lock settings to be adjusted
Legal Hold:
you set legal hold to be on or off, there is not retention period
no deletes or changes until removed


VPC sizing and structure:
overview:
what size should the VPC be?
are there any networks we can't use, do not overlap networks of VPC's, cloud, on-premises, partners, and vendors
try to predict the future situation
vpc structure: tiers and resiliency availability zones
tiers: 
seperate application components and allow different security to be applied 

VPC minimum /28 (16 IP), maximum /16 (65536)
Personal preference for the 10.x.y.z range
avoid common ranges, like 10.1-10.15
reserve 2+ networks per region being used per account
in our scenario, we have 3 US, 1 Europe, 5 australia, x 2 and assuming max 4 accounts = 40 IP
VPC sizing in depth:
how many subnets will you need?
how many IPs total?  How many per subnet?
formula?:
a subnet is located in one AZ, so determine how many AZs you will be using, he mentions by default he uses 4, 3 for AZs and one spare
each tier has its own subnet in each AZ, by default 4 tiers, web, app, db, and spare so now we have 16 subnets by default


Custom VPCs:
regional service, operates from all of the AZs in that region
allows you to create isolated networks
nothing IN or OUT without explicit configuration
flexible configuration, simple or multi-tier
hybrid networking to other cloud and on premises
default or dedicated tenancy, allows resources created inside VPC to be provisioned on shared or dedicated hardware

can use IPv4 public and IPv4 private addresses, by default uses private
public addresses are used when you want resource to communicate with public internet, or aws public zone, or allow access to them from public internet
allocated one primary private IPv4 CIDR block, configured when you create VPC, optional secondary ipv4 blocks, think of it as a VPC has a pool of private addresses and optionally it can use public addresses
optional signle assigned ipv6 /56 CIDR block

fully featured DNS:
provided by r53
vpc address is base IP + 2
enableDnsHostnames: gives instances DNS names, if set to true then instances with public ip addresses get public DNS hostnames
enableDnsSupport: enables DNS resolution in VPC, indicates whether enabled or disabled


VPC subnets:
a subnetwork of a VPC, within a particular AZ
1 subnet can have only one AZ, one AZ can have zero to many subnets
allow you to add structure, functionality, and resilience to VPCs
AZ resilient 
ipv4 CIDR is a subset of the VPC CIDR, and cannot overlap with any other subnets within VPC
optional ipv6 CIDR
subnets can communicate with other subnets in the VPC
5 reserved ip addresses:
network address: first starting address in available range
network + 1: address used for VPC router
network + 2: address used for Reserved DNS
network + 3: address used for future requirements
broadcast address: last IP in subnet

DHCP Options set: dynamic host configuration protocol
configuration object applied to VPC that allows computing devices to receive IP addresses automatically
one applied to a VPC at one time
if you want to change settings you need to create a new one
two options:
auto assign public ipv4:
automatically assigns public ipv4 address to resource that has private ipv4 address
auto assign ipv6:
automatically assigns ipv6 address to resource


VPC ROUTING AND INTERNET GATEWAY:
internet gateway: 
region resilient gateway attached to a vpc
1 VPC = 0 or 1 IGW, if 0 then it is entirely private
1 IGW = 0 or 1 VPC
Runs from within AWS Public zone, or "border between public zone and VPC"
Gateways traffic between the VPC and internet or between VPC and public aws zone
Managed by aws for performance
maintains mappings of ipv4 public addresses for private instances within VPC:
instances that have ipv4 addressing enabled, have only private ipv4 address
the igw maintains a public ipv4 address mapping to private ip address
when private instance sends packet, the igw replaces the private ip address with public ip address
ipv6 addresses for private instances are natively routable:
instances that have ipv6 addressing enabled, have both private ipv6 address and public ipv6 address
igw passes traffic, it doesn't need to do mapping


VPC ROUTER:
routes traffic between subnets
every vpc has a vpc router, highly available
in every subnet, the router uses network+1 address
controlled by route tables, each subnet has one only
a VPC has a main route table, it is default table for subnet if there hasn't been an explicit association with custom table, when subnet does get associated with custom table the main route table is dissasociated
route table:
basically just a list of routes, matches destination ip address on packet with destination field in route table
if multiple routes match, the most specific, higher prefix route is selected
target field:
point to an aws internet gateway or local, which means destination is within vpc itself


Bastion Host/Jumpbox:
is instance in a public subnet inside a VPC, generally used as an entry point for private only VPCs
historically was only way to implement entry point for private only VPCs but not there are better alternatives
allow incoming management connections and then access internal VPC resources


STATEFUL VS STATELESS FIREWALLS:
two things to think about when dealing with firewall rules:
inbound and outbound:
request and response
inbound and outbound rules can be both requests and responses, just depends on client/server relationship

stateless firewalls:
need rule for each request/response, 2 total
request is always to well known port, and response is always to ephemeral port and since it does not have state, you have to allow traffic to all ephemeral ports
stateful firewall:
recognize connection and treats each request/response as 1 rule, 1 total rule allows/denies request and response is automatically allowed/denied
lower admin overhead

NACL, NETWORK ACCESS CONTROL LISTS:
traditional stateless firewall available within aws vpc
each subnet has one only, NCLS filter traffic crossing the subnet boundary inbound or outbound
connections between things inside subnet are not affected by ACLs
contain rules grouped into INBOUND and OUTBOUND
match IP, port, and allow explicit denies or allow for that match
matches first rule found in order then stops processing, so first rule matched takes effect
default NACL is implemented in a VPC with all traffic allowed
you cannot reference any logical resources, only IPS/CIDR, ports, and protocols
good practice is to implement together with security groups to allows traffic with security groups and deny traffic with NACLS

VPC SECURITY GROUPS:
traditional stateful firewall 
if request is allowed, then response is automatically allowed 
no explicit deny, only allow or implicit deny by not allowing
can't block specific bad actors
support IP/CIDR rules and logical resource references, which includes other security groups and itself, both help scale, you could use self reference to allow intra app communcation 
attached to ENI's, network interfaces, not instances

NETWORK ADDRESS TRANSLATION, NAT, AND NAT GATEWAYS:
NAT, Network Address Translation: set of processes that remap source or destination IPs
static nat: what igw does
ip masquerading: hiding CIDR blocks behind one IP, helps overcome the ipv4 shortages by translating private IPv4 addresses to public ip addresses so packets can go to public internet then translate back in reverse, takeaway is that private ip address needs to initiate communication because if public tries to communicate with private there wont be an entry in NAT table
how to implement IP masquerading in AWS, will refer to as ip masquerading as nat:
nat gateway:
provision nat gateway into public subnet with route table, diff route table for private subnet, so that packets sent from private go to nat gateway, then get sent to internet gateway
uses eleastic IP (static ipv4 public address)
az resilient service, deploy a nat gateway in each az for region resilience!
2 charges, one per hour and another per gb of data consumed
nat instance:
implementing ec2 instance as NAT gateway, not really all that important to know because nat gateways are mostly used, however for exam know that if you do implement nat instance, you need to disable source/destination checks
differences between two:
nat instance is managed by you, so you can do other things than just performing nat service, with nat gateway you can only do nat service
nat gateway is highly available within az, nat instance isn't
nat gateways only support NACLS, while nat instances support security groups and NACLS
ipv6:
NAT isn't required for ipv6 and nat gateways don't work with ipv6
all ipv6 addresses in aws are publicly reoutable, the igw works with all ipv6 ips directly
you can enable default ipv6 address route + igw for bi directional connectivity or with egress only igw for outbound only



VIRTUALIZATION 101:
virtualization: process of running more than one operating system on a piece of physical hardware or server
user mode and privileged mode
application needs to make calls to OS(kernel)(privileged state) that makes calls to hardware, apps cannot make system calls directly to hardware

allows multiple privileged programs(os) to make calls to hardware:
emulated virtualization:
host os, which runs in privileged mode, has hypervisor, means it has knowledge of the different guest os and can allow privileges 
virtual machine:
container that contains applications and guest operating system, logical hardware is allocated by hypervisor which has access to physical hardware
binary translation:
guest os not modified, whenever the virtual machine makes call to physical hardware, the hypervisor steps in and makes the call to physical hardware
Para-virtualization:
similar architecture to emulated virtualization, but guest os is modified, instead of making calls to physical hardware, it makes calls to hypervisor 

hardware assisted virtualization:
hardware itself is virtualization aware,
guest os make calls to physical hardware and hardware does not halt execution, but redirects these calls back to hypervisor
SR-IOV:
hardware is split up into several physical hardware pieces available for guest os so no translation has to happen by hypervisor
Enhanced Networking:
this process occurs in EC2 with network cards and improves latency, less cpu usage


EC2 architecture and resilience:
ec2 instances are virtual machines running on the ec2 hosts, which is physical hardware 
ec2 hosts are either shared hosts or dedicated hosts
az resilient, host runs on a single AZ
instance store: local storage on ec2 host
storage networking:
remote storage can attach to elastic block store service
EBS, elastic block store: storage device, a volume, that can be attached to or removed from your instance,
data persists when instance is not running, 
can only be attached to one instance in same AZ,
recommended for quickly accessible data, running a database on an instance, long term data storage

data networking:
elastic network interface, ENI, is provisioned in subnet and maps to physical hardware on ec2 host when an instance is provisioned inside subnet

ec2 instances remain on same host unless host fails or host is stopped and started

What's ec2 good for?:
when you've got a traditional os+application compute
long-running compute needs
server style applications, that are waiting for incoming connections
either bursts or steady state load requirements
monolithic application stacks
migrated application workloads or disaster recovery
tends to be default compute service within aws

ec2 instance types:
characteristics:
raw resources you get, cpu memory, local storage capacity, and type
resource ratios
storage and data network bandwidth 
system architecture/vendor
additional features and capabilities, like gpus, fpgas

five main categories:
general purpose:
default, diverse workloads, equal resource ratio
compute optimized: 
have higher cpu than memory ratio
media processing, HPC, scientific modeling, gaming, machine learning,
memory optimized:
higher higher memory than cpu ratio
processing large in memory datasets, some database workloads
accelerated computing:
have additional features and capabilities
hardware gpu, FGPAs
storage optimized:
large amounts of super fast local storage
sequential and random IO, scale out transacitonal databses, data warehousing, elasticsearch, analytic workloads

Decoding EC2 types:
check out slide
[instance family] [instance generation] [additional capabilities]. [instance size]

ec2 instance connect:
need identity policy permissions 
your browser connects to aws, then it redirects to connect to ec2 instance
using the GUI can only connect to public ec2 instances, can connect to private using CLI
ec2 ssh client:
need access key


Storage Refresher:
ephemeral storage: temporary storage, instance store volumes
  direct(local) attached storage: storage on the ec2 host, called instance store volumes
persistent storage: permanent storages, lives on past the lifetime of the instance, ebs
  network attached storage: volumes delivered over the network (EBS)

3 main categories of storage available in AWS:
block storage: 
Volume presented to the OS as a collection of blocks.  no structure provided. mountable. bootable.  
implemented via SSDs, (physical storage) or by logical volumes backed by SSDs
ideal for booting storage, high performance storage inside os
file storage:
presented as a file share. has structure. mountable. not bootable.
ideal for sharing files among multiple servers, clients, services, etc
object storage:
collection of objects.  no structure (flat). Not mountable.  Not bootable.  
ideal for large access to read and write to data at scale

storage performance:
IO, or block size:
size of blocks of data that your writing to disk
think size of wheels of car
IOPS:
number of io operations the storage system can support in a second
think speed of engine of car, power
throughput: 
rate of data storage system can store on a particular piece of storage, mb/sec
think average speed of car
io x iops = throughput:

EBS, ELASTIC BLOCK STORE:
provides block storage that can be encrypted using KMS
instances see block device and create file system on this device
provisioned in one az, resilient in that AZ
attached to one ec2 instance over a storage network
persistent, they can be detached and reattached to same or different instances
you can perform backups, or snapshots, of data into s3.  You can migrate the data and create volumes from that snapshot.  
can be provisioned with different physical storage types, different sizes, and different performance profiles
billed based on gb-month, and in some cases performance

EBS VOLUME TYPES GENERAL PURPOSE SSD:
up to 16000 IOPS per volume
GP2:
default general purpose ssd storage
high performance storage for fairly low price
can be as small as 1GB or as large as 16TB
io credits and baseline performance found on slide
used for boot volumes, low latency apps, and dev and test environments
GP3:
cheaper than GP2
io credits and baseline performance found on slide
benefits of GP2 and IO1 
used for boot volumes, low latency apps, and dev and test environments, virtual desktops, medium sized single instance databases

EVS VOLUME TYPES PROVISIONED IOPS SSD:
designed for super high performance situations, IOPS can be adjusted independently of size
low latency and consistent low latency are characteristics
faster than gp3
there is a max per instance performance, found on slide
up to 64,000 IOPS per volume
io1:
io2:
up to 256,000 IOPS per volume
io2 block express:
used for high performance, latency sensitive workloads, IO intensive NoSQL and relational databases.  also used when you have small volumes and you need high performance.

EBS VOLUME TYPES HARD DISK DRIVE (HDD) -BASED:
have moving bits, slower but good for specific situations
used for larger, cheaper storage that is frequently accessed for throughput intensive sequential workloads.  think big data, data warehouses, and log processing.
cannot boot
st1:
fast hard drive, not agile
sequentially accessed data
sc1:
cold HDD
designed for infrequently accessed workloads where cheaper storage is priority

INSTANCE STORE VOLUMES:
provide block storage devices
physically connected to one EC2 host
instances on that host can access them
highest storage performance in aws by much higher levels
included in instance price
attached at launch of ec2 instance!
number of and size of instance store volumes depend on type of instance
EXAM POWER-UPS:
local on ec2 host
add at launch only
data lost on instance move, resize, or hardware failure, but not on restart or if instance stays on same host, data lost when ec2 stops and starts because aws allocates new host
price included in instance
temporary data

CHOOSING BETWEEN INSTANCE STORE AND EBS:
EBS: persistant data
EBS: higher resilience
EBS: storage isolated from instance lifecycle
depends: when you need resilience and app supports in built replication of data, instance stores are an option because you can have many ec2 instances with instance store to provide the resilience and the data is persisted within app
depends: high performance needs
  RAID0 + EBS: max number of IOPS per instance is 260,000 IOPS, used by combination of high number of volumes and efficient architecture
instance store: super high performance needs, more than 260,000 IOPS in millions
instance store: when cost is a primary concern

EBS SNAPSHOTS:
incremental volume copies to s3, improves az resilience to region resilient
the first is a full copy of data on the volume, future snaps are incremental
volumes can be created or restored from snapshots, and can be copied to another region
billing: GB/month of used data, not allocated data

when you create new EBS volume w/out snapshot: full performance immediately
when you create new EBS volume w/ snapshot: restores lazily, the blocks are fetched gradually
to force blocks to be fetched immediately you can implement a forced read of all data immediately with CLI tools or OS tools or you can implement Fast snapshot Restore(FSR)
Fast snapshot restore, FSR: 
immediately restores data, up to 50 snaps per region
extra cost

EBS ENCRYPTION:
by default, no encryption is applied
provides at rest encryption for snapshots and ebs volumes
implementation:
KMS generated encrypted DEK key to be stored on instance
Whenever wanting to read/write data, the volume makes request to KMS, KMS sends response with decrypted DEK key which is stored on the ec2 host and is used to encrypt and decrypt data stored on instance volume.  Whenever instance moves hosts, the key is discared
Instance retains encrypted DEK key, and whenever wanting to read/write data repeats step 2.  same implementation for snapshots. 
EXAM POWER-UP:
accounts can be set to encrypt ebs volumes by default, you can set a default KMS key or choose a custom KMS key to use
each volume uses 1 unique DEK, 
snapshots and future volumes use the same DEK
can't change a volume to not be encrypted once it's encrypted
OS isn't aware of the encryption, no performance loss

NETWORK INTERFACES, INSTANCE IPS, AND DNS:
every ec2 instance has one network interface called the primary ENI (elastic network interface)
optionally, you can attach one or more secondary ENIs (everything needs to be in 1 AZ), only difference is that you can detach secondary ENIs and attach to diff instances
on the eni:
mac address
primary ipv4 private ip
0 or more, secondary IPs
0 or 1 public ipv4 address: changes whenever instance is restarted because allocated new host
1 elastic ip per private ipv4 address: (elastic ips are like public ipv4 addresses but different bc normally you can only have 1 public address per interface while you can have many elastic ips 1 per private ipv4 address)
o or more ipv6 addresses
on the network interface:
security groups
source/destination check

elastic ip addresses: elastic ips are like public ipv4 addresses but different bc normally you can only have 1 public address per interface while you can have many elastic ips bc 1 elastic ip is associated per private ipv4 address on primary interface or second interface
allocated to your aws account
replaces the public ipv4 address
EXAM POWER-UPS:
secondary eni + MAC = licensing, and you can then move licenses between ec2 instances
you can apply different security groups on multiple interfaces to enforce different access permissions for different IP ranges
the OS doesn't see public ipv4, the private ip is mapped to public via NAT
0 or 1 public ipv4 address changes whenever instance is restarted, or started and stopped because allocated new host, if you don't want dynamic address then use elastic ip instead

AMI, Amazon Machine Image:
images that help us launch ec2 instance
it is template of an instance configuration and then use that template to provision instances using that configuration
you can create an AMI from an ec2 instance you want to template
aws, community provided (marketplace), or you can create your custom AMI
regional so unique id provided per region
has permissions
AMI Lifecycle:
launch: where you use an ami to launch ec2 instance
configure: add some customization
create image: create an ami with ec2 default configs + your customization, contains permissions and block device mapping, which references the snapshots created of ebs volumes 
launch: new instance is launched with resources and configs, snapshots are used to create corresponding ebs volumes in region
EXAM POWER-UPS:
AMI = one region, only works in that one region
AMI Baking = creating an AMI from a configured instance + application
An AMI can't be edited, you have to create a new one
can be copied between regions
permissions by default allow your account only to access

EC2 PURCHASE OPTIONS (LAUNCH TYPES):
ON-DEMAND:
instances of different sizes run on the same ec2 hosts, they are isolated but run on shared hardware
per second billing while an instance is running.  also some resources like storage are always billed, regardless of instance state
characteristics:
predictable pricing, no upfront cost, no discounts
default,
good for short term, unknown workloads, and apps which can't be interrupted

SPOT:
selling unused ec2 host capacity for up to 90% discount, the spot price is based on the spare capacity at a given time
implementation:
customers set max price they are willing to pay.  if capacity is available and price aws is charging is lower than max price, they are allocated capacity.  However, once the price goes above the customers max price, the customers capacity is terminated.  
characteristics:
never use spot for workloads which can't tolerate interruptions, good for anything which is stateless, anything which can be rerun
if you need burst capacity needs,
non time critical

RESERVED INSTANCES:
Standard Reserved:
committment for long term consumption of ec2 resources
reduced or no per second price for corresponding on demand instance
unused reservations are still billed
can be regional or zonal reservation or neither
commitments:
1 year or 3 year terms
no upfront, partial upfront, and all upfront.  first two reduce /sec fee, while last eliminates
characteristics:
known usage, need consistent access to compute, long-term 

Scheduled Reserved:
ideal for long term usage which doesn't run constantly
you specify frequency, duration, and time
minimum of 1200 hours per year for 1 year


DEDICATED:
Dedicated Hosts:
hosts that are allocated to you entirely
you pay for the host, no instance charges
characteristics:
host affinity, links instances to hosts
socket and core licensing requirements
when you have strict regulatory or data requirements

Dedicated Instances:
instances that are allocated to you entirely, 
host is not shared between you and other customers
you do not pay for host, pay for flat hourly charge and for dedicated instances themselves 
characteristics:
when you have strict regulatory or data requirements


Capacity Reservations:
from highest to lowest priority:
reserved purchases
on demand 
spot
types:
regional reservation: allows you to launch instance in any AZ in region but do not reserve capacity within an AZ, cheaper than on demand
zonal reservation: only apply to one AZ and reserves capacity within AZ, cheaper than on demand
on-demand capacity reservation: can be booked to ensure you always have access to capacity in an AZ but not discounted price and you pay even if you dont use it, no commitment requirements

EC2 SAVINGS PLANS:
hourly commitment for a 1 or 3 year term
two types:
general compute $ amounts: ex, 20$ per hour for 3 years, for ec2, fargate, and lambda, saves you money because savings plan rate is lower than on demand rate for these products
specific ec2 savings plan: you pick size and OS

INSTANCE STATUS CHECKS AND AUTORECOVERY:
2 status checks per instance:
system status: things like loss of system power, ec2 host or service failures
instance status: things like corrupted file system, incorrect instance networking, os kernel
autorecovery:
moves instance to a new host, keeps same configs and reperforms status checks
allows you to automatically recover from failed status checks,
termination protection:
in instance settings, you can enable termination protection which allows you to protect your instance from being accidentally terminated, or terminated by someone without the permission

HORIZONTAL VS VERTICAL SCALING:
Vertical scaling:
allows you to resize an ec2 instance,
benefits:
no application modification required
works for all applications, even monoliths
disadvantages:
each resize requires a reboot which leads to slower rection times and disruptions
larger instances often carry a money premium that is more than linear the bigger you get
there is an upper cap on performance due to set limit on instance size

horizontal scaling:
allows you to change number of instances,
how it works:
a copy of application is stored on each instance and instances work in parallel via load balancer
load balancer:
sits between customers and servers and distributes the load
sessions are everything: requires application support or off-host sessions
off host sessions:
session is stored externally somewhere else and servers are stateless
benefits:
no disruption when scaling
no real limits to scaling
often less expensive than vertical scaling
allows you to be more granular when you scale with addition or subtraction of smaller instances
EXAM POWER-UP:
horizontal vs vertical scaling slide


INSTANCE METADATA:
ec2 services provides data to instances
accessible inside all instances, anything running inside instance can query the metadata
no authentication nor encrypted: can and does get exposed
http://169.254.169.254/latest/meta-data/: IP address to access instance metadata
ec2-metadata tool allows us to easily query data via cli
categories:
environment
networking
authentication
user-data




*******************************************
CONTAINERS:
*******************************************
problem with virtualization: guest os consumes a lot of the available resources that's allocated to each virtual machine
containers:
provides an isolated environment which an application can run within
runs as an application running within host os to solve problem with virtualization
allows us to run many more applications on top of same hardware than when using virtualization, saves so much space and scales really well
portable, isolated, very consistent as they always run as expected
ports are exposed to the host and beyond
application stacks can be multi container
so container engines are like guest os and the containers are memory for application and runtime environemnt 
architecture:
dockerfile: file used to build docker images. each step creates file system (fs) layers
stacks of docker images
images: are created from a base image or scratch
images contain readonly layers, changes are layered using a differential architecture
docker container: running copy of a docker image (fs layers), with an addition read/write layer
read/write layer: allows container to run, any applications store changes in this layer and containers differ only in this area, have same base layers
container registries: hub or registry of container images, 


ECS CONCEPTS, ELASTIC CONTAINER SERVICE:
allows you to use containers running on infrastructure that aws partially or fully manages to reduce admin overhead of using docker container
clusters run in one of two modes: ec2 mode and fargate mode
ec2 mode: uses ec2 instances to run containers
fargate mode: serverless way of running docker containers

ecs container engine maintains clusters
ecs allows you to create cluster
clusters: where your containers run from, tasks or services get deployed into cluster
container definition: tells ecs information about single container, including pointer reference and which port it uses 
task definitions: represents a self contained application as a whole, contains one or more container definitions
store resources used by application like cpu, memory, compatibility, task roles
task role:
iam role the tasks can assume to interact with aws resources, 
task roles are BEST PRACTICE to give ecs containers permissions to access aws services and resouces!
service definition: defines a service, service is how for ecs we can define how we want task [definitions] to scale, or availability, resiliency, copies, capacity, etc

EC2 MODE:
ecs management components: uses ec2 instances to run containers
see slides
ecs cluster is created and runs within VPC, benefits from many AZs within region
auto scaling group helps provision isntances
you generally manage ec2 instances (container host), availabilitiy, capacity,
ecs will handle tasks and service definitions

FARGATE MODE: serverless way of running docker containers
aws provides more management of overhead, manages Fargate shared infrastructure
each cluster runs on Fargate, and ecs tasks and services are injected into VPC and given ENI to access from VPC or public if vpc config allows
ecs will handle tasks and service definitions
you pay for resources you consume

CHOOSING BETWEEN EC2 VS ECS... EC2 MODE VS FARGATE MODE
ecs: if you use containers already
ec2 mode: large workload, price conscious over effort
Fargate: large workload, overhead conscious
Fargate: small/burst workloads
Fargate: batch/periodic workloads

ECR, ELASTIC CONTAINER REGISTRY:
managed container image registry service
like docker hub but for aws
each aws account has a public and private registry
each registry can have many respositories and each repository can contain many images, think github
container images can have several unique tags within your repository
public repository: anyone has read access but write requires permissions
private registry: permissions are required for any access
benefits:
integrated with IAM
image scanning via basic and enhanced scan via aws inspector
provides real time metrics based on authentication, pushing and pulling
logs all api actions via cloudtrail
delivers events to eventbridge
you can replicate cross region and cross account

KUBERNETES, 101:
it allows you to run containers in a reliable and scalable way and uses resources efficiently and lets you expose containerized apps to outside world
like docker but automated 
open source container orchestration system that allows you to automate the scaling, deployment, and management of containerized applications
kubernetes cluster:
structure on slide
key components:
cluster: a deployment of kubernetes, management, orchestration, and service access
node: resources, pods are placed on nodes to run
pod: 1+ containers, smallest unit in jubernetes, often 1 container, 1 pod, nonpermanent
service: abstraction, service running on 1 or more pods, its what you interact with like application
job: ad hoc, creates one or more pods until completion
ingress: how external entity accesses a service, ex would be ingress->routing->service->1+ pods
ingress controller: used to provide ingress, software that allows hardware to ingress
important to uses stateless architecture for data stored on pods since they are temporary, 
persistant storage, pv: volumes whose lifecylces lives beyond any 1 pod using it

ELASTIC KUBERNETES SERVICE, EKS:
aws managed kubernetes, open source and cloud agnostic
it can run on aws, outposts, eks anywhere, or eks distro
control planse scales and runs on multiple AZs
integrates with AWS services like ECR, ELB, IAM, VPC
eks cluster: eks control plane and eks nodes
etcd: distributd across multiple AZs
Nodes: can be self managed, managed node groups, or fargate pods
for storage: can use EBS, EFS, FSx Lustre, FSx for NetApp ONTAP
architecture: on slide



*******************************************
ADVANCED EC2:
*******************************************
bootstrapping:
process of running scripts or other configurations when an instance is first launched
allows ec2 build automation once launched
enabled via user data, accessed via the meta-data IP + /latest/user-data
anything in user data is executed by the instance OS
ec2 doesn't interpret, it passes the data blindly, the os needs to understand the user data and execute, 
can be done in UI when launching ec2 instance or you can bootstrap via including user data inside cloudformation template that launches ec2 instance
architecture:
ec2 bootstrapping slide
key points:
user data is not secure, don't use it for passwords or long term credentials
user data can be modified when instance stopped
executed only once at launch time

Boot-time-to-service-time: how long it takes to provision ec2 instance and the instance is available to use, generally takes minutes
ami baking: 
removes post launch time by putting configurations into AMI
post launch time:
configuring the instance after launch before it is available for service
optimal way:
ami bake in the installation part, and bootstrap the final configurations

ENHANCED BOOTSTRAPPING WITH CFN-INIT:
allows you to pass complex bootstrapping instructions into ec2 instance, much more complex than user data instructions
much more powerful, tells ec2 desired state rather than procedural like user data does and also can run more than once, ex watching for updates to metadata then performing update
cfn-init: helper script installed on ec2 os, 
simple configuration management system that tells ec2 desired state, then ec2 performs functionality required to do so
aws::CloudFormation::Init: where cfn-init configuration is stored in cfn template
cfn CreationPolicy:
added to a logical resource inside a cfn template, does not allow resource to move into create complete status until cfn receives signal
cfn Signals:
allows ec2 to send confirmation signal to cfn stack if configuration was created successfully

EC2 INSTANCE ROLES:
roles that an instance can assume and anything running within instance, like CLI 
should always be used rather than adding access keys into instance
InstanceProfile: 
wrapper around an IAM role that allows permissions to get inside of instance
when you use UI it abstracts the process of wrapping iam instanceProfile but when you use CLI or CFN you need to implement instanceProfile manually

temp credentials are inside instance meta-data:
http://16.254.169.254/latest/meta-data/iam/security-credentials/role-name
automatically rotated and always valid

SSM PARAMETER STORE:
storage for configuration and secrets
allows 3 different types of parameters: String, StringList, SecureString
ex: license codes, database strings, full configs and passwords
hierarchies and versioning
plaintext and ciphertext, integrates with KMS to encrypt sensitive info and other services need access via IAM permissions 
public parameters, like latest AMIs per region
changes can create events

SYSTEM AND APPLICATION LOGGING ON EC2:
sysptem and application logs deal with caputuring metrics from within os, cloudwatch by default monitors external metrics
Cloudwatch Agent (software) is required to captsure system and application logs plus configuration and permissions
agent configuration: configures which logs and metrics we want to capture
permissions: roles grant permissions for instance, (and everything running inside of it), the permission to access kms and write to cloudwatch logs

EC2 PLACEMENT GROUPS:
placement groups allow you to influence placement of ec2 instance within AZ
cluster: pack instances close together,
used when you want to achieve the highest possible performance, tradeoff is that low resiliency is achieved bc if hardware fails then it could take down several instances
best practice to use same type of instance and launch all at the same time in a single AZ
all members have direct connections to each other and sometimes will share same host
can span vpc peers, but impacts performance
requires a supported instance type
use cases: high performance, fast speeds, low latency, low resiliency

spread: keep instances separated
ensure maximum availability and resiliency
can span AZs
esch instance has its own isolated networking and power supply from other instances
limit of 7 instances per AZ, (1 in each partition), placement is done by aws
not supported for dedicated instances or hosts
use cases: small number of critical instances that need to be kept separated from each other

partion: like a spread placement group but a little different because partition can span AZs divided into partitions, where you have max 7 partitions per AZ, HOWEVER some of the instances can be in same partition wheras in spread they are all separated
placement is done by you or aws
use cases: huge scale parallel processing systems where you need to create groupings of instances and need system to be seperated


EC2 DEDICATED HOSTS:
hosts that are allocated to you entirely
you pay for the host, no instance charges
host hardware has physical sockets and cores which dictate how many instances can be run and organizations can utilize for socket and core licensing requirements
use cases: when you have strict regulatory or data requirements
characteristics:
AMI limits, some are not supported
amazon RDS instances are not supported
placement groups are not supported
hosts can be shared with other organization accounts


ENHANCES NETWORKING AND EBS OPTIMIZED:
two optimization techniques
enhanced networking: uses SR-IOV, single root io virtualization, to improve latency of networking
NIC is virtualization aware
offers logical card to each instance instead of a single physical card on the host that the host manages which ec2 instance has access to
characteristics:
offers higher IO and loswer host cpu usage
no charge, available on most EC2 types
more bandwidth, high throughput
higher packets per second PPS
consistent lower latency

ebs optimized: means dedicated capacity for EBS
historically network was shared between data networking(ENIs) and EBS
most instances support and have enabled by default
on older instances some support, but enabling costs extra




*******************************************
ROUTE 53:
*******************************************
Route 53 Fundamentals:
allows us to register domains
allows us to host zones files on managed nameservers which it provides
global service with a single database
globally resilient

ROUTE 53 PUBLIC HOSTED ZONES:
DNS DB for a domain, (zone files in AWS)
hosted on four managed NS specific for the zone
accessible from the public internet and VPCs
stores RRs, resource records which are actual items of data dns uses
hosted zones are authoritative for a domain
you can point to externally registered domains
architecture differs slightly between public internet access and vpc access, see slide

R53 PRIVATE HOSTED ZONES:
accessible within VPC it was created in and accessible within associated VPCs
architecture is similar to public hosted zones via VPC, diff is not publicly accessible
you can grant access to different users within your account via UI, CLI/API, and grant access to users of different accounts via CLI/API
split-view is possible where you have overlapping public and private hosted zones for public and internal use with the same zone name

R53 CNAME RECORDS VS ALIAS RECORDS:
cname records:
maps a name to another name
invalid for naked/apex of a domain (domain with no subdomain, like no www., ex catagram.io) and problem is that many aws services use a DNS name so with just cname catagram.io would be invalid
alias records:
map a name to an aws resource
can be used for both apex and normal records
there is no charge for alias requests pointing at aws resources
for aws services, default to picking alias
sub types, so you need to match alias type to corresponding record it is pointing at
use alias records for services such as gateway, cloudfron, elb, elastic beanstalk, global accelerator, and s3

R53 SIMPLE ROUTING:
simple routing supports 1 record per name(like www)
each record can have multiple values (ips?), all are returned in a random order
client chooses and uses 1 value
simple routing does not support health checks-all values are returned for a record when queried
there are no checks by record to ensure resource being pointed at is actually operational
EXAM POWER-UP: use when you want to route requests towards one service such as a web server

R53 HEALTH CHECKS:
seperate from, but are used by records
health checkers located globally that can check aws targets and anything accessible over public internet
health checkers check every 30s, every 10s costs extra
checks can be TCP, HTTP/HTTPS, HTTP/HTTPS with string matching, these checks build upon each other
results in record being in healthy or unhealthy state, healthy if 18%+ of health checkers report as healthy
three types:
endpoint: actual endpoint that you specify
cloudwatch alarm: check cloudwatch alarms 
check of checks: check application wide health with lots of individual components

FAILOVER ROUTING:
we can add multiple records of the same name, a primary and a secondary
they both point at unique resource
use case:
when you want to configure active passive failover, meaning when you want to use the primary record if it passes its health check and the secondary record if primary record is unhealthy

MULTI VALUE ROUTING:
like a mixture between simple and failover routing policies
you can create many records with the same name
up to 8 healthy records are returned, if more exist, 8 are reandomly selected, client chooses and uses 1 value, any records which fail health checks won't be returned when queried
EXAM POWER UP: multi value improves availability.  It is not a replacement for load balancing
used when you have many resources that can service request and you want them all health checked and returned at random

WEIGHTED ROUTING:
EXAM POWER UP: use when you want simple load balancing or testing new software versions
implements record weights, where they are returned based on its record weight vs total weight
if a chosen record is unhealthy, the process of selection is repeated until a healthy record is chosen

LATENCY-BASED ROUTING:
EXAM POWER UP: use when optimizing for performance and user experience
records with same name, point to different ips, point to different region
per region supports one record with same name 
aws maintains a db of latency between the users general location and the regions tagged in records
the record returned is the one which offers the lowest estimated latency and is healthy

GEOLOCATION ROUTING:
similar to latency based routing, location of customers and location of resources are used to influence resolution decisions.  does not return the closest record.  instead returns any that are relevant(same location) or default or 'no answer'. 
ip check verifies the location of the user
records are tagged with location.  either us state, country, continent, or default.  
use case:
when you want to restrict content to a specific region.  for ex you could list us record and will only be returned if requested from a user in us.  
when you want to provide language specific content
when you want to load balance across regional endpoints

GEOPROXIMITY ROUTING:
returns lowest distance, including bias 
you define a resource a region is created in via tag or lat and long coordinates
bias: + increases a regions size, and - descreases a regions size

INTEROPABILITY:
R53 normally has 2 jobs, domain registrar and domain hosting
r53 can do both, or either domain registrar or domain hosting
when registering a domain:
checks with TLD to see if domain is available
creates a zone file for registered domain, known as hosted zone
allocates 4 nameservers for the zone,
adds nameserver records to TLD that indicate that the namerservers are authoritative for domain

to do this think of r53 having a registrar role and hosting role that are seperate and work together to perform functionality
see slides for diff architectures of working together, working seperate

IMPLEMENTING DNSSEC USING ROUTE53:
some text here




*******************************************
RDS, RELATIONAL DATABASE SERVICE:
*******************************************

RELATIONAL(SQL) VS NON-RELATIONAL(NOSQL):
relational:
use SQL language
structure in and between tables of data - rigid Schema
schema is defined in advance before you put any data into the table
fixed relationship between tables before data is put into the table
nosql:
everything which isn't relational
generally a much more relaxed schema
relationships handled differently

different types of no-sql databases:
key-value:
stores list of key value pairs
no schema
scalable and really fast

wide column store:
variation of key-value
each row has one or more keys, one of them is partition key (minimum), secondary key is called sort or range key, partition key if no secondary keys or composite key needs to be unique
no attribute schema: can be all same/different, some empty, or no attributes
scalable and really fast

document:
designed to store and query data as documents
documents: items in db, formatted as json, structure can be different, but id needs to be unique
provides flexible indexing, which allow really powerful queries
use cases: order databases, contacts, or collections databases, ie interacting with whole documents or deep attribute interactions(nested data items within a document structure)


different styles of databases:
row store:
data stored as rows, 
ideal if you are operating with rows adding, updating, deleting whole row or record at a time
ideal for Online Transaction Processing, or OLTP
column store:
data stored as columns, ie every attribute for each document is stored together
ideal for analytics and reporting
graph:
relationships between things are formally defined and stored in the database as well as data
relationships aren't rerendered every time you make a query, different from relational database because those relationships are computed every time you make a query
nodes: objects, can have key-value properties
edges: relationships between objects, have name and direction, can have name/value pairs

ACID VS BASE:
ACID and BASE are DB transaction models
tradeoff between CAP Theorem: 
consistency, availability, and partition tolerant(resilience)
consistency: every read to a database will receive the most recent write or it will get an error
availability: every request will get a non error response but without the guarantee that it contains the most recent write
partition tolerance: the system can be made of multiple network partitions and system continues to operate even if there are a number of dropped messages or errors between these network nodes
CAP theorem: 
any database product will only be able to deliver a maximum of two of these different factors

ACID:
focuses on consistency
atomic: all or no components of a transaction succeeds or fails, 
consistent: transactions move the database from one valid state to another, nothing in between is allowed, 
isolated: transactions execute as if its the only one transacting, even if multiple transactions are occurring at once, 
durable: once committed, transactions are durable.  resilient to power outages or crashes.
EXAM POWER-UP: rds databases, limits databases to scale 

BASE: 
focuses on availability
basically available: read and write operations are available as much as possible, without any consistency guarantees
soft state: database doesn't enforce consistency, offloaded onto the application/user
eventually consistent: if we wait long enough, reads from the system will be consistent
EXAM POWER-UP: dynamoDB or NoSQL databases


DATABASES ON EC2:
generally a bad practice
why you might want to do it (but still bad practice lol):
you need access to the db instance OS
you need advanced db option tuning via dbroot permissions, aws managed db services do offer some of these options without dbroot permissions,
vendor or decision makers demand it
you need db, db version, architecture, or specific os/db combination aws doesn't provide, generally the only reason why you would want to run a db on ec2
if you are going to do it:
best practice is to split monolithic stack of application, web server, and db running on single instance into two instances where one instance is running application and web server and another instance running the db on it.  This way you can export db to aws managed service if needed.
why you shouldn't do it:
adds significant cost of admin overhead of managing ec2 and dbhost
adds significant cost of backup and disaster recovery management  
ec2 is single AZ, significant reduction of availability vs having regional availability when using services
less features, you would be missing out on some amazing features offered by aws db services
does not offer serverless features or easy scaling
adds significant cost of replication overhead
adds significant cost to performance, aws invests time into optimization and features for aws services you would need to spend a lot of time and energy to replicate


RDS ARCHITECTURE:
Database Server as a Service
provides multiple databases on one DB server(instance)
range of db engines to use (MySQL, mariaDB, PostgreSQL, Oracle, Microsoft SQL server)
Amazon Aurora is a different product
Managed service by aws, no access to OS or SSH access*, rds custom provides low level access

is not a public service, required to operate within a VPC
EXAM POWER UP: RDS subnet group:
a list of subnets RDS can use for a given database instance or instances 
EXAM POWER UP: each RDS instance can have 1+ databases
EXAM POWER UP: each RDS instance has one dedicated storage EBS volume
synchronous replication: where each time data is written to primary instance, the data is then copied from primary to standby immediately
asynchronous replication: read replicas back up data asynchronously to standby instances in same region or different region
EXAM POWER UP: backups and snapshots of data are transferred to s3
costs:
you're billed for resource allocation
instance size and type
multi az or not
storage type and amount
data transferred 
backups and snapshots
licensing if applicable


RDS MultiAZ:
offers high availability, two types 

- multiAZ instance deployments:
primary rds instance contains all of the database instances and is replicated synchronously to a standby instance in another AZ, 
synchronous replication: data is written to primary database and replicated to standby before it is viewed as committed
you always access the primary database instance, all reads and writes
backups and- snapshots to s3 are implemented via the standby instance 
not free tier, extra cost for replica
one standby replica only
60-120 seconds for failover
primary and standby have to be in same region
use cases for failover switch:
AZ outage, primary failure, manual failover, instance type change, software patching
historically this used to be the only way

- multiAZ cluster deployments:
EXAM POWER UP: one writer can implement synchronous replication to 2 readers only
writer is like primary rds instance
readers can be read by clients to scale read workloads
replication is via transaction logs which are more efficient, failover is around 35s
hardware is much faster, so beenfits from fast writes to local storage, which are then backed up to ebs volume
EXAM POWER UP: data is committed when data has been replicated to at least one reader
each instance has its own local storage
cluster endpoint: like a database CNAME in previous architecture, and points at the writer.  userd for reads, writes, and administration
reader endpoint: points to any reads at an available reader instance, and in some cases does include writer instance
instance endpoint: point at a specific instance. generally these are used for testing/ fault finding. each instance has one.  


RDS BACKUP AND RESTORE:
RPO: designates the variable amount of data that will be lost or will have to be re-entered during network downtime. good rpo minimizes amount of data that will be lost.
RTO: designates the amount of “real time” that can pass before the disruption begins to seriously and unacceptably impede the flow of normal business operations.
two types: automated backups and snapshots
both are backed up to aws managed s3 buckets, so you can't see the backups in s3 service however you can see backups in the RDS console
regionally resilient
backups occur from standy in multi az deployments or from only available instance if not multi az
snapshots:
done manually
like the s3 snapshots because first snap is full size of consumed data then incremental onward 
live on even after rds instance is deleted, you have to delete manually
automated backups:
scheduled backups
automatically deleted by aws for a retention period of 0 to 35 days
transaction logs:
store the actual operations which change the data
every 5 minutes transaction logs are backed up to s3
cross-region:
rds backups can be replicated to another region
both snapshots and transaction logs
charges apply for cross region data copy and the storage in destination region, has to be enabled
rds restores via snapshots or automated backups:
creates a new rds instance with new address so you will need to update applications to point to this address
restores aren't fast, we have to think about RTO

RDS READ REPLICAS:
provide benefits such as:
performance benefits for read operations:
able to deploy 5 direct read replicas per db instance
read replicas can have read replicas but can introduce lag issues
global performance improvements for read workloads
help us provide really low RTO, meaning they speed up the process of rds restores:
snapshots and backups improve RPO but not RTO
read replicas offer near 0 rpo and near 0 rto but only good for failure and not for corruption
help us create cross region failover capability:
global resilience
characteristics:
only read operations, unless promoted to rds instance
aren't part of the primary instance in any way ,have their own db endpoint address
can be created in same or different region as primary instance
kept in sync with primary instance via asynchronous replication:
data is written to primary instance first, viewed as committed, then replicated to read replicas

RDS SECURITY:
encryption in transit is available:
EXAM POWER UP: SSL/TLS (in transit) can be mandatory
EXAM POWER UP: by default, is done via ebs volume encryption via KMS
handled by host/ebs
aws or Customer managed CMK generated DEKs (data encryption keys) which are used for encryption operations
storage, logs, snapshots, and replicas are encrypted
EXAM POWER UP: encryption can't be removed once it's added

RDS MSSQL and RDS Oracle support TDE:
transparent data encryption
encryption handled within the db engine
EXAM POWER UP: oracle supports integration with CloudHSM which has much stronger key controls because it removes aws from the chain of trust

IAM authentication:
you can configure rds to allow IAM user authentication against a database
EXAM POWER UP: policy attached to users or roles maps that IAM identity onto the local RDS user via generating token that cna be used in place of db user password that generates an auth token
EXAM POWER UP: authentication only! authorization is always controlled by the db engine which grants permissions to lcoal db user

RDS CUSTOM:
fills gap between rds and ec2 running a db engine of fully managed by you and os access
works for MSSQL and Oracle and can connect using SSH, RDP, and session manager

AMAZON AURORA (provisioned) ARCHITECTURE:
is a database engine officially part of RDS but its architecture is radically different than any of the other db engines
key differences:
uses a cluster
consists of a single primary instance + 0 or more replicas, provides benefits of both rds multi az and rds read replicas
can have a max of 15 replicas
storage:
doesn't use local storage for the instances, instead uses shared cluster volume
faster provisioning, improved availability, and performance
storage has 6 replicas that spread across AZ, replicated automatically across az's
all SSD based, high IOPS, low latency
features:
backups in aurora work in same way as rds
restores create a new cluster
backtrack, can be used which allows in place rewinds to a previous point in time
fast clones, allow you to make a new database much faster than copying all data, provides reference to unchanged data and copies changed data, known as copy on write
billing:
 is different bc storage is billed based on what's used, high water mark so billed for the most used, and replicas can be added and premoved without requiring storage provisioning
no free tier option
beyond micro instances, aurora offers better value
compute charges are hourly charge, per second, 10 minute minimum
storage charges are gb per month consumer, and io cost per request
100% db size in backups are included
access method:
use endpoint, dns addresses which are used to connect to cluster, they have many available
cluster endpt: always points to primary instance
reader endpt: if there are replicas, will load balance reads amongst the replicas
custom endpts: point to specific subsets of instances within cluster
unique endpt: each primary and replica isntances have their own endpt, allows customization


AURORA SERVERLESS:
provides a version of the aurora database product where you don't need to statically provision database instances of a certain size or need to manage those database instances
removes admin overhead of managing database instances, ie easier to use
same resilience as aurora (6 copies across azs)
aurora capcacity units (acus):
represent a certain amount of compute and a corresponding amount of memory
aurora serverless cluster has a min and max acu
cluster adjusts based on load, can go to 0 and be paused after consecutive minutes of inactivity
architecture:
uses acus that are allocated from a shared warm pool used by aws, stateless, no local storage
acus access shared storage in same way that provisioned instances would
connections are a little bit more complex:
shared proxy fleet of instances managed by aws,
user connects to application via proxy fleet, and fleet brokers connection between acus
scaling is fluid
use cases:
infrequently used applications
new applications, or unpredictable workloads 
multi tenant applications
development and test databases


AURORA GLOBAL DATABASE:
allow you to create global level replication using aurora from a master region to up to 5 secondary aws regions
secondary clusters are read only, up to 16
1s replication
no impact on db performance

AURORA MULTI-MASTER:
feature that allows aurora to have multiple instances which are capable of performing both reads and writes
default aurora mode is single-master
in multi-master mode all instances are read/write
no concept of a load balancer, the application connects to one or all of the instances in cluster directly
benefit is that failover process is much quicker, because it does not need to switch endpts because it is already connected to multiple instances
when a write occurs:
an instance proposes a write change to all of the shared storage nodes and all other instance nodes have to agree for write to occur.
the write is then written to all of the shared storage nodes and then to each instance node's memory cache

RDS PROXY:
problem it fixes:
opening and closing connections consume resources and often take bulk of operation time
with serverless every lambda opens and closes
handling failure of datbase instances is hard, doing it within your application adds risks
db proxies help but managing them is somewhat hard 
what it does:
maintains a pool of connections that remain open for the database and applications
multiplexes, so there is a smaller number of connections open to the database than the number of connections open to the applications
EXAM POWER UP: each lambda function connection to proxy is much quicker to establish vs direct connection to db and no load on db
EXAM POWER UP: abstracts clients away from db failure or failover events, bc proxy connection is established and waits even if target db is unresponsive
use cases:
too many connections errors, db instances using t2/t3, smaller/burst instances
aws lambda, time saved via connection reuse and iam auth reuse
long running connections for SAAS apps
where resilience to db failure is a priority and make it transparent to application
key facts:
auto scaling, highly available by default
fully managed db proxy for rds/aurora
only accessible from a vpc via proxy endpt
can enforce ssl/tls connections
can reduce failover time by 60% and abstracts failure


DATABASE MIGRATION SERVICE (DMS):
managed database migration service
runs using a replication instance
source and destination endpts point at source and target databases and one endpt must be on aws
replication instance:
can define replication tasks, define all of the options relate to migration
three types of jobs:
  -full load migration: migrates existing data, outage until migrated
  -full load + CDC, change data capture: migrates existing data and replicates any ongoing changes from source to target
  -CDC only: replicates only data changes 
implements schema conversion tool: 
used when converting one database engine to another
use for larger migrations
use only when db engines are not compatible
when converting larger migrations:
dms can ultilize snowball products
whenever you utilize snowball products you can use sct when db engines are compatible also bc technically the db is changing




*******************************************
EFS, ELASTIC FILE SYSTEM:
*******************************************
provides network based file systems which can be mounted by within linux ec2 instances and used by multiple instances at once
EFS is an implementation of NFSv4
different from EBS because ebs is block storage and EFS is file storage
private service, 
access via mount targets inside a VPC, or from outside vpc via hybrid networking if you enable, vpn or direct connect (dx)
architecture:
see slide
posix permissions, standard for interoperability which is used in linux
mount targets in multiple azs
key takeaways:
linux only 
general purpose and max io performance+ models, general purpose is default for 99.99% of uses
bursting(default) and provisioned throughput modes
standard(default) and infrequent access classes


AWS BACKUP:
fully managed data protection (backup/restore) service
consolidate management into one place, across accounts and across regions
implements a wide range of products perform central management 
backup plans:
frequency, window, lifecycle, vault, region copy
resources: what resources are backed up
vaults: 
backup destination (container), you can assign kms key for encryption
vault lock: enables write once, read many, 72 hour cool off, then even aws can't delete, designed for compliant style situations
PITR: point in time recovery, you can restore state to a certain point of time within retention window




*******************************************
HIGH AVAILABILITY AND SCALING:
*******************************************
REGIONAL AND GLOBAL AWS ARCHITECTURE:
global components:
  -global service location and discovery: how does your machine discover correct reference?
  -content delivery and optimization: how does data get to users globally?
  -global health checks and failover: determining if endpoints are healthy and if not how to implement failover?
regional components:
  -scaling and resilience:
  -application services and components:

tiers: high level groupings of functinality or different zones of your application
web tier: communications from applications will generally enter web tier first, ex application load balancer or api gateway
compute tier: provides functionality for web tier, ex lambda ec2, ecs
storage tier: provides file storage, ex ebs, s3, efs
caching tier: improve performance and reduce costs, ex elasticache, dax
db tier: provides data storage, ex rds, dynamodb, redshift
app services tier: provide some type of functionality to applications, sqs, kinesis,


EVOLUTION OF ELASTIC LOAD BALANCER (ELB):
3 types of load balancers available within aws
there is one v1 type and two v2 types, avoid using v1
v2 is faster, cheaper, support target groups and rules, truly layer 7 devices
classic load balancer: v1, 2009, not really layer 7, lacking features, 1 sll cert. per clb limit which limits ability to scale
application load balancer:
v2, truly layer 7 device, supports HTTP/ HTTPS / WebSocket
network load balancers:
v2, truly layer 7 device, supports TCP/ TLS/ TLS & UDP

ELB ARCHITECTURE:
accepts connections from customer and distributes those connections across any registered backend compute, the physical infrastructure is abstracted away from the customer
without load balancers, everything would have to be tied to everything in a sequential flow and if there is failure, service would be disrupted
EXAM POWER UPS (ALL):
need 8+ IPs per subnet and a /27 or larger subnet to allow for scale
configured to run in 2+ AZs
1+ elb nodes are placed into a subnet in each AZ and scale with load
a record: each elb is configured with an A record DNS name that resolves to the ELB nodes
two different type of elb nodes:
  -internet-facing: 
  have public IPs and private IPs, can access public and private ec2 instances
  -internal: 
  only have private IPs, generally used to seperate different tiers of applications
listeners: nodes are configured with listeners which accept traffic on a port and protocol and communicate with targets on a port and protocol

elb's allow each tier to scale independently
cross zone elb:
feature that allows elb node to distribute connections across all AZs


APPLICATION AND NETWORK LOAD BALANCER (ALB VS NLB):
v2 load balancers support rules that contain 1 ssl per rule, each rule targets a target group, which contains resources corresponding to ssl
application load balancer:
listens on HTTP and/or HTTPS
can understand l7 content type, custom headers, user location, app behavior
can't understand any other layer 7 protocols and no TCP/ UDP/TLS listeners
HTTP HTTPS always terminated on the ALB, no unbroken SSL connection from customer through to your application instances, then a new connection is made from load balancer to the application
ALBs must have SSL certs if HTTPS is used
ALBs are slower than NLBs, bc there is more levels of the network stack to process
ALBs can perform application health checks
rules:
direct connections which arrive at a listener
processed in priority order, there is a default rule 
  -rule conditions: like host-header, http-header, http-request-method, path-pattern, query-string, source-ip
  -actions: forward, redirect, fixed-response, authenticate-oidc, authenticate-cognito
network load balancer:
layer 4 load balancer, TCP, TLS, UDP, TCP_UDP
SMTP, SSH, game servers, financial apps or apps that do not use http
no visibility or understand of HTTP or HTTPS, no headers, no cookies, no session stickiness
really really really fast, 75% faster than ALBs
health checks just check icmp/tcp handhsake, they do not provide application health checks
NLB's can have static IPs which are useful for whitelisting
can forward TCP to instances, so can you do unbroken end to end encryption
used with private link to provide services to other VPCs
NLB or ALB?:
NLB: unbroken encryption
NLB: static ip for whitelisting
NLB: fastest performance
NLB: protocols not HTTP or HTTPS
NLB: privatelink
ALB: any other scenario


LAUNCH CONFIGURATIONS AND LAUNCH TEMPLATES:
allow you to define the configuration of an ec2 instance in advance
you can define anything you normally define instance type, storage and key pair, networking and security groups, userdata, IAM role
not editable, launch templates have versions
lauch templates provide newer features: 
they provide all of the features launch configurations provide and more, including T2/T3 unlimited, placement groups, capacity reservations, and elastic graphics
launch configurations have one purpose: provide configuration of ec2 isntances of an auto scaling group
launch templates purpose: same as launch configuration or can be used to provision ec2 instances from the console UI or CLI

AUTO SCALING GROUPS:
allows ec2 to scale automatically based on demand placed on the system
generally used with elbs and launch templates to deliver elastic architectures
EXAM POWER UP: ASG defines when and where, uses launch templates or launch configurations to define what
has a minimum size, desired capacity, and maximum size
self healing for ec2
can implement cooldown periods to not make changes to avoid high expenses for rapid scaling 
free to use, only resources created are billed
more, smaller instances provide granularity vs having bigger instances
goal: to keep running instances at the desired capacity by provisioning or terminating instances
scaling policies: automatically adjust the desired capacity between the min and max values based on metrics like cpu load
don't need scaling policies, they can have none
    -manual: you manually adjust the desired capacity
    -scheduled: time based adjustment
    -dynamic: 
        -simple: simple if then adjustment
        -stepped: bigger +- based on difference, recommended by aws bc more flexible
        -target tracking: maintains a desired aggregate metric like keep cpu at 40%
        -scaling based on SQS queue: ApproximateNumberOfMessagesVisible, scale base on approximate number of messages visible
self healing:
implements ec2 status checks: monitors health of ec2 instances, and if they do not pass status check, terminates the instance and provisions a new one

scaling processes:
different process or functions to be performed by asg
  -LAUNCH and TERMINATE: if set to suspend then won't launch/terminate
  -AddToLoadBalancer: add instance to lb on launch
  -alarmNotification: determins if asg reacts to alarms from CW
  -AZRebalance: balances instances evenly across all AZs
  -Healthcheck: whether instance health checks across group are on/off
  -replaceUnhealthy: terminate unhealthy and replace
  -scheduledActions: scheduled on/off
  -standby: use this for instances that you don't want to be affected by anuthing the asg does

asg + load balancers:
LBs give ability app to point to many instances via target group, and ASG provides dynamic number of instances as needed
ASG can use application LB health checks rather than ec2 status checks but depend on situation as to which u might want to implement

ASG LIFECYCLE HOOKS:
allow you to configure custom actions on instances during ASG actions
actions are either instance launch or instance terminate transitions
whenever you create  scale out (creating instance) lifecycle hook: instance moves to pending:wait state, performs lifecycle hook then moves to a pending:proceed state, then instantiated 
whenever you create  scale in (terminating instance) lifecycle hook: instance moves to a terminating:wait state, then moves to terminating:proceed state when timeout expires or you can resume with CompleteLifecycleAction code, then performs lifecycle hook, then terminated 
lifecycle hooks can be integrated with SNS so that notifications for lifecycle hooks can be sent to a SNS topic
lifecycle hooks can be integrated with EventBridge so that other processes can be initiated based on the hooks

ASG HEALTH CHECKS:
ASG can use application LB health checks rather than ec2 status checks but depend on situation as to which u might want to implement
three different types: EC2(default), ELB(can be enabled), and custom 
EC2 unhealthy:
stopping, stopped, terminated, shutting down, or impaired(not 2/2 status checks)
ELB healthy:
running and passing ELB health check
they can be more application aware because has knowledge of layer 7 
custom:
instaces marked healthy or unhealthy by an external system
health check grace period: default is 300s, delay before starting checks, allows system launch, bootstrapping, and application start.  make sure you enable a suitable grace period so instances have enough time to be provisioned. 


SSL OFFLOAD AND SESSION STICKINESS:
3 ways an elb can handle secure connections: bridging, pass-through, or offload
-bridging mode:
  default mode of an of an application load balancer
  1 or more clients makes 1 or more connections to a load balancer
  listener is configured for HTTPS
  connection is terminated (DECRYPTED) on the ELB and needs a ssl certificate for the domain name the application uses
  aws does have some level of access to certificate
  elb decrypts the http then routes to backend based on that http, but before connecting to backend it encrypts data and initiates a new ssl connection to backend instances.
  instances need ssl certificates and compute required for cryptographic operations
  benefits:
  elb gets to decrypt http data and perform actions based on that data, flexible
  disadvantages:
  certificate stores on elb and that's a risk and ec2 instances need a copy of certificate which adds admin overhead
-pass-through: 
  network load balancer performs this architecture
  listener is configured for tcp.  no encryption or decryption happens on the NLB.
  client connects to elb, but elb just passes that connection to backend instances
  each instance needs to have certificate installed but elb doesn't so no certificate exposure to aws
-offload:
  connects to elb via https
  connection is terminated (DECRYPTED) on the ELB and needs a ssl certificate for the domain name the application uses
  elb to instance connections uses HTTP so not certificate or cryptographic requirements
  benefits:
  encrypted on public internet to aws
  disadvantages:
  not encrypted on aws network

session stickiness:
first time user makes a request, alb generates a cookie, AWSALB, which locks the device to a single backend instance for a duration
has to be enabled on an application load balancer for a target group

with no stickiness, connections are distributed fairly equal across all in service backend instances
if application is stateful, then user would lose session details if resuming on different instance
if application is stateless, then application could operate ok without stickiness connections


GWLB, GATEWAY LOAD BALANCERS:
transparent security appliance: scans data after it leaves and before it enters the application instance
allow to deploy, scale, and manage 3rd party virtual appliances such as firewalls, intrustion detection and prevention systems, and deep packet inspection systems
flow:
traffic source => GWLBE => GWLB => appliances => GWLB => GWLBE => aws destination: 
it combines a transparent network gateway (GWLB endpoint that is single entry and exit point for all traffic) that distributes traffic to GWLB that distributes traffic via a Geneve encapsulation tunnel to your 3rd party appliances on demand, and scales this load,
then the appliances filter traffic, send back to GWLB, that sends back to GWLB endpt, who then sends to aws destination





*******************************************
EVENT-DRIVEN ARCHITECTURE:
*******************************************

DIFFERENT TYPES OF ARCHITECTURE:
  -monolithic:
  all of the different components of application are coupled together, they fail together, scale together, and bill together because full capacity is needed at all times in case they are all running even if certain components are not running
  -tiered:
  different components can be run on different servers,
  still tightly coupled because specific instances are coupled to other specific instances
  -tiered w/ internal load balancers:
  different components can be run on different servers
  lb in between tiers allows instances to connect to any other instance
  still coupled bc there is still dependency from one tier to another that expects other tier to be there to answer its request, impossible to scale individual tiers down to 0 bc the communication is synchronous and needs another tier to be running at same time request is sent
  -tiered with queues, asg:
  different components can be run on different servers
  queues in between tiers removes coupling because asynchronous communication can occer, tiers are not tightly coupled anymore because a tier transacts with queue, not tier, then is done
  auto scaling group, provisions instances in a processing component based on queue length
  queue: system which accepts messages, messages can be ordered in a FIFO order, but not always the case
  
  microservice architecture:
  collection of microservices
  microservices:
  a tiny self sufficient application
  has its own input, output components, logic, and its own store of data 
  are small units that do things individually very well, they can be unique services or copies of same service
  producer, consumer, or both microservices: producer produces data, consumer consumes data, and some do both
  Microservices produce and consume events
  
  event driven architecture:
  event producers produce events in reaction to something
  event consumers wait for events to occur and do something with event or in response to event
  components can be both producers and consumers
  both producers and consumers are not running or consuming resources unless they have been triggered and resources are required
    -event routers: highly available, central exchange point for events
    contains event bus which is like a constant flow of information, producers send events here and event router routes them to consumers
  takeways:
  only consumes resources while handling events


AWS Lambda:
  function as a service, SAAS, short running and focused
  you are billed for the duration that a function runs
  key part of serverless architectures
  runtime environment: functions are loaded and run in a runtime environment with defined runtime
  you directly control the memory allocated for lambda functions whereas vCPU is allocated indirectly, 128MB to 10240 MB
  lambda function :
  deployment package service lamba executes
  contains code, wrappings, and configuration
  docker:
  does not support running docker containers, however you can use docker image to spin up a lambda image
  custom runtimes: allow people to create custom runtimes using layers
common uses:
serverless applications, s3, api gateway, lambda
file processing, s3, s3 events, lambda
database triggers, dynamodb, streams, lambda
serverless CRON, eventbridge/cwevents + lambda
realtime stream data processing, kinesis + lambda
EXAM POWER-UP: lambda functions have a timeout of 900s, or 15 mins

Lambda function permissions:
execution permissions and resource-based policies
execution permissions: 
define what your lambda functions can do,
use IAM role based access, do not embed credentials into lambda execution environment
also needs basic execution permissions in order to do things like posting logs in cloudwatch, AWSLambdaBasicExecutionRole
resource based policies:
define who can invoke or manage your lambda function
you can allow or deny certain entities from interacting with your lambda function
can only be changed via cli or api

two lambda networking modes: public and VPC networking 
  -public:
  default, can access public aws services and the public internet
  best performance because no customer specific vpc networking is required
  lambda functions have no access to VPC based services unless public IPs are provided and security controls allow external access
  -VPC networking:
  configured to run in private subnet, obey all of the vpc networking rules
  vpc gateway endpts can provide access to public aws services from private subnet
  nat gateway and internet gateway are required to access public internet from public subnet
    creates ENI to run:
    needs permission
    creates one ENI per unique set of security group and subnets, for lambda fns
    takes 90s for initial setup

lambda logging:
  lambda uses cloudwatch, cloudwatch logs, and xray for logging
  logs from lambda executions: cloudwatchlogs
  metrics (invocations, success/failure, retries, latency): cloudwatch
  distributed tracing: x-ray


Three types of lambda invocations: synchronous, asynchronous, and event source mappings
-synchronous: 
  cli/api invokes a function, passing in data and waiting for a response
  EXAM POWER UP: lambda returns response data, whether success or failure 
  EXAM POWER UP: errors or retries have to be handled within the client
-asynchronous:
  typically used when aws services invoke lambda functions
  asynchronous communication, so not waiting for response 
  EXAM POWER UP: error handling: incoming events are placed in the queue before being sent to the actual fn, if the fn returns an error to lambda service then lambda will retry fn twice by default or keep event kept in the queue for a certain time period.  If either are reached, then you can have error sent to DLQ, or Lambda destinations
  EXAM POWER UP: need to be idempotent: reprocessing a result should have the same end state
  EXAM POWER UP: lambda supports destinations (sqs, sns, lambda, eventbridge) where successful or failed events can be sent
-event source mapping: EXAM POWER UP: typically used on streams or queues which don't 
  support event generation to invoke lambda (sqs, kinesis, Dynamodb streams) 
  event source mapping within lambda service polls queues or streams for new data and gets batches of source data and sends into lambda functions as event batches
  EXAM POWER UP: permissions from the lambda execution role are used by the event source mapping to interact with event source
  EXAM POWER UP: lambda supports destinations (sqs, sns, lambda, eventbridge) where successful or failed batches can be sent

Aliases and Versions:
Versions: 
  includes:
  code and dependencies,  runtime setting and env variables, and ARN
  help manage the deployment of functions, enabling the ability to publish a new version of a function for testing, without affecting stable prod version
  with every version there are two ARNS, (references), one that points to the published fn and another that points to the copy
  published fn is immutable
alias: 
  like a pointer to a specific lambda function version,
  has ARN that is a reference to alias
  implement services within serverless architecture to point to alias so that when a lambda fn changes, you only have to update alias to point to new published version istead of changing all references for your services to point to new published version
 
Lambda Execution Context Reuse:
all EXAM POWER-UP:
execution context: 
container that holds all code and objects declared outside the handler method, like libraries, runtime, etc
cold start: 
there is added latency when a function is executed because execution context needs to be "bootstrapped", (created and configured)
warm start: 
after a function is executed, lambda maintains the execution context for 15 mins, "warming" the context for reuse so reduced latency when called again because context is reused
After 15 mins, lambda will terminate the execution context, and if called again will need to be bootstrapped again
provisioned concurrency:
feature that allows you to launch the amount of execution contexts you have specified and will keep them warm for you.  can be less expensive than running a Lambda on demand if you are using that capacity you reserved.


CLOUDWATCH EVENTS AND EVENTBRIDGE:
cloudwatch events: allows a near real time stream of system events that describe changes in aws services has visibility of cloudwatch,
eventbridge: 
super set of cloudwatch events functionality,
allows us to handle third party events as well as events from custom applications,
overview:
if X happens, or at Y time(s), ... do Z,
Eventbridge is basically CloudWatch Events v2,
a default event bus(stream of events which occurs from any supported service) for the account,
Cloudwatch events has only one implicit bus, eventbridge can have additional event busses,
rules match incoming events, or schedule based rules, and route events to 1+ targets, ex lambda,
flow:
Eventbridge sits over the top of event bus listening for events,
rules are linked to event bus, match incoming events, and route events to 1+ targets,


SERVERLESS ARCHITECTURE:
main focus is you do not have to manage the servers, or if you do very few,
applications are smaller than microservices, they are a collection of small and specialized functions,
run in stateless and ephemeral (short-lived) environment, implements duration billing,
event driven, consumption only when being used,
FaaS is used where possible for compute functionality
managed services are used where possible
  

SNS, SIMPLE NOTIFICATION SERVICE:
helps coordinate the sending and delivery of messages,
messages are <=256KB payloads,
public aws service,
SNS topics are the base entity of sns, they contain permissions and configurations,
  publisher: sends messages to a topic
  subscribers: receive messages from a topic, ex HTTP, email, sqs, mobile push, sms messages, lambda
sns used across aws for notifications
characteristics:
by default, each message published to a topic is received by every subscriber, but you can enable filter on which messages a subscriber receives
  delivery status: confirm receipt of message by subscribers
  delivery retries: reliable delivery
  HA and scalable: regionally resilient service
  SSE, server side encryption:
  Cross-acount via TOPIC policy: resource policy for sns topics


AWS STEP FUNCTIONS:
problems/limitations with Lambda:
  15 min max execution time,
  lambda chaining gets messy at scale, and not possible to transfer state
state machine:
  serverless workflow, has startpt, states, and endpt,
  states: are things which occur inside state machine, input data, modify data, output data,
maximum duration:
  1 year
two different types of workflows: standard and express
  standard workflow:
    default, has a 1 year maximum duration,
  express workflow:
    designed for high volume event processing workloads and high transactions
    has a 5 minute maximum duration
you can start via: api gateway, iot rules, eventbridge, lambda
  implements ASL, amazon states language
  IAM role is used for permissions
states:
  are things which occur inside state machine, input data, modify data, output data,
  succeed and fail states: process succeeded or failed,
  wait state: pauses processing for a certain period of time,
  choice state: takes a different path based on input,
  parallel: allows you to create parallel branches to perform multiple processes at same time,
  map: accepts a list of things, performs actions for each item,
  task: represents a single unit of work, coordinates with other services to actually perform that work, can be integrated with Lambda, batch, dynamodb, ecs, sns, sqs, glue, sagemaker, emr, step functions


API GATEWAY:
allows you to create and manage APIs,
sits between applications and integrations(services which provide the functionality of API gateway), ex: http endpts, lambda, step fns, sns, ddb,
highly available, scalabled, handles authorization, transformations, throttling, caching, CORS, OpenAPI spec, direct integration and more,
can connect to services/endpts in aws or on premises,
HTTP APIs, REST APIs, and Websocket APIs,
integrates with cloudwatch,
authentication: can natively integrate with cognito,

gateway cache:
  helps reduce the number of calls made to backend integrates and improves latency,
  configured per stage
  cache ttl default is 300 seconds, but can be configured to be 0-3600
  can be encrypted,
  cache size is 500MB to 237GB,

endpt types:
  edge-optimized: 
    incoming requests are routed to nearest cloudfron POP,
  regional:
    used when clients are in the same region,
  private:
    accessible only within a VPC via interface endpt

resource: abstract concept that allows you to expose something to be consumed by a client 
stage:
  is a reference to a deployment, or snapshot.  Helps you manage and optimize a particular deployment. every time you make a change to your api, you must deploy to a stage for it to go live. 
  each stage has its unique url as well as its own settings
  canary deployments: 
    Deployments are made to the canary (sub part of stage), not stage.
    can be configured so a certain percentage of traffic is sent to the canary.  This can be adjusted over time, to eventually promoting canary to be new base stage (consuming all of traffic)

3 phases in most api gateway interactions:
  request phase: 
    client makes a request to api gateway,
    authorizes, validates, transforms to data integration can handle,
  integration phase:
    request is moved thhrough api gateway to integration,
  response phase:
    where response is sent to client,
    transforms to data client can handle, prepares, returns to client,

facts and figures to remember:
  4xx errors: client error, invalid request on client side
    400: bad request, generic
    403: access denied, 
    429: exceeded configured throttling amount
  5xx errors: server error, valid request, backend issue
    502: bad gateway exception, bad output returned by lambda
    503: service unavailable, major service issues
    504: integration failure/timeout, 29s timeout for api gateway


SQS, SIMPLE QUEUE SERVICE:
public, fully managed, highly available queues,
messages up to 256 kb in size, if you need bigger message you can store link inside queue
clients can send messages to queue, and other clients poll the queue
encryption at rest via KMS can be enabled, in transit encryption is default
all sqs queues have enqueue timestamps (time when message is received) and retention period 
EXAM POWER UP: REMEMBER SNS AND SQS FANOUT ARCHITECTURE SLIDE
visibilityTimeout: 
  time period that a message is hidden after is it received.  If client (does not delete message or processing fails) after this time period is over, then message reappears back in the queue to be reprocessed
Dead Letter Queues (DLQ): can be used for problem messages
implementations:
  decouple components: one component sends messages to queue, other reads from the queue, that way they do not have to communicate directly
  scaling: ASGs can scale based on queue length and lambdas can be invoked when messages appear on a queue
billing:
  based on requests, request is a single request you make to sqs
  1 request = 1-10 messages up to 64KB total
two types of polling:
  -short: immediate, if 0 messages in queue then returns 0 messages
  -Long:(waitTimeSeconds): can be as long as 20s, if 0 messages in queue then waits for a message to appear before returning
two types: 
  -Standard:
    at least once delivery, no guarantee on order
    much faster performance, near unlimited TPS
    used for decoupling, worker pools, batch for future processing
  -FIFO:
    exactly once delivery, guaranteed order
    slower performance, 3000 TPS with batching, 300 TPS without batching
    must have a .fifo suffix in name
    used for workflow ordering, command ordering, price adjustments
Policies:
  -Queue policies: (resource policy) grant access to queue only from external accounts
  -identity polcies: grant access within your account


SQS DELAY QUEUES:
  allow you to postpone the delivery of messages to consumers
  messages are hidden for a set period of time, DelaySeconds, as soon as they are added to queue
  can set up to 15 minutes
  you can set per message setting of delay on standard queues but not FIFO queues

SQS DEAD LETTER QUEUES:
  help you handle reoccuring failures while processing messages which are within an SQS queue
  specifically you can analyze data being sent to DLQ, perform seperate processing, or testing
  each message contains a ReceiveCount, which is number of times it has entered queue
  allows you to configure an alarm when a message is delivered to a dead letter queue
  messages are automatically deleted from queue if retention period of DLQ > enqueue timestamp
  redrive policy:
    specifies the source queue, the DLQ, and conditions where messages will be moved from one to the other, it defines the maxReceiveCount, if maxReceiveCount > ReceiveCount message is deleted

KINESIS DATA STREAMS:
  scalable streaming service- allows you to ingest lots of data from a lot of different applications
  multiple producers can send data into a kinesis stream and multiple consumers can access data from the same stream 
  streams can scale from low to near infinite data rates
  public service and highly available
  kinesis data record:
    allows streams to store data for 24 hours by default but up to 365 days for cost 
  shards:
    streams have 1 to n shards
    the more shards the more performance and cost
    each shard has 1 MB/s ingestion capacity and 2 mb/s consumption capacity
  kinesis data firehose:
    connects to a stream and moves data from a stream into another aws service, ex s3

KINESIS DATA FIREHOSE:
  connects to a stream and moves data from a stream into another aws service, ex s3
  fully managed delivery service to load data for data lakes, data stores, and analytics services
  automatic scaling, fully serverless, resilient
  near real time delivery, approximately 60s or when buffer is filled with 1 MB of data
  exists so that data can be persisted past the set interval for storing data or to pass data to the available services natively, or when kinesis analysis isn't enough
  billing: volume through firehose
  architecture:
    EXAM POWER UP: kinesis data firehose or kinesis data stream can deliver to: 
      HTTP endpts (third party providers),
      splunk,
      redshift (uses s3 as intermediate, then copied to redshift, process is handled by firehose),
      elasticSearch,
      s3
    lambas can be used to transform data to expected format for delivery

KINESIS DATA ANALYTICS:
  real time processing of data using SQL
  ingests data from kinises data streams or firehose, or optionally static source like s3
  destinations: firehose(s3, redshift, elasticSearch, splunk), lambda, kinesis data streams
  good visual representation of architecture on KINESIS DATA ANALYTICS slide
  billing: pay for data you process, not cheap
  use cases:
    streaming data needing real time SQL processing
    time series analytics, elections/ e-sports
    real time dashboards, leaderboards for games
    real time metrics, security and response teams

KINESIS VIDEO STREAMS:
  allows you to ingest live video data from producers
  consumers can access data frame-by-frame or as needed
  can persist and encrypt data in transit and at rest as a managed service
  can't access directly via storage, access only via APIs 
  integrates with other aws services like Rekognition and Connect
  ex: 
    security cameras, smartphones, cars, drones, time serialized audio, thermal, depth, radar data

SQS VS KINESIS?:
SQS: 1 production group, 1 consumption group
SQS: decoupling and asynchronous communications
SQS: no persistence of messages, no window
Kinesis: huge scale ingestion
Kinesis: mulitple consumers, rolling window
Kinesis: data ingestion, analytics, monitoring, app clicks


AMAZON COGNITO:
allows authentication, authorization, and user management for web/mobile apps
USER POOLS:
  goal is for you to sign in and get a JSON web token, JWT
  imagine a db of users which can include external identities
  about login and about managing user identities
  JWTs can grant access to APIs via Legacy Lambda custom authorizers and API gateway, but not aws credentials
  features:
    user directory management and profiles, sign up and sign in (customizable UI), MFA, and other security features
IDENTITY POOLS:
  allow you to offer access to temporary aws credentials in exchange for token
  unauthenticated identities: guest users
  federated identities:
    swap external identities for short term aws credentials, ex would be google, facebook, twitter, saml2.0, user pool 
  process: Identity pool and external identity:
    token, which can be different types, is passed to identity pool,
    authenticated or unauthenticated role is granted to cognito, who grants aws temporary credentials to user
  process: Identity pool and user pool:
    jwt token is passed to identity pool,
    authenticated or unauthenticated role is granted to cognito, who grants aws temporary credentials to user

EXAM POWER UP: web identity federation: swapping of any external identity provider token for aws credentials.


AWS GLUE:
  glue jobs:
    crawls data sources and generates the aws glue data catalog
    moves and transforms data between source and destination 
    can be initiated manually or triggered by eventBridge
  serverless ETL, extract, transform, and load system
  similar to datapieline but datapipeline uses servers
  data sources include:
    stores: s3, rds, jdbc compatible and dynamodb
    streams: kinesis data stream and apache kafka
  data targets include:
    s3, rds, jdbc databases
  crawlers:
    connect to data stores, determine schema, and create metadata in data catalog
  data catalog:
    persistant metadata about data sources in region
    improves visbilitiy of data within organization because it is publicized and accessible
    one unique catalog per region per account
    some services that use data catalog:
      athena, redshift spectrum, EMR, Lake Formation
  AWS warm pool:
    aws managed compute resources to perform glue jobs


AMAZON MQ:
  solves this problem:
    EXAM:  helps orgs migrate from an existing system that uses topics or queues into AWS with little to no application change
  SNS and SQS won't work out of the box
  need standards compliant solution for migration
  what is it?:
  open source message broker based on managed Apache ActiveMQ(popular in enterprise)
    EXAM: supports JMS API, protocols such as AMQP, MQTT, OpenWire, and Stomp
  provides queues (one to one communication) and topics (one to many comm)
  characteristics:
  single instance available or HA pair
    EXAM: no aws native integrations...delivers activeMQ product which you manage
    EXAM: vpc based, not a public service, private networking required


AMAZON APPFLOW:
fully managed integration service,
like middleware, you can exchange data between applications using flows
syncs data across applications or aggregrate data from different source
public endpts, but works with privateLink(privacy)
appflow custom connector sdk, you can build your own




*******************************************
GLOBAL CONTENT DELIVERY AND OPTIMIZATION:
*******************************************
CLOUDFRONT ARCHITECTURE:
origin:
  the source location of your content
  s3 origin or custom origin
distribution:
  the base configuration entity of cloudfront
  contains behaviors
  behaviors:
    like a sub-configuration within a distribution
    link to origins
    have default behavior, but you can add more
    if path pattern of behavior is matched, then it is used
edge locations:
  local cache of your data
regional edge cache:
  larger version of an edge location.  provides another layer of caching.
  generally supports a number of local edge locations

cloudfront integrates with aws certificate manager for HTTPS so you can use SSL certificates with cloudfront
cloudfront performs read-only caching, does not ever write to origin
EXAM: security policy is defined on the distribution
EXAM: cache invalidation defined on the distribution


CLOUDFRONT BEHAVIORS:
EXAM: a single distribution can have multiple behaviors
defines:
  origin and origin groups
  viewer protocol policy: which policy is used between viewer and edge location
  allowed http methods 
  cache policies:
  restrict viewer access to a behavior: via trusted key groups or trusted signer, need signed cookies or signed urls to access
  function associations: lambda@edge


CF TTL AND INVALIDATIONS:
influence how long cf objects are cached and when rejected
basic process:
  TTL: determines how long object is valid in cache
  user requests object from edge location, if object is valid then it is returned.  
  user requests object from edge location, if not valid then fetched from origin:
    if versioned filed name of object in edge location is same as version file name in origin, then a 304 not modified is returned
    if versioned file name of object in edge location is same as versioned file name in origin, then object is returned

EXAM: default TTl is 24 hours, 
you can change TTL values, minimum TTL and maximum TTL, per object values, the minimum and maximum override a per object value if the per object value is not within the constraints

EXAM: you can set custom per object TTL values via custom origin or S3 via object metadata
  EXAM: Origin Header: cache-control max-age (seconds), 
  EXAM: Origin Header: cache-control s-maxage (seconds), 
  EXAM: Origin Header: Expires (Date and time)

invalidations:
  invalidates objects based on a path pattern that you specify,
  applies to all edge locations immediately but times time to invalidate objects
  a way to corrects errors and has a cost of using
  EXAM: use versioned file names rather than relying on invalidations


AWS CERTIFICATE MANAGER, ACM:
  ACM lets you run a public or private CA, certificate authority
  Private CA: applications need to trust your private CA
  Public CA: browsers trust a list of providers issued within OS, which can trust other providers
characteristics:
  can generate or import existing certificates
  EXAM: if generated, can automatically renew
  EXAM: if imported, you are responsible for renewal
  EXAM: can be deployed out to supported services, cloudfront, albs, api gateway
  EXAM: regional service, cannot leave the region they are generated or imported in, so the certificate needs to be in same region by service accessing it, however cloudfront operates as though its within us-east-1 so use us-east-1 for cloudfront and other global services


CLOUDFRONT AND SSL:
  EXAM: SSL supported by default as long as you use *.cloudfront.net default domain name
  to use SSL on custom domain name:
    verify ownership using a matching certificate, HTTPS optional
    EXAM: generate or import certificate in ACM in us-east-1
  two SSL connections when using cloudfront:
    viewer protocol: viewer and cloudfront
    origin protocol: cloudfront and origin
    EXAM: both need valid public certificates!
  ALB can use ACM, customon origin needs to use an external generated cert
  billing:
  SNI is free, dedicated host costs around 600$/mo per distribution
  SNI, server name identification: allows a client to tell a server which domain name its attempting to access, happens at TCP layer
    allows one IP for edge location to host many HTTPS websites that need their own certificate
    old browsers don't support SNI
    you can established a dedicated host ip address for edge location within CF


ORIGIN TYPES AND ARCHITECTURE:
  s3 buckets:
    EXAM: does not include website hosted on a s3 bucket
    you can restrict access to only CF, via origin access.  access is granted via trusted key groups or trusted signer
    protocol for origin protocol and viewer protocol are always the same
    custom headers available
  aws media package channel endpts:
  aws media store container endpts:
  everything else (web servers):
    custom origins
    EXAM: includes website hosted on a s3 bucket
    minimum origin ssl protocol: minimum ssl protocol that cf uses with the origin
    can differ from viewer protocol
    customize http port and https port between origin and edge location
    custom headers available: custom origins does not have origin access like s3 buckets so if you want to ensure connections only from cf you can do so via custom headers


SECURING CD AND S3 USING OAI:
securing connection between edge locations and origins:
  for s3 origins:
    oac, origin access control (new version of oai, origin access identity):
    type of identity that allows us to control access to s3 origins
    oac allows cf to sign requests going to s3 origin and need to update s3 bucket policy to allow oac
      legacy: cf becomes oai when accessing s3 origin and the oai can be used in s3 bucket policies to restrict everything but that or more OAIs
  for custom origins (2 ways):
    require custom header 
    use firewall that only allows IPs of edge locations

PRIVATE DISTRIBUTION AND BEHAVIORS:
securing connection between edge locations and viewers:
  behaviors are either public or private
  EXAM: private behaviors require signer, which is entity that provides signed cookie or url
  2 ways of configuring private behaviors:
    EXAM: old way, a cf key is created by an account root user, given to account, the account is added as a trusted signer
    EXAM: new way, create trusted key groups and add those as signers, gets put on the distribution

  Signed URLS vs cookies:
    EXAM: urls provide access to one object only
    use urls if your client doesn't support cookies
    EXAM: cookies provide access to groups of objects, all files of a type
    use cookies if you want to keep application url the same, signed urls change url format


Lambda@Edge:
  feature of CF that allows you to run lightweight lambda functions at edge locations
  allows lambda to adjust data between the viewer and origin
  currently supports node.js and python
  you cant access any vpc based resources since it runs in aws public space
  layers are not supported
  different limits vs normal lambda fns
  architecture:
    lambda fns can run at four different events of architecture (origin req/res, viewer req/res)
    30s timeout
  use cases:
    perform a/b testing with viewer request function
    migration between s3 origins, origin request
    different objects based on device, origin request
    content by country, origin request
    many more


GLOBAL ACCELERATOR:
  comprised of anycast IP addresses: allow a single IP to be in multiple locations.  routing moves traffic to closest location.
  traffic initially uses public internet and enters a global accelerator edge location, from the edge data transits globally across the aws global network leading to significantly better performance
  moves the aws network closer to customers
  EXAM: customer connections enter at edge, using anycast IPs
  EXAM: transits over aws backbone to 1+ locations
  
  CLOUDFRONT VS GLOBAL ACCELERATOR:
  GA used for non http/s (TCP/UDP) where CF can cache HTTP/HTTPS content only
  GA moves your connection/data faster to desired location whereas CF aims to cache content closer to you
  GA does not cache any content





*******************************************
ADVANCED VPC NETWORKING:
*******************************************
VPC FLOW LOGS:
  helps provide details of of traffic flow within private network
  captures metadata, not contents, ex srcip, dstip, srcport, dstport, protocol, etc, they can be configured to capture accepted, rejected, or all metadata

  can be attached to:
  can be attached to a vpc, which monitors every eni in that vpc
  can be attached to a subnet, which monitors every eni in that subnet
  can be attached to an eni directly
 
  not real time
  can be logged to s3 or cloudwatch logs, or athena for querying

EGRESS-ONLY INTERNET GATEWAY:
  allows connections to be initiated inside vpc to outside only, and allows response traffic back in
  like NAT for ipv4, but egress only is for ipv6
  architecture:
  exactly the same as NAT gateway

VPC ENDPTS - GATEWAY endpt:
  allows a private only resource within a VPC to access s3 or dynamodb
  allows a resource within a private only VPC to access s3 or dynamodb
  used in lieue of providing infrastructure like NAT
 
  regional, can't access cross-region services
  s3 buckets can be set to private only by allowing access only from a gateway endpt 
  gateway endpts are not accessible outside the VPC
  implementation:
  does not get deployed to subnet, instead prefix list added to route table of subnet that points traffic to gateway endpt.  gateway endpt is HA across all AZs in a region by default
  endpt policy: controls what things can be connected to by that gateway endpt

VPC ENDPTS - INTERFACE endpt:
  allows a private only resource within a VPC to any service but dynamodb
  allows a resource within a private only VPC to any service but dynamodb
  used in lieue of providing infrastructure like NAT

  added to specific subnents, to a single ENI, so not HA
  network access controlled via security groups
  endpoint policies: controls what things can be connected to by that gateway endpt
  only support TCP and IPV4 protocols
  implemented via PrivateLInk:
    allows external services to be injected into your VPC
  implementation:
    endpt provides a new service dns name so services within vpc can access new service via dns name via endpt
    regional dns
    zonal dns
    privateDNS: overrides the default DNS for services so the services can still retain same dns name, however when services within vpc try to access, they do so via endpt interface rather than accessing service directly

VPC PEERING:
  allows you to create a private and encrypted network link between two VPCs
  works between same/cross region and same/cross account
  option of public hostnames resolve to private IPs
  if in same region, security groups can reference peer security groups
  EXAM: does not support transitive peering, vpcs cannot connect via intermediate vpcs
  routing configuration is needed on both sides, SGs and NACLs can filter 
  EXAM: peering connections cannot be created where there is overlap in the VPC CIDrs, so odn't use the same address ranges in multiple VPCs


  

*******************************************
HYBRID ENVIRONMENT AND MIGRATION:
*******************************************
BGP, BORDER GATEWAY PROTOCOL:
  routing protocol that control how data flows from different points, based on shortest path of interconnected networks
  operates over TCP/179, it's reliable
  not automatic, peering is manually configured
  made up of many autonomous sytems (AS): 
    routers controlled by one entity, a network in BGP, details abstracted away from BGP
    allocated unique ASN via IANA, 16-32 bit numbers, allows BGP to identify networks
  path vector protocol:
    exchanges the best path (shortest) to a destination between peers, called the ASPATH
    by default exchanges the shortest ASPATH, but you can influence via path prepending, which can be used to make a certain path look like multiple paths if the connection is slow or unfavorable
  iBGP: internal BGP, routing within an AS
  eBGP: external BGP, routing between AS's, used most often with AWS

IPSEC VPN FUNDAMENTALS:
  group of protocols that allows you to set up secure tunnels across insecure networks between two routers (peers)
  provides authentication and data traveling through tunnels is encrypted
  interesting traffic:
    traffic that matches certain patterns to be transported via vpn tunnels
  two main phases:
    IKE phase one: 
      slow and heavy, protocol for how keys are exchanged within vpn
      authenticate with certifice or pre shared authentication
      uses asymmetric encryption to create a shared symmetric key
      creates SA, security association, phase 1 tunnel
    IKE phase two:
      fast and agile, uses the symmetric keys agreed in phase 1
      agrees on encryption method, and IPSEC key used for bulk data transfer
      creates IPSEC SA, phase 2 tunnel running over phase 1

    policy based VPNS:
      rules created to match traffic, 
      traffic for each rule is sent over a pair of SAs with unique IPSEC key
    route-based VPNS:
      do target matching based on prefix
      traffic is sent over a single pair of SAs
      one IPSEC key
      provides less functionality but simpler to set up

AWS SITE TO SITE VPN:
  logical connection between a VPC and on premises network encrypted using IPSec, running over the public internet
  EXAM: highly available, if you design and implement correctly
  EXAM: can be provisioned in less than an hour
  EXAM: speed limitations around 1.25Gbps
  EXAM: latency inconsistency possible over the public internet
  can be used as a backup for Direct Connect, DX
  can be used with Direct Connect, DX
  cost: aws hourly cost, GB out cost, data cap (on premises)
  components:
    virtual private gateway, VGW:
      has same functionality as internet gateway, except
      has endpoints located in different AZs
    customer gateway, CGW:
      logical gateway that corresponds to customers router
    VPN connection between the VGW AND CGW:
      vpn tunnels connect the endpoints and the CGW
  architecture:  
    partially highly available design:
      single point of failure lies with the customer gateway
    highly available design:
      having two customer gateways, with two different internet connections and seperate buildings
  
  static vs dynamic vpn:
    static:
      static routes of the other network are manually configured for each network
      no load balancing and multi connection failover
    dynamic:
      BGP is configured between VGW and CGW using ASN, network info is exchanged via BGP
      multiple VPN connects are possible providing HA and traffic distribution
      route propogation: if enabled means routes are added to route tables automatically


DIRECT CONNECT, DX:
  a physical connection (1, 10, or 100 Gbps)
  aws allocates you a port and authorizes you to use port at a DX location, you need to arrange connection to that port
  no resilience for physical connection
  low and consistent latency, highest speeds
  can access VPCs (private services) and aws public services, not public internet though
  connection: 
    business premises => dx location => aws region
  cost: 
    port hourly cost and outbound data transfer
  DX location:
    EXAM: is a large regional data center not owned by aws, aws rents out space and equipment, the DX endpts called DX routers
    customer or comms partner cage is space rented out by you or comms partner, your router is called customer DX router
    EXAM: if you don't have customer cage, then a comms partner can extend the DX port into your business premises
  AWS DX Router connection to DX port: the port allocated to you is connected to the aws DX router port
  Cross connect: connection between AWS DX Router port and customer/partner dx router port


DX RESILIENCE AND HA:
  multiple dx routers connect to multiple customer dx routers
  provision several customer premises with routers


TRANSIT GATEWAY, TGW:
  network transit hub that helps connect VPCs to on premise networks
  allows you to significantly reduce network complexity
  signle network object,  highly available and scalable
  allows attachments to other network types:
    VPC, Site to site VPN, and Direct Connect Gateway
  benefits:
    supports transitive routing, very helpful between VPCs
    can be used to create global networks via connecting to different TGW in different regions and different accounts
    share between accounts using AWS RAM


STORAGE GATEWAY:
  virtual machine on premises
  acts as a bridge to share data in storage on premise with aws storage services
  integrates with EBS, S3, and Glacier
  presents storage using iSCSI, NFS, or SMB
  EXAM: helps with migrations, extensions of a data center into AWS, storage tiering, DR and replacement of backups systems
STORAGE GATEWAY- VOLUME:
  EXAM: iSCSI raw block devices
  architecture:
    cached mode:
      EXAM: main location for data is not on premises, rather in s3 in aws, 
      has local cache that stores frequently accessed data
      EXAM: allows data center extension
    stored mode: 
      presents volumes over iSCSI to servers running on premises
      local storage on storage gatweay VM is source of storage 
      upload buffer asynchronously copies data to SG endpt in aws, data then sent to EBS snapshots
      EXAM: great for full disk backups of servers
      EXAM: great for disaster recovery
      EXAM: doesn't improve datacenter capacity, many copy of data is stored on the GW
      32 volumes per gateway, 16TB per volume, 512TB per gateway
STORAGE GATEWAY- TAPE (VTL, virtual tape library):
  EXAM: allows you to do tape backups in aws
  some large backups are written to tape, up to 60TB compressed
  the tape medium allows write as a whole or read as a whole
  architecture:
    has upload buffer and local cache on premise
    on premise server implements iSCSI to modify virtual tapes, which  sends instructions to endpt, which communicates with virtual tape library in aws running on s3, and archives from s3 into tape shelf with unlimited storage in glacier
STORAGE GATEWAY - FILE GATEWAY:
  EXAM: allows you to link local on premise file storage to s3
  EXAM: mount points (shares) available via NFS or SMB
  EXAM: primary data is held in s3
  on premise files map directly onto an s3 bucket, are visible as objects in an s3 bucket and vice versa
  read and write caching ensures LAN-like performance
  bucket share:
    one aws s3 bucket linked to on premise file share
  10 bucket shares per file gateway
  use the NotifyWhenUploaded API to notify other gateways when objects are changed 
  file gateway doesn't support object locking, so if two writes occer at same time there can be data loss.  To prevent this, use read only mode on all shares besides one or tightly control file access
  when a file share modies a file on premise, it gets replicated to s3, but s3 doesn't notify other fileshares automatically of change


SNOWBALL/ EDGE / SNOWMOBILE:
  allow you to move large amounts of data in and out of aws
  physical storage devices range from suitcase or truck
  snowball:
    ordered from aws, log a job, device delivered
    any data on device is encrypted using KMS
    50TB or 80TB capacity
    1 Gbps or 10 Gbps network
    EXAM: economical range when to use is 10TB to 10PB
    EXAM: you can order multiple snowbalss to multiple premises
    EXAM: helps only with storage
  snowball edge:
    EXAM: both storage and compute capability
    larger capacity vs snowball and faster networking
    10 Gbps, 10/25 SFP, 45/50/100 Gbps
    Storage optimized option: with EC2, 1TB SSD
    Compute optimized option: 100TB + ...
    Compute with GPU optimized option: same but with a GPU
    EXAM: ideal for remote sites or where data processing on ingestion is needed
  snowmobile:
    portable datacenter within a shipping container on a truck
    EXAM: economical range when to use is for one location when 10PB - 100PB


AWS DIRECTORY SERVICE:
  aws implementation of directory service
  runs within a VPC
  HA by deploying into multiple AZs
  some aws services need a directory, ex Amazon Workspaces
  can be isolated, independent of any other directories, or integrated with existing on premises system
  can act as proxy back to on premises
  directory:
    stores objects( users, groups, computers, servers, file shares) with a structure (domain/tree)
    multiple trees can be grouped into a forest
    commonly used in windows environments, Microsoft Active Directory Domain Services, AD DS 
    sign in to multiple devices with same credentials provides centralized management for assets
  architecture:
    simple AD mode:
      standalone directory which uses Samba 4, open source
      small mode is up to 500 users, large mode is up to 5000 users
      integrates with aws services
      not designed to integrate with any existing on premises 
      use case:
        default. simple requirements. a directory in aws.
    managed microsoft AD:
      full microsoft AD DS running in AWS
      direct connect or vpn connection set up between aws and on premises
      primary running location in aws, resilient if the vpn fails
      use case:
        applications in aws which need MS AD DS, or you need to trust on premise AD DS
    AD connector:
      when you want to use only one aws service that requires you to implement AD service
      only a proxy to integrate your aws service with your on premise directory via VPN
      use case:
        use aws services which need a directory without storing any directory info in the cloud, is a proxy to your on premises directory

      
AWS DATASYNC:
  data transfer service to and from aws
  migrations, data processing transfers and then back out, archival/cost effective storage
  designed to work at huge scale
  keeps metadata
  built in data validation
  features:
    scalable, around 100TB per day
    bandwidth limiters
    incremental and scheduled transfer options
    supports compression and encryption
    automatic recovery from transit errors
    aws service integration s3, EFS, FSx
